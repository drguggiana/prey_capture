{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a111aa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath(r'C:/Users/mmccann/repos/bonhoeffer/prey_capture/'))\n",
    "\n",
    "\n",
    "import panel as pn\n",
    "import holoviews as hv\n",
    "from holoviews import opts, dim\n",
    "from holoviews.operation import histogram\n",
    "hv.extension('bokeh')\n",
    "from bokeh.resources import INLINE\n",
    "\n",
    "import paths\n",
    "import functions_bondjango as bd\n",
    "import functions_kinematic as fk\n",
    "import functions_misc as fm\n",
    "import functions_plotting as fp\n",
    "import functions_loaders as fl\n",
    "from functions_tuning import normalize, calculate_dff\n",
    "from snakemake_scripts.classify_batch import reverse_roll_shuffle, chunk_shuffle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import processing_parameters\n",
    "import importlib\n",
    "from pprint import pp as pprint\n",
    "from itertools import product\n",
    "\n",
    "import scipy.signal as ss\n",
    "import scipy.stats as stat\n",
    "import sklearn.preprocessing as preproc\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d5b7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the figure config\n",
    "importlib.reload(fp)\n",
    "importlib.reload(processing_parameters)\n",
    "# define the target saving path\n",
    "save_path = os.path.join(paths.figures_path, 'wf_GLM_NN')\n",
    "\n",
    "# define the printing mode\n",
    "save_mode = True\n",
    "# define the target document\n",
    "target_document = 'poster'\n",
    "# set up the figure theme\n",
    "fp.set_theme()\n",
    "# load the label dict\n",
    "label_dict = processing_parameters.wf_label_dictionary\n",
    "variable_list = processing_parameters.variable_list_free + processing_parameters.variable_list_visual\n",
    "# define ca activity type\n",
    "ca_type = 'spikes'    # 'fluor' or 'spikes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7006ed39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_extra_angles(ds):\n",
    "    \n",
    "    # Apply wrapping for directions to get range [0, 360]\n",
    "    ds['direction_wrapped'] = ds['direction'].copy()\n",
    "    mask = ds['direction_wrapped'] > -1000\n",
    "    ds.loc[mask, 'direction_wrapped'] = ds.loc[mask, 'direction_wrapped'].apply(fk.wrap)\n",
    "\n",
    "    # Now find the direction relative to the ground plane\n",
    "    try:\n",
    "        ds['direction_rel_ground'] = ds['direction_wrapped'].copy()\n",
    "        ds.loc[mask, 'direction_rel_ground'] = ds.loc[mask, 'direction_rel_ground'] + ds.loc[mask, 'head_roll']\n",
    "    except KeyError:\n",
    "        ds['direction_rel_ground'] = ds['direction_wrapped'].copy()\n",
    "\n",
    "    # Calculate orientation explicitly\n",
    "    if 'orientation' not in ds.columns:\n",
    "        ds['orientation'] = ds['direction_wrapped'].copy()\n",
    "        ds['orientation_rel_ground'] = ds['direction_rel_ground'].copy()\n",
    "        mask = ds['orientation'] > -1000\n",
    "        ds.loc[mask, 'orientation'] = ds.loc[mask, 'orientation'].apply(fk.wrap, bound=181)\n",
    "        ds.loc[mask, 'orientation_rel_ground'] = ds.loc[mask, 'orientation_rel_ground'].apply(fk.wrap, bound=181)\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38c6640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "\n",
    "importlib.reload(processing_parameters)\n",
    "importlib.reload(fl)\n",
    "\n",
    "# get the paths from the database using search_list\n",
    "all_paths, all_queries = fl.query_search_list()\n",
    "mice = ['_'.join(os.path.basename(path).split('_')[7:10]) for path in all_paths[0]]\n",
    "print(all_paths, mice)\n",
    "\n",
    "data_list = []\n",
    "# load the data\n",
    "for path, queries in zip(all_paths, all_queries):\n",
    "    \n",
    "    data, _, _  = fl.load_preprocessing(path, queries, latents_flag=False)\n",
    "    data_list.append(data)\n",
    "\n",
    "for i, (ds, mouse) in enumerate(zip(data_list[0], mice)):\n",
    "    ds['mouse'] = mouse\n",
    "\n",
    "    # drop activity not of correct type\n",
    "    cols_to_drop = [el for el in ds.columns if ('cell' in el) and (ca_type not in el)]\n",
    "    ds.drop(cols_to_drop, axis='columns', inplace=True)\n",
    "\n",
    "    # If using fluorescence data, calculate dF/F\n",
    "    if ca_type == 'fluor':\n",
    "        ds = calculate_dff(ds, baseline_type='iti', inplace=True)\n",
    "\n",
    "    # Do a quick calculation of orientation & dir/ori relative to ground\n",
    "    if ('direction_wrapped' in variable_list) and ('direction_wrapped' not in ds.columns):\n",
    "        ds = calculate_extra_angles(ds)\n",
    "\n",
    "    # Drop the ITI\n",
    "    ds.drop(ds[ds['trial_num'] == 0].index, inplace=True)\n",
    "    ds.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    data_list[0][i] = ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa2e5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxmin(array_in):\n",
    "    return (array_in-np.nanmin(array_in))/(np.nanmax(array_in)-np.nanmin(array_in))\n",
    "\n",
    "def basis_predictors(variable, basis_number, kernel, kernel_spacing, total_length,label):\n",
    "    # initialize the output dataframe\n",
    "    out_frame = pd.DataFrame()\n",
    "    # generate the displaced basis functions\n",
    "    for idx2 in np.arange(basis_number):\n",
    "        # generate the sizes of the before and after padding of the kernel\n",
    "        back = int(kernel_spacing*idx2)\n",
    "        front = int(total_length-kernel.shape[0]-back)\n",
    "        # generate the full kernel\n",
    "        if back == 0:\n",
    "            current_kernel = np.concatenate((kernel, np.zeros(front)))\n",
    "        elif idx2 == basis_number-1:\n",
    "            current_kernel = np.concatenate((np.zeros(back), kernel))\n",
    "        else:\n",
    "            current_kernel = np.concatenate((np.zeros(back), kernel, np.zeros(front)))\n",
    "\n",
    "        # convolve with the data\n",
    "        vector = np.convolve(variable, current_kernel, 'same')\n",
    "        # normalize to 0-1\n",
    "        vector = maxmin(vector)\n",
    "        # if the vector was all zeros, it'll turn into nans so remove\n",
    "        vector[np.isnan(vector)] = 0\n",
    "\n",
    "        # generate the field in the new data frame\n",
    "        out_frame[label+'_'+str(idx2)] = vector\n",
    "        \n",
    "    return out_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77801149-0195-455f-bc39-5c620b19630d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assemble the feature and calcium matrices\n",
    "\n",
    "# set up the feature and calcium matrices\n",
    "# list the radial features in the dataset\n",
    "# radial_features = ['cricket_0_delta_heading', 'cricket_0_visual_angle', 'mouse_heading', \n",
    "#                    'cricket_0_delta_head', 'cricket_0_heading', 'head_direction']\n",
    "radial_features = ['head_direction', 'mouse_heading', 'head_pitch', 'head_yaw', 'head_roll', \n",
    "                   'direction', 'direction_wrapped', 'orientation', 'direction_rel_ground', 'orientation_rel_ground']\n",
    "# define the design matrix\n",
    "feature_list = variable_list\n",
    "\n",
    "# define the frame rate (fps)\n",
    "frame_rate = processing_parameters.wf_frame_rate\n",
    "# define the width of the kernel (s), multiplied to convert to frames\n",
    "sigma = 1*frame_rate\n",
    "# calculate the kernel\n",
    "kernel = ss.gaussian(sigma*5, sigma)\n",
    "# define the number of basis functions per regressor\n",
    "basis_number = 9\n",
    "# define the kernel spacing (in s)\n",
    "kernel_spacing = 0.2*frame_rate\n",
    "# get the total length of the kernel\n",
    "total_length = kernel_spacing*(basis_number-1) + kernel.shape[0]\n",
    "# # get the start positions of the basis functions (assume sigma defines the interval)\n",
    "# basis_starts = [int(el) for el in np.arange(-sigma*((basis_number-1)/2), \n",
    "#                                        sigma*((basis_number-1)/2)+1, sigma)]\n",
    "# allocate memory for the output\n",
    "feature_trials = []\n",
    "# allocate memory for a data frame without the encoding model features\n",
    "feature_raw_trials = []\n",
    "# allocate memory for the calcium\n",
    "calcium_trials = []\n",
    "# allocate a list for the mouse/day pairs\n",
    "pairs_list = []\n",
    "# get the number of trials\n",
    "trial_number = len(data_list[0])\n",
    "# get the features\n",
    "for idx, el in enumerate(data_list[0]):\n",
    "    # drop any nans\n",
    "    el = el.dropna()\n",
    "    \n",
    "    # get the intersection of the labels\n",
    "    label_intersect = [feat for feat in feature_list if feat in el.columns]\n",
    "    \n",
    "    if len(label_intersect) != len(feature_list):\n",
    "        continue\n",
    "        \n",
    "    # get the features of interest\n",
    "    target_features = el.loc[:, feature_list]\n",
    "    # save the original features for simpler calculations\n",
    "    feature_raw_trials.append(target_features.copy())\n",
    "    # get the original columns\n",
    "    original_columns = target_features.columns\n",
    "    \n",
    "    # turn the radial variables into linear ones\n",
    "    # for all the columns\n",
    "    for label in original_columns:\n",
    "        # calculate head speed\n",
    "        if label == 'head_direction':\n",
    "            # get the head direction\n",
    "            head = target_features[label].copy().to_numpy()\n",
    "            # get the angular speed and acceleration of the head\n",
    "            speed = np.concatenate(([0], np.diff(ss.medfilt(head, 21))), axis=0)\n",
    "            acceleration = np.concatenate(([0], np.diff(head)), axis=0)\n",
    "            # add to the features\n",
    "            target_features['head_speed'] = speed\n",
    "            target_features['head_acceleration'] = acceleration\n",
    "   \n",
    "        # check if the feature is radial\n",
    "        if label in radial_features:\n",
    "            # get the feature\n",
    "            rad_feature = target_features[label].copy().to_numpy()\n",
    "            # convert to radians\n",
    "            rad_feature = np.deg2rad(rad_feature)\n",
    "            # perform angular decomposition (assume unit circle)\n",
    "            x = np.cos(rad_feature)\n",
    "            y = np.sin(rad_feature)\n",
    "            # replace the original column by the extracted ones\n",
    "            target_features[label+'_x'] = x\n",
    "            target_features[label+'_y'] = y\n",
    "            # drop the original column\n",
    "            target_features.drop(labels=label, axis=1, inplace=True)\n",
    "            \n",
    "        # check if the label is a speed and calculate acceleration\n",
    "        if 'speed' in label:\n",
    "            # get the speed\n",
    "            speed = target_features[label].copy().to_numpy()\n",
    "            # calculate the acceleration with the smoothed speed\n",
    "            acceleration = np.concatenate(([0], np.diff(ss.medfilt(speed, 21))), axis=0)\n",
    "            # add to the features\n",
    "            target_features[label.replace('speed', 'acceleration')] = acceleration\n",
    "    \n",
    "    # Generate the gaussian convolved and displaced regressors\n",
    "    # allocate an empty dataframe for the outputs\n",
    "    new_dataframe = pd.DataFrame()\n",
    "    # for all the regressors\n",
    "    for label in target_features:\n",
    "        # get the variable\n",
    "        variable = target_features[label].to_numpy().copy()\n",
    "        # Remove nans\n",
    "        variable[np.isnan(variable)] = 0\n",
    "\n",
    "        # get the basis function-based predictors\n",
    "        out_frame = basis_predictors(variable, basis_number, kernel, kernel_spacing, total_length,label)\n",
    "        # add to the dataframe\n",
    "        new_dataframe = pd.concat((new_dataframe, out_frame), axis=1)\n",
    "\n",
    "#     # add a constant factor\n",
    "#     constant = np.ones(new_dataframe.shape[0])\n",
    "#     new_dataframe['constant'] = constant\n",
    "    # add a trial factor\n",
    "#     new_dataframe['trial'] = idx*np.ones(vector.shape[0])\n",
    "#     # for all the trials\n",
    "#     for trial in np.arange(trial_number):\n",
    "#         new_dataframe['trial_'+str(trial)] = np.zeros(vector.shape[0])\n",
    "#         if trial == idx:\n",
    "#             new_dataframe['trial_'+str(trial)] += 1\n",
    "\n",
    "    \n",
    "    # replace the old dataframe with the new one\n",
    "    target_features = new_dataframe\n",
    "        \n",
    "    # store the columns\n",
    "    resulting_columns = target_features.columns\n",
    "    # turn the dataframe into an array\n",
    "    target_features = target_features.to_numpy()\n",
    "\n",
    "    # store the array\n",
    "    \n",
    "    feature_trials.append(target_features)\n",
    "    \n",
    "    # get the calcium data\n",
    "    cells = [cell for cell in el.columns if 'cell' in cell]\n",
    "    cells = el.loc[:, cells].to_numpy()\n",
    "\n",
    "    # store\n",
    "    calcium_trials.append(cells)\n",
    "    # store the mouse and date\n",
    "    pairs_list.append([el.loc[0, 'mouse'], el.loc[0, 'datetime'][:10]])\n",
    "\n",
    "print(f'Time by features: {feature_trials[0].shape}')\n",
    "print(f'Time by ROIs: {calcium_trials[0].shape}')\n",
    "print(resulting_columns)\n",
    "\n",
    "# calculate the unique pairs for mouse and date\n",
    "unique_pairs = np.unique(pairs_list, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9388c8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot an example feature matrix and corresponding calcium\n",
    "raster = hv.Raster(feature_trials[0].T)\n",
    "raster.opts(width=800, height=600, xlabel='Frames', ylabel='Features', tools=['hover'], cmap='Reds')\n",
    "\n",
    "raster1 = hv.Raster(calcium_trials[0].T)\n",
    "raster1.opts(width=800, height=300, xlabel='Frames', ylabel='Cells', tools=['hover'], cmap='Viridis')\n",
    "(raster+raster1).cols(1).opts(shared_axes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943cd778",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(calcium_trials[0].shape, feature_trials[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35b6d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_glm_nn(current_features, current_calcium, scaler=None, activation='relu', l1=0.01, l2=0.01, loss='mse', learning_rate=0.001, validation_split=0.3,\n",
    "                     batch_size=100, epochs=200, test_train_shuffle=True, verbose=0, sample_shuffle=None, scale_calcium=False):\n",
    "    \"\"\"Train a GLM-NN for the given data\"\"\"\n",
    "    \n",
    "    # scale the features\n",
    "    if scaler is not None:\n",
    "        current_features = [scaler().fit_transform(el) for el in current_features]\n",
    "    # scale the calcium\n",
    "    if scale_calcium:\n",
    "        current_calcium = [el/np.max(el) for el in current_calcium]\n",
    "    # concatenate them\n",
    "    current_features = np.vstack(current_features)\n",
    "    current_calcium = np.vstack(current_calcium)\n",
    "    # get the trial data\n",
    "    X = current_features.copy()\n",
    "\n",
    "    # get the calcium\n",
    "    y = current_calcium.copy()\n",
    "    # shuffle if needed\n",
    "    if sample_shuffle is not None:\n",
    "        y = sample_shuffle(y)\n",
    "    \n",
    "    # get the number of output features\n",
    "    output_features = y.shape[1]\n",
    "    # define the optimizer and network parameters\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(output_features, activation='relu'))\n",
    "    model.add(layers.ActivityRegularization(l1=0.02, l2=0.03))\n",
    "    # compile the model with the Adam optimizer\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
    "\n",
    "    # train the model\n",
    "    history = model.fit(X, y, validation_split=validation_split, batch_size=batch_size, epochs=epochs, shuffle=test_train_shuffle, verbose=verbose)\n",
    "    \n",
    "    # calculate performance\n",
    "    predictions = model.predict(X)\n",
    "    correlations_per_cell = [stat.spearmanr(predictions[:, el], y[:, el])[0] for el in np.arange(predictions.shape[1])]\n",
    "    average_correlation = np.nanmean(correlations_per_cell)\n",
    "    performance = [correlations_per_cell, average_correlation]\n",
    "    # extract the weights\n",
    "    weights = model.layers[0].get_weights()[0]\n",
    "    # extract the history\n",
    "    history = history.history\n",
    "    \n",
    "    return performance, weights, history, X, y, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dccb51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define network parameters\n",
    "kwargs = {\n",
    "    'learning_rate': 0.001,\n",
    "    'validation_split': 0.3,\n",
    "    'batch_size': 100,\n",
    "    'epochs': 100,\n",
    "    'test_train_shuffle': False,\n",
    "    'verbose': 0,\n",
    "    'scaler': preproc.StandardScaler,\n",
    "    'l1': 5,\n",
    "    'l2': 1,\n",
    "    'scale_calcium': True,\n",
    "    'sample_shuffle': None,\n",
    "#     'sample_shuffle': reverse_roll_shuffle,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68905c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# calculate the fits for all the trials\n",
    "\n",
    "# allocate memory for the performances\n",
    "glm_performances = []\n",
    "# allocate memory for the weights\n",
    "glm_weights = []\n",
    "# allocate memory for the losses\n",
    "glm_loss = []\n",
    "\n",
    "# for all the pairs\n",
    "for mouse, day in unique_pairs[:1]:\n",
    "    print(mouse, day)\n",
    "    # find the corresponding trials\n",
    "    trial_idx = [el for el in np.arange(len(feature_trials)) if (mouse == pairs_list[el][0]) & (day == pairs_list[el][1])]\n",
    "    \n",
    "    # for all the trials\n",
    "    current_features = [feature_trials[el] for el in trial_idx]\n",
    "    current_calcium = [calcium_trials[el] for el in trial_idx]\n",
    "    \n",
    "    # train the net\n",
    "    performance, weights, loss, X, y, model = train_test_glm_nn(current_features, current_calcium, **kwargs)\n",
    "    # save the output\n",
    "    glm_performances.append(performance)\n",
    "    glm_weights.append(weights)\n",
    "    glm_loss.append(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05ade2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot lossess and mse\n",
    "\n",
    "plot_list = []\n",
    "# for all the days\n",
    "for idx, (mouse, day) in enumerate(unique_pairs):\n",
    "    if idx + 1 > len(glm_loss):\n",
    "        continue\n",
    "    loss = glm_loss[idx]['loss']\n",
    "    val_loss = glm_loss[idx]['val_loss']\n",
    "    \n",
    "    performance = glm_performances[idx][1]\n",
    "    # assemble the title\n",
    "    title = f'{mouse} {day} {performance:0.3f}'\n",
    "\n",
    "    loss_plot = hv.Curve(loss, label='Train Loss')\n",
    "    loss_plot.opts(width=400, height=400, tools=['hover'], xlabel='Epochs', ylabel='Loss', title=title)\n",
    "    val_loss_plot = hv.Curve(val_loss, label='Val Loss')\n",
    "\n",
    "    overlay = (loss_plot*val_loss_plot).opts(show_legend=True)\n",
    "    plot_list.append(overlay)\n",
    "    \n",
    "layout = hv.Layout(plot_list).cols(4)\n",
    "\n",
    "layout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5828f452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all predictions and all real data\n",
    "# predict the full traces\n",
    "\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# calculate the correlations per cell and average\n",
    "correlations_per_cell = [stat.spearmanr(predictions[:, el], y[:, el])[0] for el in np.arange(predictions.shape[1])]\n",
    "print(f'{correlations_per_cell}', f'Average correlation: {np.nanmean(correlations_per_cell):0.3f}')\n",
    "\n",
    "# print(predictions.shape)\n",
    "real = hv.Raster(y.T)\n",
    "# real.opts(width=1000, height=400)\n",
    "pred = hv.Raster(predictions.T)\n",
    "\n",
    "(real+pred).cols(1).opts(opts.Raster(tools=['hover'], width=1000, height=400, cmap='Inferno', ylabel='Cells', xlabel='Frames'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57c2ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the weights\n",
    "weights = model.layers[0].get_weights()[0]\n",
    "\n",
    "raster = hv.Raster(weights.T)\n",
    "raster.opts(width=800, height=600, tools=['hover'], cmap='RdBu', colorbar=True)\n",
    "raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2debab14-d124-4ee2-a729-52fa9a66bd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argwhere(np.array(correlations_per_cell) > 0.1).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222654d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a defined prediction\n",
    "\n",
    "# define the target cell\n",
    "target_cell = 197\n",
    "\n",
    "# get the real cell\n",
    "real_cell = y[:, target_cell]\n",
    "# get the prediction\n",
    "predicted_cell = predictions[:, target_cell]\n",
    "\n",
    "real_plot = hv.Curve(real_cell)\n",
    "real_plot.opts(width=1000, height=400, tools=['hover'])\n",
    "predicted_plot = hv.Curve(predicted_cell)\n",
    "\n",
    "real_plot*predicted_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2024c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the kernels for each variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66773d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# do a grid search on regularization\n",
    "\n",
    "# define the parameters to grid search on\n",
    "# l1_list = [0.001, 0.005, 0.01, 0.05, 0.1]\n",
    "# l2_list = [0.001, 0.005, 0.01, 0.05, 0.1]\n",
    "l1_list = [0.01, 0.05, 0.1, 0.5, 1, 5]\n",
    "l2_list = [0.1, 1, 10, 100, 1000]\n",
    "# l1_list = [0.01]\n",
    "# l2_list = [0.01]\n",
    "\n",
    "# define the number of iterations\n",
    "number_iterations = 10\n",
    "\n",
    "# define the base parameters\n",
    "params = {\n",
    "    'learning_rate': 0.001,\n",
    "    'validation_split': 0.3,\n",
    "    'batch_size': 100,\n",
    "    'epochs': 100,\n",
    "    'test_train_shuffle': False,\n",
    "    'verbose': 0,\n",
    "    'scaler': preproc.StandardScaler,\n",
    "}\n",
    "\n",
    "# allocate the output\n",
    "performance_list = []\n",
    "# set up the for loop\n",
    "for l1, l2 in product(l1_list, l2_list):\n",
    "    print(l1, l2)\n",
    "    # add the regularizations to the parameters\n",
    "    params['l1'] = l1\n",
    "    params['l2'] = l2\n",
    "    \n",
    "    # for all the iterations\n",
    "    for el in np.arange(number_iterations):\n",
    "        \n",
    "        # train the real net\n",
    "        params['sample_shuffle'] = None\n",
    "        performance, _, _, _, _, _ = train_test_glm_nn(current_features, current_calcium, **params)\n",
    "        # save the output\n",
    "        performance_list.append([l1, l2, performance[1], el, False])\n",
    "        # train the shuffle net\n",
    "        params['sample_shuffle'] = reverse_roll_shuffle\n",
    "        performance, _, _, _, _, _ = train_test_glm_nn(current_features, current_calcium, **params)\n",
    "        # save the output\n",
    "        performance_list.append([l1, l2, performance[1], el, True])\n",
    "# create a dataframe with the output\n",
    "performance_df = pd.DataFrame(performance_list, columns=['l1', 'l2', 'performance', 'iteration', 'shuffle'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f764ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results of the grid search\n",
    "\n",
    "# average across iterations\n",
    "average_results = performance_df.groupby(['l1', 'l2', 'performance', 'shuffle'], as_index=False).mean()\n",
    "\n",
    "# allocate memory for the output matrices\n",
    "real_data = np.zeros((len(l1_list), len(l2_list)))\n",
    "shuffle_data = np.zeros((len(l1_list), len(l2_list)))\n",
    "# run through the rows\n",
    "for idx, row in average_results.iterrows():\n",
    "    # get the indexes of the l1 and l2 values\n",
    "    l1_idx = np.argwhere(row['l1'] == np.array(l1_list))[0][0]\n",
    "    l2_idx = np.argwhere(row['l2'] == np.array(l2_list))[0][0]\n",
    "    \n",
    "    # assign the performance to the corresponding matrix\n",
    "    if row['shuffle']:\n",
    "        shuffle_data[l1_idx, l2_idx] = row['performance']\n",
    "    else:\n",
    "        real_data[l1_idx, l2_idx] = row['performance']\n",
    "        \n",
    "# plot\n",
    "real_plot = hv.Raster(real_data).opts(title='Real')\n",
    "shuffle_plot = hv.Raster(shuffle_data).opts(title='Shuffle')\n",
    "# print(real_data)\n",
    "yticks = [(idx + 0.5, el) for idx, el in enumerate(l1_list)]\n",
    "xticks = [(idx + 0.5, el) for idx, el in enumerate(l2_list)]\n",
    "\n",
    "layout = real_plot+shuffle_plot\n",
    "layout.cols(2).opts(opts.Raster(width=600, height=600, cmap='Reds', colorbar=True, tools=['hover'], xticks=xticks, yticks=yticks, xrotation=45, xlabel='L2', ylabel='L1'))\n",
    "layout\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6f137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd1cc76-09ad-4c3f-a8dc-cf50582e06a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
