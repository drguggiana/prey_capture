{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6892cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath(r'D:\\Code Repos\\prey_capture'))\n",
    "\n",
    "import paths\n",
    "import functions_bondjango as bd\n",
    "import functions_loaders as fl\n",
    "import functions_plotting as fp\n",
    "import snakemake_scripts.classify_batch as class_fun\n",
    "import yaml\n",
    "import processing_parameters\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import sklearn.metrics as smet\n",
    "import sklearn.linear_model as lin\n",
    "import sklearn.model_selection as mod\n",
    "import sklearn.svm as svm\n",
    "import sklearn.multiclass as multi\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "import h5py\n",
    "import functions_misc as fm\n",
    "import random\n",
    "import scipy.stats as stat\n",
    "import scipy.signal as signal\n",
    "import importlib\n",
    "\n",
    "import panel as pn\n",
    "import holoviews as hv\n",
    "from holoviews import opts, dim\n",
    "hv.extension('bokeh')\n",
    "from bokeh.resources import INLINE\n",
    "from bokeh import palettes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c7d15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the figure config\n",
    "importlib.reload(fp)\n",
    "importlib.reload(processing_parameters)\n",
    "# define the target saving path\n",
    "save_path = os.path.join(paths.figures_path, 'Regression_prototype')\n",
    "\n",
    "# define the printing mode\n",
    "save_mode = True\n",
    "# define the target document\n",
    "target_document = 'poster'\n",
    "# set up the figure theme\n",
    "fp.set_theme()\n",
    "# load the label dict\n",
    "label_dict = processing_parameters.label_dictionary\n",
    "variable_list = processing_parameters.variable_list\n",
    "units_dict = processing_parameters.tc_units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26599d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the target files\n",
    "\n",
    "# get the search string\n",
    "animal = processing_parameters.animal\n",
    "day = processing_parameters.day\n",
    "rig = processing_parameters.rig\n",
    "search_string = 'imaging:doric, slug:%s' % day\n",
    "\n",
    "# query the database for data to plot\n",
    "data_all = bd.query_database('analyzed_data', search_string)\n",
    "\n",
    "input_path = [el['analysis_path'] for el in data_all if ('preproc' in el['slug'] and animal.lower() in el['slug'])]\n",
    "print(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e64216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "data_list = []\n",
    "meta_list = []\n",
    "frame_list = []\n",
    "for idx, el in enumerate(input_path):\n",
    "    # get the trial timestamp (for frame calculations)\n",
    "    time_stamp = int(''.join(os.path.basename(el).split('_')[3:6]))\n",
    "    # also get the trial time signature, used as unique ID\n",
    "    # time_signature = int(''.join(os.path.basename(el).split('_')[0:6]))\n",
    "\n",
    "    try:\n",
    "        temp_data = pd.read_hdf(el, 'matched_calcium')\n",
    "        # temp_data['trial_id'] = time_signature\n",
    "        temp_data['id'] = data_all[idx]['id']\n",
    "\n",
    "        meta_list.append([data_all[idx][el1] for el1 in processing_parameters.meta_fields])\n",
    "        # try to load the motifs and latents\n",
    "        try:\n",
    "            latents = pd.read_hdf(el, 'latents')\n",
    "            motifs = pd.read_hdf(el, 'motifs')\n",
    "            egocentric_coords = pd.read_hdf(el, 'egocentric_coord')\n",
    "            egocentric_coords = egocentric_coords.loc[:, ['cricket_0_x', 'cricket_0_y']]\n",
    "            egocentric_coords = egocentric_coords.rename(columns={'cricket_0_x': 'ego_cricket_x',\n",
    "                                                                  'cricket_0_y': 'ego_cricket_y'})\n",
    "            # pad the latents\n",
    "            [latents, motifs] = fl.pad_latents([latents, motifs], temp_data.shape[0])\n",
    "            # concatenate with the main data\n",
    "            temp_data = pd.concat([temp_data, egocentric_coords, latents, motifs], axis=1)\n",
    "        except KeyError:\n",
    "            print(f'No latents in file {el}')\n",
    "        data_list.append(temp_data)\n",
    "        frame_list.append([time_stamp, 0, temp_data.shape[0]])\n",
    "    except KeyError:\n",
    "        # data_list.append([])\n",
    "        frame_list.append([time_stamp, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74252cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# define the regression parameters and perform the regression\n",
    "importlib.reload(class_fun)\n",
    "# parameters\n",
    "target_behavior = 'mouse_speed'\n",
    "time_shift = 0\n",
    "chunk = True\n",
    "repeats = 10\n",
    "shuffle_f = True\n",
    "chunk_size = 0.05\n",
    "chunk_size_shuffle = 0.05\n",
    "\n",
    "# get the cell labels\n",
    "labels = list(np.unique(np.array([el.columns for el in data_list]).flatten()))\n",
    "cells = [el for el in labels if 'cell' in el]\n",
    "# get the data\n",
    "sub_data = [el[[target_behavior]+cells] for el in data_list if (target_behavior in el.columns)]\n",
    "sub_data = pd.concat(sub_data)\n",
    "# get the parameter of interest\n",
    "parameter_working = sub_data.loc[:, target_behavior].to_numpy().copy()\n",
    "calcium_data_working = np.array(sub_data[cells].copy())\n",
    "\n",
    "# bin the parameter\n",
    "# parameter_working = stat.binned_statistic(parameter_working, parameter_working, bins=5, statistic='count')[2]\n",
    "bin_ranges = [np.percentile(parameter_working, el) for el in np.linspace(0, 100, 6)]\n",
    "parameter_working = stat.binned_statistic(parameter_working, parameter_working, bins=bin_ranges, statistic='count')[2]\n",
    "\n",
    "# allocate lists for the outputs\n",
    "pred_list = []\n",
    "coeff_list = []\n",
    "score_list = []\n",
    "# for all the repeats\n",
    "for reps in np.arange(repeats):\n",
    "    # create the regressor\n",
    "#     regressor = lin.TweedieRegressor(alpha=0.01, max_iter=5000, fit_intercept=False, power=0)\n",
    "#     regressor = AutoReg(lags=1)\n",
    "#     regressor = lin.MultiTaskElasticNetCV(max_iter=5000, l1_ratio=[.1, .5, .7, .9, .95, .99, 1], \n",
    "#                            n_jobs=7, alphas=[.0001, .01, .1, 1, 10, 100], fit_intercept=True)\n",
    "#                            n_jobs=7, alphas=[.001])\n",
    "#     regressor = lin.HuberRegressor(alpha=.1, max_iter=5000, fit_intercept=True, epsilon=10)\n",
    "#     regressor = svm.SVR(max_iter=10000, kernel='rbf', C=100)\n",
    "#     regressor = svm.LinearSVR(max_iter=5000, C=100)\n",
    "#     regressor = svm.LinearSVC(max_iter=1000, C=1000, class_weight='balanced')\n",
    "    regressor = svm.SVC(max_iter=10000, C=.1, class_weight='balanced', kernel='rbf')\n",
    "\n",
    "    # run the training function\n",
    "    linear_pred, coefficients, cc_score = class_fun.train_test_regressor(parameter_working,\n",
    "                                                                         calcium_data_working,\n",
    "                                                                         preprocessing.StandardScaler,\n",
    "                                                                         regressor,\n",
    "#                                                                          stat.spearmanr,\n",
    "#                                                                          smet.r2_score,\n",
    "                                                                         smet.balanced_accuracy_score,\n",
    "                                                                         shuffle_f=shuffle_f,\n",
    "                                                                         time_s=time_shift,\n",
    "                                                                         shuffle=False,\n",
    "                                                                         empty=False,\n",
    "                                                                         chunk=chunk,\n",
    "                                                                         chunk_size=chunk_size,\n",
    "                                                                         chunk_size_shuffle=chunk_size_shuffle,\n",
    "                                                                         test_size=0.3)\n",
    "\n",
    "    pred_list.append(linear_pred)\n",
    "    coeff_list.append(coefficients)\n",
    "    score_list.append(cc_score)\n",
    "print(f'Mean performance: {np.mean(score_list)}')\n",
    "# print(f'Correlation of the mean prediction: {stat.spearmanr(parameter_working[time_shift:], np.mean(pred_list, axis=0))[0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ed4d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq, location = np.histogram(parameter_working)\n",
    "hv.Scatter((location, freq)).opts(tools=['hover'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458ed4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.array(pred_list)[:, trial_range].shape)\n",
    "print(data_list[0].columns[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2072540f",
   "metadata": {},
   "source": [
    "# Regression example plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbf9259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the predictions\n",
    "\n",
    "importlib.reload(fp)\n",
    "plot_list = []\n",
    "\n",
    "# define the section of the trial to plot\n",
    "trial_range = np.arange(500)\n",
    "target_stretch = np.array(pred_list)[:, trial_range]\n",
    "target_time = []\n",
    "for idx, el in enumerate(data_list):\n",
    "    if idx == 0:\n",
    "        target_time.append(el['time_vector'].to_numpy())\n",
    "    else:\n",
    "        temp_time = el['time_vector'].to_numpy()\n",
    "        target_time.append(temp_time + target_time[-1][-1])\n",
    "            \n",
    "#     target_time = pd.concat([el['time_vector'] for el in data_list], axis=0)\n",
    "\n",
    "# x = np.arange(target_stretch.shape[1])\n",
    "x = np.concatenate(target_time, axis=0)[trial_range]\n",
    "# # for all the repeats\n",
    "# for reps in np.arange(repeats):\n",
    "#     plot = hv.Curve((x, pred_list[reps]))\n",
    "#     plot.opts(width=1000)\n",
    "#     plot_list.append(plot)\n",
    "# also plot the sem for each time point\n",
    "mean = np.mean(target_stretch, axis=0)\n",
    "# sem = stat.sem(pred_list, axis=0)\n",
    "std = np.std(target_stretch, axis=0)\n",
    "plot = hv.Spread((x, mean, std)).opts(color='blue')\n",
    "mean_plot = hv.Curve((x, mean))\n",
    "mean_plot.opts(color='blue', line_dash='dotted')\n",
    "plot_list.append(plot)\n",
    "plot_list.append(mean_plot)\n",
    "# and the original trace\n",
    "# param_plot = parameter_working[~np.isnan(parameter_working)].copy()\n",
    "param_plot = parameter_working.copy()\n",
    "if time_shift >= 0:\n",
    "    y = param_plot[time_shift:].copy()\n",
    "else:\n",
    "    y = param_plot[:time_shift].copy()\n",
    "y = y[trial_range]\n",
    "\n",
    "plot = hv.Curve((x, y))\n",
    "# plot.opts(line_dash='dotted', title=f'Mean cc: {np.mean(score_list):0.2f}')\n",
    "plot.opts(color='black')\n",
    "plot_list.append(plot)\n",
    "    \n",
    "plot_overlay = hv.Overlay(plot_list)\n",
    "plot_overlay.opts(opts.Curve(width=1000, height=300, xlabel='Time (s)', ylabel=' '.join((label_dict[target_behavior], units_dict[target_behavior]))))\n",
    "\n",
    "# assemble the file name\n",
    "save_name = os.path.join(save_path, '_'.join((target_document, 'Reg_ex')) + '.png')\n",
    "# save the figure\n",
    "fig = fp.save_figure(plot_overlay, save_name, fig_width=23, dpi=1200, fontsize=target_document, target='screen')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158d1478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the weights\n",
    "\n",
    "plot_list = []\n",
    "\n",
    "x = np.arange(coeff_list[0].shape[0])\n",
    "# for all the repeats\n",
    "for reps in np.arange(repeats):\n",
    "    plot = hv.Scatter((x, coeff_list[reps]))\n",
    "    plot.opts(width=1000, size=10)\n",
    "    plot_list.append(plot)\n",
    "# also plot the sem for each weight\n",
    "mean = np.mean(coeff_list, axis=0)\n",
    "# sem = stat.sem(coeff_list, axis=0)\n",
    "std = np.std(coeff_list, axis=0)\n",
    "plot = hv.ErrorBars((x, mean, std))\n",
    "plot.opts(title=f'Average weight std: {np.mean(std):0.2f}')\n",
    "plot_list.append(plot)\n",
    "\n",
    "hv.Overlay(plot_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01be48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot importance index\n",
    "\n",
    "# plot_list = []\n",
    "\n",
    "x = np.arange(coeff_list[0].shape[0])\n",
    "# # for all the repeats\n",
    "# for reps in np.arange(repeats):\n",
    "#     plot = hv.Scatter((x, coeff_list[reps]))\n",
    "#     plot.opts(width=1000, size=10)\n",
    "#     plot_list.append(plot)\n",
    "\n",
    "importance_list = []\n",
    "# for all of the repeats\n",
    "for reps in np.arange(repeats):\n",
    "    importance_list.append(coeff_list[reps]/np.sum(coeff_list[reps]))\n",
    "# also plot the sem for each weight\n",
    "importance = np.sum(coeff_list, axis=0)\n",
    "# sem = stat.sem(coeff_list, axis=0)\n",
    "# std = np.std(coeff_list, axis=0)\n",
    "# plot = hv.ErrorBars((x, mean, std))\n",
    "plot = hv.Curve((x, importance))\n",
    "plot.opts(title=f'Average weight std: {np.mean(std):0.2f}')\n",
    "plot_list.append(plot)\n",
    "\n",
    "hv.Overlay(plot_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5f9f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# calculate regression over time\n",
    "\n",
    "importlib.reload(class_fun)\n",
    "# parameters\n",
    "target_behavior = 'cricket_0_x'\n",
    "time_shift = [-40, -30, -20, -10, 0, 10, 20, 30, 40]\n",
    "# time_shift = [-10, 0, 10]\n",
    "# time_shift = [-15, -10, -5, 0, 5, 10, 15]\n",
    "chunk = True\n",
    "repeats = 100\n",
    "# shuffle = True\n",
    "chunk_size = 0.05\n",
    "chunk_size_shuffle = 0.05\n",
    "\n",
    "# get the cell labels\n",
    "labels = list(np.unique(np.array([el.columns for el in data_list]).flatten()))\n",
    "cells = [el for el in labels if 'cell' in el]\n",
    "# get the data\n",
    "sub_data = [el[[target_behavior]+cells] for el in data_list if (target_behavior in el.columns)]\n",
    "sub_data = pd.concat(sub_data)\n",
    "# get the parameter of interest\n",
    "parameter_working = sub_data.loc[:, target_behavior].to_numpy().copy()\n",
    "calcium_data_working = np.array(sub_data[cells].copy())\n",
    "\n",
    "# # bin the parameter\n",
    "# parameter_working = stat.binned_statistic(parameter_working, parameter_working, bins=10, statistic='count')[2]\n",
    "\n",
    "# allocate memory for the shuffles\n",
    "pred_shuffle = []\n",
    "coeff_shuffle = []\n",
    "score_shuffle = []\n",
    "# for real and shuffle\n",
    "for realvshuffle in np.arange(2):\n",
    "    \n",
    "    if realvshuffle == 0:\n",
    "        shuffle_f = False\n",
    "    else:\n",
    "        shuffle_f = True\n",
    "    # allocate lists for the outputs\n",
    "    pred_time = []\n",
    "    coeff_time = []\n",
    "    score_time = []\n",
    "    # for all the time shifts\n",
    "    for time_s in time_shift:\n",
    "        pred_list = []\n",
    "        coeff_list = []\n",
    "        score_list = []\n",
    "        # for all the repeats\n",
    "        for reps in np.arange(repeats):\n",
    "            # create the regressor\n",
    "            regressor = lin.TweedieRegressor(alpha=0.01, max_iter=5000, fit_intercept=False, power=0)\n",
    "#             regressor = lin.MultiTaskElasticNetCV(max_iter=5000, l1_ratio=[.1, .5, .7, .9, .95, .99, 1], \n",
    "#                                    n_jobs=7, alphas=[.0001, .01, .1, 1, 10, 100], fit_intercept=True)\n",
    "#                                    n_jobs=7, alphas=[.001])\n",
    "#             regressor = lin.HuberRegressor(alpha=.1, max_iter=5000, fit_intercept=True, epsilon=10)\n",
    "#             regressor = svm.SVR(max_iter=10000, kernel='rbf', C=100)\n",
    "#             regressor = svm.LinearSVR(max_iter=5000, C=100)\n",
    "#             regressor = svm.LinearSVC(max_iter=1000, C=1000, class_weight='balanced')\n",
    "#             regressor = svm.SVC(max_iter=1000, C=.1, class_weight='balanced', kernel='rbf')\n",
    "#             regressor = multi.OutputCodeClassifier(estimator=regressor, code_size=1, n_jobs=-1)\n",
    "\n",
    "            # run the training function\n",
    "            linear_pred, coefficients, cc_score = class_fun.train_test_regressor(parameter_working,\n",
    "                                                                                 calcium_data_working,\n",
    "                                                                                 preprocessing.StandardScaler,\n",
    "                                                                                 regressor,\n",
    "                                                                                 stat.spearmanr,\n",
    "#                                                                                  smet.r2_score,\n",
    "#                                                                                  smet.accuracy_score,\n",
    "                                                                                 time_s=time_s,\n",
    "                                                                                 shuffle_f=shuffle_f,\n",
    "                                                                                 empty=False,\n",
    "                                                                                 chunk=chunk,\n",
    "                                                                                 test_size=0.3,\n",
    "                                                                                 shuffle=False,\n",
    "                                                                                 chunk_size=chunk_size,\n",
    "                                                                                 chunk_size_shuffle=chunk_size_shuffle)\n",
    "                                                                                 \n",
    "            pred_list.append(linear_pred)\n",
    "            coeff_list.append(coefficients)\n",
    "            score_list.append(cc_score)\n",
    "        pred_time.append([np.mean(pred_list, axis=0), np.std(pred_list, axis=0)])\n",
    "        coeff_time.append([np.mean(coeff_list, axis=0), np.std(coeff_list, axis=0)])\n",
    "        score_time.append([np.mean(score_list), np.std(score_list)])\n",
    "#         # get the score\n",
    "#         param_plot = parameter_working[~np.isnan(parameter_working)].copy()\n",
    "#         if time_s >= 0:\n",
    "#             param_plot = param_plot[time_s:].copy()\n",
    "#         else:\n",
    "#             param_plot = param_plot[:time_s].copy()\n",
    "#         score_of_mean = stat.spearmanr(param_plot, pred_time[-1][0])[0]\n",
    "#         score_of_std = stat.spearmanr(param_plot, pred_time[-1][1])[0]\n",
    "#         score_time.append([score_of_mean, score_of_std])\n",
    "    pred_shuffle.append(pred_time)\n",
    "    coeff_shuffle.append(coeff_time)\n",
    "    score_shuffle.append(score_time)\n",
    "print(np.mean(score_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a154ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results over time\n",
    "\n",
    "plot_list = []\n",
    "mean = hv.Scatter((time_shift, [el[0] for el in score_shuffle[0]]), kdims='Time shift', vdims='Activity')\n",
    "std = hv.Spread((time_shift, [el[0] for el in score_shuffle[0]], [el[1] for el in score_shuffle[0]]))\n",
    "mean.opts(width=1000, color='red', title=target_behavior)\n",
    "std.opts(width=1000, color='red')\n",
    "plot_list.append(mean*std)\n",
    "mean = hv.Scatter((time_shift, [el[0] for el in score_shuffle[1]]))\n",
    "std = hv.Spread((time_shift, [el[0] for el in score_shuffle[1]], [el[1] for el in score_shuffle[1]]))\n",
    "mean.opts(width=1000, color='black')\n",
    "std.opts(width=1000, color='black')\n",
    "plot_list.append(mean*std)\n",
    "hv.Overlay(plot_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146afbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the autocorrelation of a given variable\n",
    "\n",
    "plot_list = []\n",
    "autocorrelation = signal.correlate(parameter_working, parameter_working, 'same')\n",
    "x = np.arange(-autocorrelation.shape[0]/2, autocorrelation.shape[0]/2)\n",
    "plot = hv.Curve((x, autocorrelation))\n",
    "plot.opts(width=1000, color='red')\n",
    "plot_list.append(plot)\n",
    "for reps in np.arange(repeats):\n",
    "    parameter_shuffle = class_fun.chunk_shuffle(parameter_working, chunk_size_shuffle=0.01)\n",
    "#     print(parameter_shuffle)\n",
    "    auto_shuffle = signal.correlate(parameter_shuffle, parameter_shuffle, 'same')\n",
    "    plot = hv.Curve((x, auto_shuffle))\n",
    "    plot.opts(width=1000, color='black')\n",
    "    plot_list.append(plot)\n",
    "\n",
    "hv.Overlay(plot_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-prey_capture] *",
   "language": "python",
   "name": "conda-env-.conda-prey_capture-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
