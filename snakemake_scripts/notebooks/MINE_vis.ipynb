{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994c1a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath(r'C:/Users/mmccann/repos/bonhoeffer/prey_capture/'))\n",
    "sys.path.insert(0, os.path.abspath(r'C:/Users/mmccann/repos/mine_pub'))\n",
    "\n",
    "import panel as pn\n",
    "import holoviews as hv\n",
    "from holoviews import opts, dim\n",
    "hv.extension('bokeh')\n",
    "from bokeh.resources import INLINE\n",
    "\n",
    "import paths\n",
    "import processing_parameters\n",
    "import functions_loaders as fl\n",
    "import functions_plotting as fp\n",
    "import functions_kinematic as fk\n",
    "from functions_tuning import normalize, calculate_dff\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import importlib\n",
    "import h5py\n",
    "import sklearn.preprocessing as preproc\n",
    "import joblib as jb\n",
    "from umap.umap_ import UMAP\n",
    "from rastermap import Rastermap\n",
    "\n",
    "from mine import Mine, MineData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d14236-f25b-49d0-8e04-6852dd041f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_extra_angles(ds):\n",
    "    \n",
    "    # Apply wrapping for directions to get range [0, 360]\n",
    "    ds['direction_wrapped'] = ds['direction'].copy()\n",
    "    mask = ds['direction_wrapped'] > -1000\n",
    "    ds.loc[mask, 'direction_wrapped'] = ds.loc[mask, 'direction_wrapped'].apply(fk.wrap)\n",
    "\n",
    "    # Now find the direction relative to the ground plane\n",
    "    try:\n",
    "        ds['direction_rel_ground'] = ds['direction_wrapped'].copy()\n",
    "        ds.loc[mask, 'direction_rel_ground'] = ds.loc[mask, 'direction_rel_ground'] + ds.loc[mask, 'head_roll']\n",
    "    except KeyError:\n",
    "        ds['direction_rel_ground'] = ds['direction_wrapped'].copy()\n",
    "\n",
    "    # Calculate orientation explicitly\n",
    "    if 'orientation' not in ds.columns:\n",
    "        ds['orientation'] = ds['direction_wrapped'].copy()\n",
    "        ds['orientation_rel_ground'] = ds['direction_rel_ground'].copy()\n",
    "        mask = ds['orientation'] > -1000\n",
    "        ds.loc[mask, 'orientation'] = ds.loc[mask, 'orientation'].apply(fk.wrap, bound=180)\n",
    "        ds.loc[mask, 'orientation_rel_ground'] = ds.loc[mask, 'orientation_rel_ground'].apply(fk.wrap, bound=180)\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e73a00-291c-4fbf-8ce7-fa2c7f40042d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### set up the figure config\n",
    "importlib.reload(fp)\n",
    "importlib.reload(processing_parameters)\n",
    "# define the target saving path\n",
    "save_path = os.path.join(paths.figures_path, 'MINE_vis')\n",
    "\n",
    "# define the printing mode\n",
    "save_mode = True\n",
    "# define the target document\n",
    "target_document = 'paper'\n",
    "# define ca activity type\n",
    "ca_type = 'fluor'    #'spikes' or 'fluor'\n",
    "# define if dropping ITI\n",
    "drop_ITI = False\n",
    "# set up the figure theme\n",
    "fp.set_theme()\n",
    "# load the label dict\n",
    "# label_dict = processing_parameters.label_dictionary\n",
    "# variable_list = processing_parameters.variable_list\n",
    "variable_list = processing_parameters.variable_list_free + processing_parameters.variable_list_visual\n",
    "label_dict = processing_parameters.wf_label_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c80112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "importlib.reload(processing_parameters)\n",
    "importlib.reload(fl)\n",
    "\n",
    "# get the paths from the database using search_list\n",
    "all_paths, all_queries = fl.query_search_list()\n",
    "mice = ['_'.join(os.path.basename(path).split('_')[7:10]) for path in all_paths[0]]\n",
    "print(all_paths)\n",
    "\n",
    "data_list = []\n",
    "# load the data\n",
    "for path, queries in zip(all_paths, all_queries):\n",
    "    \n",
    "    data, _, _  = fl.load_preprocessing(path, queries, latents_flag=False)\n",
    "    data_list.append(data)\n",
    "\n",
    "for i, (ds, mouse) in enumerate(zip(data_list[0], mice)):\n",
    "    ds.loc[:, 'mouse'] = mouse\n",
    "    \n",
    "    # drop activity not of correct type\n",
    "    cols_to_drop = [el for el in ds.columns if ('cell' in el) and (ca_type not in el)]\n",
    "    ds.drop(cols_to_drop, axis='columns', inplace=True)\n",
    "\n",
    "    # If using fluorescence data, calulate dF/F\n",
    "    if ca_type == 'fluor':\n",
    "        ds = calculate_dff(ds, baseline_type='iti', inplace=True)\n",
    "\n",
    "    # Do a quick calculation of orientation & dir/ori relative to ground\n",
    "    if ('direction_wrapped' in variable_list) and ('direction_wrapped' not in ds.columns):\n",
    "        ds = calculate_extra_angles(ds)\n",
    "\n",
    "    # Drop the ITI\n",
    "    if drop_ITI:\n",
    "        ds.drop(ds[ds['trial_num'] == 0].index, inplace=True)\n",
    "        ds.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    data_list[0][i] = ds\n",
    "\n",
    "# print(f'Number of trials: {len(data_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0c2026",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_rate = processing_parameters.wf_frame_rate\n",
    "# define the MINE parameters\n",
    "MINE_params = {\n",
    "    'train_fraction': 2.0/3,\n",
    "    'model_history': frame_rate*10,\n",
    "    'corr_cut': 0.2,\n",
    "    'compute_taylor': True,\n",
    "    # 'complexity': True,\n",
    "    'return_jacobians': True,\n",
    "    'taylor_look_ahead': frame_rate*5,\n",
    "    'taylor_pred_every': frame_rate*5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581d2bce",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# test MINE\n",
    "\n",
    "# # define the range of augmentations\n",
    "# augmentation_list = [-1, 0, 0.05]\n",
    "# # define the repeat number\n",
    "# repeat_number = 5\n",
    "\n",
    "# # allocate memory for the instances\n",
    "# results_array = np.zeros([len(augmentation_list), repeat_number])\n",
    "\n",
    "# # for all augmentation factors\n",
    "# for aug_idx, augmentation_factor in enumerate(augmentation_list):\n",
    "#     # for all reps\n",
    "#     for rep_idx, _ in enumerate(np.arange(1, repeat_number+1)):\n",
    "        # define the augmentation factor\n",
    "    #     augmentation_factor = 0.01\n",
    "# get the unique dates and mice\n",
    "unique_dates_mice = np.unique([(el.loc[0, 'mouse'], el.loc[0, 'datetime'][:10]) for el in data_list[0]], axis=0)\n",
    "\n",
    "# allocate a list to store the psid objects\n",
    "mine_list = []\n",
    "mouse_date_list = []\n",
    "calcium_list = []\n",
    "\n",
    "predictor_columns = variable_list\n",
    "\n",
    "# define the columns to exclude\n",
    "exclude_columns = ['mouse', 'datetime', 'motifs'] #+ ['latent_'+el for el in np.arange(15)]\n",
    "# for all the pairs\n",
    "for idx, pair in enumerate(unique_dates_mice):\n",
    "\n",
    "    print(f'Current mouse: {pair[0]}, current date: {pair[1]}, current index: {idx}')\n",
    "    # get the relevant trials\n",
    "    target_trials = [el for el in data_list[0] if (pair[0] in el.loc[0, 'mouse']) & (pair[1] in el.loc[0, 'datetime'])]\n",
    "\n",
    "    # concatenate\n",
    "    all_trials = pd.concat(target_trials, axis=0)\n",
    "    all_trials.dropna(inplace=True)\n",
    "\n",
    "    # check that all predictors are present\n",
    "    if not set(variable_list).issubset(all_trials.columns):\n",
    "        print(f'Mouse {pair[0]} on day {pair[1]} does not have all predictors')\n",
    "        continue\n",
    "#     print(all_trials.shape[1])\n",
    "#     continue\n",
    "\n",
    "    # get the calcium and predictors\n",
    "    # cell_columns = [el for el in all_trials.columns if 'cell' in el]\n",
    "    # cell_columns = [el for el in all_trials.columns if ('cell' in el) and ('spikes' in el)]\n",
    "    cell_columns = [el for el in all_trials.columns if ('cell' in el)]\n",
    "    \n",
    "#     predictor_columns = [el for el in all_trials.columns if ('cell' not in el) & (el not in exclude_columns)]\n",
    "#     predictor_columns = variable_list\n",
    "    calcium = all_trials[cell_columns].fillna(0).to_numpy()\n",
    "    predictors = all_trials[predictor_columns].fillna(0).to_numpy()\n",
    "    \n",
    "    if calcium.shape[0] < 600:\n",
    "        print(f'Mouse {pair[0]} and date {pair[1]} have less than 600 timepoints so skip')\n",
    "        continue\n",
    "    print(f'Timepoints: {calcium.shape[0]}, Cells: {calcium.shape[1]}, Predictors: {predictors.shape[1]}')\n",
    "    \n",
    "    # create the cell ids\n",
    "#     cell_ids = pd.DataFrame(np.arange(calcium.shape[1]), columns=['id'])\n",
    "#     cell_ids['mouse'] = pair[0]\n",
    "#     cell_ids['day'] = pair[1]\n",
    "    \n",
    "    cell_ids = np.array([[el, pair[0], pair[1]] for el in np.arange(calcium.shape[1])])\n",
    "    \n",
    "#     print(cell_ids.shape, cell_ids)\n",
    "#     raise ValueError\n",
    "#     continue\n",
    "    # remove nans\n",
    "#     predictors[np.isnan(predictors)] = 0\n",
    "\n",
    "#     plot1 = hv.Raster(predictors.T).opts(width=1200)\n",
    "#     plot2 = hv.Raster(calcium.T).opts(width=1200)\n",
    "#     break\n",
    "\n",
    "#     # z score them\n",
    "#     calcium = preproc.StandardScaler().fit_transform(calcium)\n",
    "#     predictors = preproc.StandardScaler().fit_transform(predictors)\n",
    "\n",
    "    # split into train and test for scaling and augmentation\n",
    "    train_frames = int(MINE_params['tt_split']*calcium.shape[0])\n",
    "    calcium_train = calcium[:train_frames, :]\n",
    "    calcium_test = calcium[train_frames:, :]\n",
    "    calcium_scaler = preproc.StandardScaler().fit(calcium_train)\n",
    "    calcium_train = calcium_scaler.transform(calcium_train)\n",
    "    calcium_test = calcium_scaler.transform(calcium_test)\n",
    "    # duplicate calcium for the augmented predictors\n",
    "#     if augmentation_factor == -1:\n",
    "#         MINE_params['tt_split'] = 2/3\n",
    "    calcium = np.concatenate([calcium_train, calcium_test], axis=0)\n",
    "#     else:\n",
    "#         MINE_params['tt_split'] = 4/5\n",
    "#         calcium = np.concatenate([calcium_train, calcium_train, calcium_test], axis=0)\n",
    "\n",
    "    predictors_train = predictors[:train_frames, :]\n",
    "    predictors_test = predictors[train_frames:, :]\n",
    "    predictors_scaler = preproc.StandardScaler().fit(predictors_train)\n",
    "    predictors_train = predictors_scaler.transform(predictors_train)\n",
    "    predictors_test = predictors_scaler.transform(predictors_test)\n",
    "#     if augmentation_factor == -1:\n",
    "    predictors = np.concatenate([predictors_train, predictors_test], axis=0)\n",
    "#     else:\n",
    "#         predictors_aug = predictors_train.copy() + np.random.randn(*predictors_train.shape)*augmentation_factor\n",
    "#         # add the augmented data\n",
    "#         predictors = np.concatenate([predictors_train, predictors_aug, predictors_test], axis=0)\n",
    "#     print(predictors_train[:5, 0])\n",
    "#     print(predictors_aug[:5, 0])\n",
    "#     raise ValueError\n",
    "    # create the MINE element\n",
    "#     miner = Mine(tt_split, window, , True, False, True, 25, 5)\n",
    "    miner = Mine(*MINE_params.values())\n",
    "    # run Mine\n",
    "    mine_data = miner.analyze_data(predictors.T, calcium.T)\n",
    "    \n",
    "    # create the path\n",
    "    save_path = os.path.join(r'Z:\\test_mine_wf\\fluor', f'{pair[0]}_{pair[1]}_mine.hdf5')\n",
    "    with h5py.File(save_path, 'w') as f:\n",
    "        mine_data.save_to_hdf5(f, overwrite=True)\n",
    "        f.create_dataset('cell_ids', data=cell_ids.astype('S'))\n",
    "    mine_list.append(mine_data)\n",
    "    mouse_date_list.append(pair)\n",
    "\n",
    "#             results_array[aug_idx, rep_idx] = mine_data.correlations_test[0]\n",
    "# (plot1+plot2).opts(shared_axes=False).cols(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a74af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # get the unique dates and mice\n",
    "# unique_dates_mice = np.unique([(el.loc[0, 'mouse'], el.loc[0, 'datetime'][:10]) for el in data_list[0]], axis=0)\n",
    "\n",
    "# # split the data into day/mouse packages\n",
    "# trial_list = []\n",
    "# # for all the pairs\n",
    "# for pair in unique_dates_mice:\n",
    "#     trials = [el for el in data_list[0] if (pair[0] in el.loc[0, 'mouse']) & (pair[1] in el.loc[0, 'datetime'])]\n",
    "#     trial_list.append([trials, pair[0], pair[1]])\n",
    "# # allocate a list to store the psid objects\n",
    "# # mine_list = []\n",
    "# # mouse_date_list = []\n",
    "\n",
    "# predictor_columns = variable_list\n",
    "\n",
    "# # define the columns to exclude\n",
    "# exclude_columns = ['mouse', 'datetime', 'motifs']\n",
    "# with jb.Parallel(n_jobs=-1) as parallel:\n",
    "    \n",
    "#     nn_outputs = parallel(jb.delayed(run_mine)(el[0], el[1], el[2]) for el in trial_list)\n",
    "#         for idx, el in enumerate(nn_outputs):\n",
    "#             performance_list.append([l1, l2, batch, el[0][1], idx, False])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41eeee26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the mine data\n",
    "# define the base directory\n",
    "base_path = r'Z:\\test_mine_wf\\fluor'\n",
    "# define the list of fields\n",
    "field_list = ['correlations_trained', 'correlations_test', 'taylor_scores', 'taylor_true_change', 'taylor_full_prediction', 'taylor_by_predictor', 'model_lin_approx_scores', 'me_scores', 'jacobians'] # 'nl_probs',\n",
    "\n",
    "# get the files in the directory\n",
    "files = os.listdir(base_path)\n",
    "# allocate memory for the mine data\n",
    "mine_list = []\n",
    "df_list = []\n",
    "mouse_date_list = []\n",
    "# for all the files\n",
    "for file in files:\n",
    "    \n",
    "    # get the mouse and day\n",
    "    name = file.split('_')\n",
    "    mouse = '_'.join(name[0:3])\n",
    "    day = name[3]\n",
    "    filepath = os.path.join(base_path, file)\n",
    "    # load the file\n",
    "    with h5py.File(filepath, mode='r') as f:    \n",
    "        current_mine = [np.array(f[el]) for el in field_list]\n",
    "\n",
    "    current_mine.append(None)\n",
    "    # create the mine object\n",
    "    current_mine = MineData(*current_mine)\n",
    "    \n",
    "    # store on a list\n",
    "    mine_list.append(current_mine)\n",
    "    mouse_date_list.append((mouse, day))\n",
    "\n",
    "# load a single file to get the predictor columns variable (crappy crappy hacky code)\n",
    "predictor_columns = variable_list\n",
    "# # get the paths from the database using search_list\n",
    "# all_paths, all_queries = fl.query_search_list()\n",
    "# # print(all_paths)\n",
    "\n",
    "# # data_list = []\n",
    "# # load the data\n",
    "# for path, queries in zip(all_paths[:1], all_queries[:1]):\n",
    "    \n",
    "#     data, _, _  = fl.load_preprocessing(path[:1], queries[:1])\n",
    "# #     data_list.append(data)\n",
    "    \n",
    "# # print(data)\n",
    "# predictor_columns = data[0].columns\n",
    "# predictor_columns = [el for el in predictor_columns if (el not in ['motifs', 'mouse', 'datetime']) & ('cell_' not in el)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a052e505-ad7f-46d6-9693-a1b827825ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_mine.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1cf19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print([el.taylor_scores.shape for el in mine_list])\n",
    "# print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6369df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the info for the cells that were fit,\n",
    "\n",
    "# get the number of predictors from MINE\n",
    "predictor_number = len(predictor_columns)\n",
    "scores_number = ((predictor_number**2) - predictor_number)/2 +predictor_number\n",
    "\n",
    "# allocate for the current trial\n",
    "mine_cells = []\n",
    "\n",
    "# for all date/mice\n",
    "for current_mine, (mouse, date) in zip(mine_list, mouse_date_list):\n",
    "    \n",
    "    # for all the cells\n",
    "    for cell, _ in enumerate(current_mine.correlations_trained):\n",
    "#         print(current_mine.taylor_scores[cell, :, 0].shape)\n",
    "\n",
    "        taylor_scores = current_mine.taylor_scores[cell, :, 0].flatten()\n",
    "        if taylor_scores.shape[0] < scores_number:\n",
    "            taylor_scores = np.concatenate([taylor_scores, np.zeros((int(scores_number - taylor_scores.shape[0])))*np.nan], axis=0)\n",
    "#         raise ValueError\n",
    "        \n",
    "        temp_list = [current_mine.correlations_test[cell], *taylor_scores, current_mine.model_lin_approx_scores[cell], current_mine.mean_exp_scores[cell], mouse, date]\n",
    "#         print(len(temp_list))\n",
    "        mine_cells.append(np.array(temp_list))\n",
    "\n",
    "# generate the column names\n",
    "interaction_names = []\n",
    "idx = 1\n",
    "for name1 in predictor_columns:\n",
    "    for name2 in predictor_columns[idx:]:\n",
    "#         if name1 == name2:\n",
    "#             continue\n",
    "        interaction_names.append(f'{name1}_{name2}')\n",
    "    idx += 1\n",
    "taylor_columns = predictor_columns + interaction_names\n",
    "columns = ['correlation_test', *taylor_columns, 'model_lin_approx_scores', 'mean_exp_score', 'mouse', 'date']\n",
    "# turn the output into a dataframe\n",
    "mine_cells = pd.DataFrame(mine_cells, columns=columns)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df8e6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the distribution of tunings\n",
    "# print(mine_cells.columns)\n",
    "# allocate the output\n",
    "mine_stats = []\n",
    "model_lin_approx_scores_distribution = []\n",
    "mean_exp_score_distribution = []\n",
    "\n",
    "for (mouse, date), data in mine_cells.groupby(['mouse', 'date']):\n",
    "    total_cells = data.shape[0]\n",
    "    # get the fit cells\n",
    "    selection_vector = ~np.isnan(data['correlation_test'].to_numpy().astype(float)) & (data['correlation_test'].to_numpy().astype(float) > MINE_params['threshold'])\n",
    "    fit_cells = data.iloc[selection_vector, :]\n",
    "    count_cells = fit_cells.shape[0]\n",
    "    fraction_fit = count_cells/total_cells\n",
    "    \n",
    "    mine_stats.append([mouse, date, count_cells, fraction_fit])\n",
    "    \n",
    "    model_lin_approx_scores_distribution.extend(fit_cells['model_lin_approx_scores'].to_numpy().astype(float))\n",
    "    mean_exp_score_distribution.extend(fit_cells['mean_exp_score'].to_numpy().astype(float))\n",
    "    \n",
    "mine_stats = pd.DataFrame(mine_stats, columns=['mouse', 'date', 'count', 'fraction'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9d738f",
   "metadata": {},
   "source": [
    "# Number of fitted cells "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95eb54f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the numbers of fit cells\n",
    "counts = mine_stats['count'].to_numpy().astype(int)\n",
    "location, freq  = np.unique(counts, return_counts=True)\n",
    "ticks = [(int(el), el) for el in np.arange(0, np.max(location), 1)]\n",
    "plot = hv.Scatter((location, freq))\n",
    "plot.opts(width=600, xrotation=45, xlabel='Cells fit', ylabel='Sessions', xticks=ticks, size=5)\n",
    "\n",
    "# assemble the file name\n",
    "save_name = os.path.join(save_path, '_'.join((target_document, 'Cells_fit')) + '.png')\n",
    "# save the figure\n",
    "fig = fp.save_figure(plot, save_name, fig_width=15, dpi=1200, fontsize=target_document, target='screen')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412a692f",
   "metadata": {},
   "source": [
    "# Fraction of fitted cells "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6608e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the fractions\n",
    "\n",
    "freq, location  = np.histogram(mine_stats['fraction'].to_numpy().astype(float)*100, bins=20)\n",
    "ticks = [(int(el), int(el)) for el in np.arange(0, np.max(location), 1)]\n",
    "plot = hv.Scatter((location, freq))\n",
    "plot.opts(xrotation=45, xlabel='Percentage of cells fit', ylabel='Sessions', width=600, xticks=ticks, size=5)\n",
    "\n",
    "# assemble the file name\n",
    "save_name = os.path.join(save_path, '_'.join((target_document, 'Cells_perc')) + '.png')\n",
    "# save the figure\n",
    "fig = fp.save_figure(plot, save_name, fig_width=15, dpi=1200, fontsize=target_document, target='screen')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c4e753",
   "metadata": {},
   "source": [
    "# Nonlinear prob vs second order "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb787a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot complexity\n",
    "\n",
    "plot = hv.Scatter((model_lin_approx_scores_distribution, mean_exp_score_distribution))\n",
    "plot.opts(xrotation=45, ylabel='R2 2nd order model fit', xlabel='R2 1st order model fit', width=400, xlim=(0,1), ylim=(0,1))\n",
    "\n",
    "# assemble the file name\n",
    "save_name = os.path.join(save_path, '_'.join((target_document, 'Nonlinear_scatter')) + '.png')\n",
    "# save the figure\n",
    "fig = fp.save_figure(plot, save_name, fig_width=15, dpi=1200, fontsize='poster', target='screen')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c52f02-00c4-4728-9eb3-286636ee42c9",
   "metadata": {},
   "source": [
    "# Linear Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3d9bfd-2b5c-4824-bfc3-77ffb6ffb784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of nonlinear probability\n",
    "\n",
    "# plot = hv.Scatter((nl_probability_distribution, mean_exp_score_distribution))\n",
    "\n",
    "freq, location  = np.histogram(model_lin_approx_scores_distribution)\n",
    "plot = hv.Bars((location, freq))\n",
    "plot.opts(xrotation=45, ylabel='Cells', xlabel='Linear probability', width=400, xformatter='%.2f')\n",
    "\n",
    "# assemble the file name\n",
    "save_name = os.path.join(save_path, '_'.join((target_document, 'Linear_prob')) + '.png')\n",
    "# save the figure\n",
    "fig = fp.save_figure(plot, save_name, fig_width=10, dpi=1200, fontsize='paper', target='screen')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650d649b",
   "metadata": {},
   "source": [
    "# Nonlinear probability "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f411f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of nonlinear probability\n",
    "\n",
    "# plot = hv.Scatter((nl_probability_distribution, mean_exp_score_distribution))\n",
    "\n",
    "freq, location  = np.histogram(mean_exp_score_distribution)\n",
    "plot = hv.Bars((location, freq))\n",
    "plot.opts(xrotation=45, ylabel='Cells', xlabel='Nonlinear probability', width=400, xformatter='%.2f')\n",
    "\n",
    "# assemble the file name\n",
    "save_name = os.path.join(save_path, '_'.join((target_document, 'Nonlinear_prob')) + '.png')\n",
    "# save the figure\n",
    "fig = fp.save_figure(plot, save_name, fig_width=10, dpi=1200, fontsize='paper', target='screen')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31b4223",
   "metadata": {},
   "source": [
    "# Percentage of nonlinear weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1881a664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of significant interaction weights across the dataset\n",
    "\n",
    "interaction_weights = mine_cells.loc[:, interaction_names].copy().astype(float).dropna()\n",
    "\n",
    "interaction_weights[interaction_weights<0.05] = 0\n",
    "\n",
    "# print((interaction_weights>0).sum(axis=0))\n",
    "\n",
    "interaction_matrix = np.zeros((len(variable_list), len(variable_list)))\n",
    "for idx0, feature0 in enumerate(variable_list):\n",
    "    for idx1, feature1 in enumerate(variable_list):\n",
    "        interaction_feature = feature0+'_'+feature1\n",
    "        if interaction_feature not in interaction_names:\n",
    "            continue\n",
    "        interaction_matrix[idx0, idx1] = 100*(interaction_weights[interaction_feature]>0).sum(axis=0)/(interaction_weights.shape[0])\n",
    "    \n",
    "interaction_matrix[interaction_matrix==0] = np.nan\n",
    "ticks = [(idx+0.5, label_dict[el]) for idx, el in enumerate(variable_list)]\n",
    "plot = hv.Raster(interaction_matrix)\n",
    "plot.opts(width=900, height=600, tools=['hover'], cmap='Viridis', xticks=ticks, xrotation=45, yticks=ticks, xlabel='', ylabel='', colorbar=True)\n",
    "plot\n",
    "\n",
    "# assemble the file name\n",
    "save_name = os.path.join(save_path, '_'.join((target_document, 'Interaction_fraction')) + '.png')\n",
    "# save the figure\n",
    "fig = fp.save_figure(plot, save_name, fig_width=10, dpi=1200, fontsize='paper', target='screen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ed9883",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(((interaction_weights>0.05).sum(axis=1)==1).sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c7bbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the tunings\n",
    "\n",
    "# eps = 1\n",
    "# min_samples = 3\n",
    "perc = 99\n",
    "\n",
    "tunings = []\n",
    "# get the tunings\n",
    "for (mouse, date), data in mine_cells.groupby(['mouse', 'date']):\n",
    "    current_tunings = data[predictor_columns].copy()\n",
    "    selection_vector = ~np.isnan(data['correlation_test'].to_numpy().astype(float)) & (data['correlation_test'].to_numpy().astype(float) > MINE_params['threshold'])\n",
    "    current_tunings = current_tunings.iloc[selection_vector, :].to_numpy().astype(float)\n",
    "    if selection_vector.sum() == 0:\n",
    "        continue\n",
    "#     current_tunings = np.abs(current_tunings)\n",
    "#     current_tunings[current_tunings>np.percentile(current_tunings, perc)] = np.percentile(current_tunings, perc)\n",
    "#     current_tunings[current_tunings<np.percentile(current_tunings, 100-perc)] = np.percentile(current_tunings, 100-perc)\n",
    "    current_tunings[current_tunings<0.05] = 0\n",
    "    \n",
    "    tunings.extend(current_tunings)\n",
    "tunings = np.array(tunings)\n",
    "tunings[np.isnan(tunings)] = 0\n",
    "raw_tunings = tunings.copy()\n",
    "tunings = preproc.StandardScaler().fit_transform(tunings)\n",
    "# clusters = cluster.DBSCAN(eps=eps, min_samples=min_samples).fit_predict(tunings)\n",
    "# clusters = cluster.AgglomerativeClustering(distance_threshold=5, n_clusters=None).fit_predict(tunings)\n",
    "# print(clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7b2728",
   "metadata": {},
   "source": [
    "# Tunings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac4898b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_tunings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb349ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the tunings from MINE\n",
    "\n",
    "ticks = [(idx+0.5, label_dict[el]) for idx, el in enumerate(predictor_columns)]\n",
    "plot_matrix = raw_tunings.copy()\n",
    "plot_matrix[plot_matrix<0.05] = 0\n",
    "# model = Rastermap(n_components=2, n_clusters =30, n_PCs=200, init='pca')\n",
    "model = Rastermap(n_clusters =2, n_PCs=200)\n",
    "model.fit(plot_matrix)\n",
    "plot_matrix = plot_matrix[model.isort, :]\n",
    "plot = hv.Raster(plot_matrix)\n",
    "plot.opts(width=1000, height=600, cmap='RdBu_r', tools=['hover'], clim=(-1, 1), xticks=ticks, xrotation=45, xlabel='', ylabel='Cells', colorbar=True)\n",
    "\n",
    "# assemble the file name\n",
    "save_name = os.path.join(save_path, '_'.join((target_document, 'MINE_tunings')) + '.png')\n",
    "# save the figure\n",
    "fig = fp.save_figure(plot, save_name, fig_width=10, dpi=1200, fontsize='paper', target='screen')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43003fd",
   "metadata": {},
   "source": [
    "# UMAP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d1e572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform umap on the fit cell tuning\n",
    "reducer1 = UMAP(min_dist=0.1, n_neighbors=20)\n",
    "embedded_data1 = reducer1.fit_transform(tunings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd471575",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_field = 'head_yaw'\n",
    "perc = 99\n",
    "\n",
    "label_idx = [idx for idx, el in enumerate(predictor_columns) if target_field == el]\n",
    "raw_labels = tunings[:, label_idx]\n",
    "\n",
    "# raw_labels = np.abs(raw_labels)\n",
    "\n",
    "raw_labels[raw_labels>np.percentile(raw_labels, perc)] = np.percentile(raw_labels, perc)\n",
    "raw_labels[raw_labels<np.percentile(raw_labels, 100-perc)] = np.percentile(raw_labels, 100-perc)\n",
    "\n",
    "plot_data = np.concatenate([embedded_data1, raw_labels.reshape((-1, 1))], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba80afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# umap plot of the fit cell tunings\n",
    "umap_plot = hv.Scatter(plot_data, vdims=['Dim 2', 'Parameter'], kdims=['Dim 1'])\n",
    "# umap_plot = hv.HexTiles(umap_data, kdims=['Dim 1', 'Dim 2'])\n",
    "umap_plot.opts(colorbar=True, color='Parameter', cmap='Spectral_r', tools=['hover'], alpha=1)\n",
    "umap_plot.opts(width=1200, height=1000, size=5)\n",
    "\n",
    "# assemble the file name\n",
    "save_name = os.path.join(save_path, '_'.join((target_document, 'MINE_UMAP')) + '.png')\n",
    "# save the figure\n",
    "fig = fp.save_figure(umap_plot, save_name, fig_width=15, dpi=1200, fontsize=target_document, target='screen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ea34c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the importances per neuron\n",
    "\n",
    "# print(len(mine_data.taylor_by_predictor))\n",
    "# print(mine_data.taylor_full_prediction[0].shape)\n",
    "# print(mine_data.taylor_true_change.shape)\n",
    "# print(mine_data.__dict__.keys())\n",
    "\n",
    "mine_data = mine_list[0]\n",
    "\n",
    "print(f'Number of cells: {calcium.shape[1]}, Number of timepoints: {calcium.shape[0]}, Number of predictors: {predictors.shape[1]}')\n",
    "for el in mine_data.__dict__.keys():\n",
    "    try:\n",
    "        print(el, getattr(mine_data, el).shape)\n",
    "    except AttributeError:\n",
    "        print(el)\n",
    "\n",
    "# taylor scores is predictors + n choose 2 predictors, i.e. the interaction terms\n",
    "# first dimension of taylor_true_change, taylor_full_prediction and taylor_by_predictor is the fitted cells\n",
    "# the second dimension in the Taylor metrics comes from (total time - past - future_prediction)/frame_step , see TaylorDecomp\n",
    "\n",
    "# second dimension in jacobians is predictors x timepoints set for calculation\n",
    "\n",
    "        \n",
    "# hv.Raster(np.squeeze(mine_data.taylor_by_predictor[0][1, :, :])).opts(width=800, height=800)\n",
    "# hv.Curve(mine_data.taylor_full_prediction[0]).opts(width=1200)\n",
    "# hv.Curve(np.squeeze(mine_data.taylor_by_predictor[0][:, 20, 0])).opts(width=1200)\n",
    "ticks = [(idx+0.5, el) for idx, el in enumerate(predictor_columns)]\n",
    "\n",
    "plot = hv.Raster(np.squeeze(mine_data.taylor_scores[:, :len(predictor_columns), 0]))\n",
    "# plot = hv.Raster(np.squeeze(mine_data.taylor_scores[:, :, 0]))\n",
    "plot.opts(width=1200, height=800, xticks=ticks, xrotation=90, ylabel='Cells', xlabel='', tools=['hover'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9a5ed7",
   "metadata": {},
   "source": [
    "# Single cell interactions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f885083b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the interaction predictors for a given cell\n",
    "\n",
    "predictor_idx = [idx for idx, el in enumerate(variable_list)]\n",
    "# define the target cell\n",
    "mine_data = mine_list[-1]\n",
    "target_cell = 0\n",
    "print(mine_data.taylor_scores.shape)\n",
    "# get the taylor coefficients\n",
    "current_taylor = mine_data.taylor_scores[target_cell, len(predictor_columns):, 0]\n",
    "# allocate the plotting matrix\n",
    "plot_matrix = np.zeros((len(predictor_columns), len(predictor_columns)))\n",
    "# fill the triangular matrix\n",
    "plot_matrix[np.tri(len(predictor_columns), k=-1, dtype=bool).T] = current_taylor\n",
    "diagonal = np.diag(np.ones((len(predictor_columns)))).astype(bool)\n",
    "plot_matrix[diagonal] = mine_data.taylor_scores[target_cell, :len(predictor_columns), 0]\n",
    "plot_matrix[plot_matrix==0] = np.nan\n",
    "\n",
    "plot_matrix = plot_matrix[predictor_idx, :]\n",
    "plot_matrix = plot_matrix[:, predictor_idx]\n",
    "yticks = [(idx+0.5, label_dict[el]) for idx, el in enumerate(variable_list)]\n",
    "clim = (-np.nanmax(plot_matrix), np.nanmax(plot_matrix))\n",
    "plot = hv.Raster(plot_matrix)\n",
    "plot.opts(width=900, height=600, cmap='Spectral_r', yticks=yticks, ylabel='', xticks=yticks, xlabel='', xrotation=45, tools=['hover'], colorbar=True)\n",
    "plot.opts(clim=clim)\n",
    "plot\n",
    "\n",
    "# assemble the file name\n",
    "save_name = os.path.join(save_path, '_'.join((target_document, 'Jacobian_example')) + '.png')\n",
    "# save the figure\n",
    "fig = fp.save_figure(plot, save_name, fig_width=10, dpi=1200, fontsize='paper', target='screen')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a43c2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a particular cell\n",
    "\n",
    "# define the target cell\n",
    "target_cell = 0\n",
    "# define the shift factor\n",
    "shift_factor = 0.1\n",
    "number_window = MINE_params['window']\n",
    "\n",
    "# get the cells data\n",
    "current_cell = mine_data.jacobians[target_cell, :]\n",
    "current_cell = current_cell.reshape([1, number_window, predictor_number], order='F').transpose([2, 1, 0]).reshape([predictor_number, -1], order='F')\n",
    "# normalize per predictor\n",
    "# current_cell = ((current_cell - current_cell.min(axis=0))/(current_cell.max(axis=0) - current_cell.min(axis=0)))\n",
    "# allocate the plot list\n",
    "plot_list = []\n",
    "tick_list = []\n",
    "\n",
    "predictor_idx = [idx for idx, el in enumerate(predictor_columns) if el in variable_list]\n",
    "counter = 0\n",
    "\n",
    "xticks = [(int(number_window)-int(el), str(el)) for el in np.arange(0, number_window, 10, dtype=int)]\n",
    "print(xticks)\n",
    "# for all the predictors\n",
    "for idx, predictor in enumerate(current_cell):\n",
    "    if idx not in predictor_idx:\n",
    "        continue\n",
    "    plot = hv.Curve(predictor + counter*shift_factor)\n",
    "    plot.opts(width=500, color='red')\n",
    "    plot_list.append(plot)\n",
    "    tick_list.append(((counter)*shift_factor, label_dict[variable_list[counter]]))\n",
    "    counter += 1\n",
    "\n",
    "overlay = hv.Overlay(plot_list)\n",
    "overlay.opts(height=800, yticks=tick_list, xrotation=45, xlabel='Time', ylabel='', xticks=xticks)\n",
    "overlay\n",
    "\n",
    "# assemble the file name\n",
    "save_name = os.path.join(save_path, '_'.join((target_document, 'Trace_example')) + '.png')\n",
    "# save the figure\n",
    "fig = fp.save_figure(overlay, save_name, fig_width=10, dpi=1200, fontsize=target_document, target='screen')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b0f800-265b-440e-a243-b2179f2f1268",
   "metadata": {},
   "source": [
    "# Single Cell fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f5f42c-8b35-4a18-92ee-18826eca7c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_traces = []\n",
    "mine_pred = []\n",
    "\n",
    "for current_mine, (mouse, date) in zip(mine_list, mouse_date_list):\n",
    "    target_trials = [el for el in data_list[0] if (mouse in el.loc[0, 'mouse']) & (date in el.loc[0, 'datetime'])]\n",
    "    all_trials = pd.concat(target_trials, axis=0)\n",
    "    all_trials.dropna(inplace=True)\n",
    "\n",
    "    selection_vector = ~np.isnan(np.array(current_mine.correlations_test).astype(float)) & \\\n",
    "                        (np.array(current_mine.correlations_test).astype(float) > MINE_params['threshold']) & \\\n",
    "                        (np.array(current_mine.model_lin_approx_scores).astype(float) >= -1) & \\\n",
    "                        (np.array(current_mine.mean_exp_scores).astype(float) >= -1)\n",
    "    \n",
    "    fit_cells = np.argwhere(selection_vector).flatten()\n",
    "    fit_cols = [el for el in all_trials.columns for cell in fit_cells if ('cell_{:04d}'.format(cell) in el)]\n",
    "    \n",
    "    for idx, (cell, col) in enumerate(zip(fit_cells, fit_cols)):\n",
    "        ca_traces.append(all_trials[col].to_numpy().flatten())\n",
    "        mine_pred.append(current_mine.taylor_full_prediction[idx, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dfff79-cb36-40ce-be89-bd8659323432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the target cell\n",
    "target_cell = 0\n",
    "\n",
    "# get the cells data\n",
    "current_cell_pred = mine_pred[target_cell]\n",
    "current_cell_calcium = ca_traces[target_cell][MINE_params['window']::MINE_params['interval']]\n",
    "\n",
    "current_cell_pred_norm = normalize(current_cell_pred)\n",
    "current_cell_pred_norm -= np.mean(current_cell_pred_norm)\n",
    "current_cell_calcium_norm = normalize(current_cell_calcium)\n",
    "\n",
    "pred = hv.Curve((np.arange(len(current_cell_pred_norm)), current_cell_pred_norm)).opts(alpha=0.75)\n",
    "calcium = hv.Curve((np.arange(len(current_cell_calcium_norm)), current_cell_calcium_norm)).opts(alpha=0.75)\n",
    "overlay = pred * calcium\n",
    "overlay.opts(width=600, height=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63e242b-18b8-411b-bb97-9321d0214bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = mine_cells.loc[~np.isnan(np.array(mine_cells.correlation_test).astype(float)) & \\\n",
    "                    (np.array(mine_cells.correlation_test).astype(float) > MINE_params['threshold']) & \\\n",
    "                    (np.array(mine_cells.model_lin_approx_scores).astype(float) >= -1) & \\\n",
    "                    (np.array(mine_cells.mean_exp_score).astype(float) >= -1), :].reset_index()\n",
    "a = a.sort_values('correlation_test', ascending=False)[['correlation_test', 'model_lin_approx_scores', 'mean_exp_score']]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8381660-57d2-48f9-a567-3851e2959f52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
