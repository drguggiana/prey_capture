{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "described-oriental",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "# import pixiedust\n",
    "import logging\n",
    "logging.getLogger(\"param.Dimension\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"param.ParameterizedMetaclass\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"param.SpreadPlot\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"param.CurvePlot\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"param.AdjointLayout\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"param.HoloMap\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"param.OverlayPlot\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"param.BarPlot\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"param.ErrorPlot\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"param.RasterPlot\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"param.Layout\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"param.PointPlot\").setLevel(logging.CRITICAL)\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath(r'D:\\Code Repos\\prey_capture'))\n",
    "\n",
    "import panel as pn\n",
    "import holoviews as hv\n",
    "from holoviews import opts, dim\n",
    "hv.extension('bokeh')\n",
    "from bokeh.resources import INLINE\n",
    "\n",
    "import paths\n",
    "import functions_bondjango as bd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import functions_plotting as fp\n",
    "import functions_data_handling as fd\n",
    "import functions_kinematic as fk\n",
    "from scipy.stats import sem\n",
    "import sklearn.decomposition as decomp\n",
    "import umap\n",
    "import sklearn.mixture as mix\n",
    "from scipy.stats import sem\n",
    "import pickle as pk\n",
    "import itertools as it\n",
    "import processing_parameters\n",
    "\n",
    "from pprint import pprint as pp\n",
    "\n",
    "# # define the name to be used for the saved figures\n",
    "# save_name = 'acrossTrials'\n",
    "# line_width = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hundred-lodging",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the analyzed VAME data\n",
    "# define the type of VAME\n",
    "vame_type = 'prey_capture_15'\n",
    "# define the folder\n",
    "target_folder = os.path.join(r'J:\\Drago Guggiana Nilo\\Prey_capture\\temp_VAME', vame_type)\n",
    "\n",
    "# get a list of the result folders\n",
    "result_list = os.listdir(os.path.join(target_folder,'results'))\n",
    "\n",
    "# pp(result_list)\n",
    "# load the latent and labels\n",
    "label_list = [np.load(os.path.join(target_folder,'results',el,'VAME','kmeans-15','15_km_label_'+el+'.npy')) \n",
    "              for el in result_list]\n",
    "latent_list = [np.load(os.path.join(target_folder,'results',el,'VAME','kmeans-15','latent_vector_'+el+'.npy')) \n",
    "               for el in result_list]\n",
    "\n",
    "# load the aligned data\n",
    "data_list = [np.load(os.path.join(target_folder,'data',el,el+'-PE-seq.npy')) for el in result_list]\n",
    "\n",
    "# get the motif sorting\n",
    "motif_sort = np.array(processing_parameters.motif_sort)\n",
    "motif_revsort = np.array(processing_parameters.motif_revsort)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governmental-consistency",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # UMAP embedding of the VAME data\n",
    "\n",
    "# # compile the data\n",
    "# compiled_latent = np.vstack(latent_list)\n",
    "# # embed using UMAP\n",
    "# # original parameters 0.5 and 10\n",
    "# # 0.1 and 30 also works\n",
    "# # 0.05 and 30 works too\n",
    "# reducer = umap.UMAP(min_dist=0.5, n_neighbors=10)\n",
    "# embedded_data = reducer.fit_transform(compiled_latent)\n",
    "\n",
    "# # save the embedding\n",
    "# np.save(os.path.join(target_folder, 'UMAP_result'), embedded_data)\n",
    "\n",
    "# # generate the model name\n",
    "# model_name = os.path.join(target_folder, 'UMAP_model.pk')\n",
    "# # save the estimator\n",
    "# with open(model_name, 'wb') as file:\n",
    "#     pk.dump(reducer, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "false-female",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-existing embedding\n",
    "embedded_data = np.load(os.path.join(target_folder, 'UMAP_result.npy'))\n",
    "\n",
    "# generate the model name\n",
    "model_name = os.path.join(target_folder, 'UMAP_model.pk')\n",
    "with open(model_name, 'rb') as file:\n",
    "    reducer = pk.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valued-italy",
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Calculate average start and end of the motifs\n",
    "\n",
    "# plot the average and standard deviation of each motif\n",
    "# initialize a list for the averages and stds\n",
    "motif_average = []\n",
    "motif_sem = []\n",
    "# for all the files\n",
    "for idx, data in enumerate(data_list):\n",
    "    # get the labels\n",
    "    labels = label_list[idx]\n",
    "    # cut the data by the time window used\n",
    "#     data = data[:, 15:-15]\n",
    "    # initialize a list for the starts and ends averages and stds\n",
    "    temp_average = np.zeros((latent_list[0].shape[1], data.shape[0], 2))\n",
    "    temp_sem = np.zeros((latent_list[0].shape[1], data.shape[0], 2))\n",
    "\n",
    "    # for all motifs\n",
    "    for motif in np.arange(latent_list[0].shape[1]):\n",
    "        # find all the starts and ends for this motif\n",
    "        m_idx = (labels==motif).astype(int)\n",
    "        starts = np.argwhere(np.diff(m_idx)==1) + 1\n",
    "        ends = np.argwhere(np.diff(m_idx)==-1) + 1\n",
    "        \n",
    "        # average them\n",
    "        temp_average[motif, :, 0] = np.nanmean(data[:, starts], axis=1).flatten()\n",
    "        temp_average[motif, :, 1] = np.nanmean(data[:, ends], axis=1).flatten()\n",
    "        \n",
    "        temp_sem[motif, :, 0] = sem(data[:, starts], axis=1, nan_policy='omit').flatten()\n",
    "        temp_sem[motif, :, 1] = sem(data[:, ends], axis=1, nan_policy='omit').flatten()\n",
    "    # save the file data\n",
    "    motif_average.append(temp_average)\n",
    "    motif_sem.append(temp_sem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exotic-publisher",
   "metadata": {
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the distributions of lengths for the motifs\n",
    "\n",
    "# initialize a list for the durations\n",
    "motif_duration = []\n",
    "# also allocate a list for the motif location\n",
    "motif_location = []\n",
    "# get the number of motifs\n",
    "num_motifs = latent_list[0].shape[1]\n",
    "# for all the files\n",
    "for idx, data in enumerate(data_list):\n",
    "    # get the labels\n",
    "    labels = label_list[idx]\n",
    "    \n",
    "    # initialize a list for the durations\n",
    "    motif_perfile = []\n",
    "    location_perfile = []\n",
    "    # for all motifs\n",
    "    for motif in np.arange(num_motifs):\n",
    "        # find all the starts and ends for this motif\n",
    "        m_idx = (labels==motif).astype(int)\n",
    "        starts = np.argwhere(np.diff(m_idx)==1) + 1\n",
    "        ends = np.argwhere(np.diff(m_idx)==-1) + 1\n",
    "\n",
    "        # skip if any of the arrays is empty\n",
    "        if (starts.shape[0] == 0) or (ends.shape[0] == 0):\n",
    "            motif_perfile.append(np.empty((0, 1)))\n",
    "            location_perfile.append(np.empty((0, 1)))\n",
    "            continue\n",
    "        # trim the starts and ends based on ordering\n",
    "        if starts[0] > ends[0]:\n",
    "            if ends.shape[0] > 1:\n",
    "                ends = ends[1:]\n",
    "            else:\n",
    "                motif_perfile.append(np.empty((0, 1)))\n",
    "                location_perfile.append(np.empty((0, 1)))\n",
    "                continue\n",
    "        if starts[-1] > ends [-1]:\n",
    "            if starts.shape[0] > 1:\n",
    "                starts = starts[:-2]\n",
    "            else:\n",
    "                motif_perfile.append(np.empty((0, 1))) \n",
    "                location_perfile.append(np.empty((0, 1)))\n",
    "                continue\n",
    "            \n",
    "        # trim the starts or ends depending on size\n",
    "        if starts.shape[0] > ends.shape[0]:\n",
    "            starts = starts[:-2]\n",
    "        if ends.shape[0] > starts.shape[0]:\n",
    "            ends = ends[1:]\n",
    "        # make sure the ends are always bigger than the starts\n",
    "        try: \n",
    "            assert np.all((ends-starts)>0) \n",
    "        except AssertionError:\n",
    "            print(str(idx)+'_'+str(motif))\n",
    "            print(starts)\n",
    "            print(ends)\n",
    "            \n",
    "        # get the duration of each motif instance and save\n",
    "        motif_perfile.append(ends-starts) \n",
    "        location_perfile.append(starts)\n",
    "    # save the start and origin\n",
    "    motif_location.append(location_perfile)\n",
    "    # save the file data\n",
    "    motif_duration.append(motif_perfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "packed-starter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the durations\n",
    "\n",
    "# allocate a list for the bars\n",
    "bar_list = []\n",
    "# also allocate a list for the median durations\n",
    "median_duration = []\n",
    "# for all the motifs\n",
    "for motif in np.arange(num_motifs):\n",
    "    # get the current motif\n",
    "#     current_motif = motif_duration[:][motif]\n",
    "    current_motif = [el[motif] for el in motif_duration]\n",
    "    \n",
    "    # concatenate\n",
    "    current_motif = np.vstack(current_motif)\n",
    "    # get and store the median\n",
    "    median_duration.append(np.median(current_motif))\n",
    "    \n",
    "    frequencies, edges = np.histogram(current_motif, 50)\n",
    "#     print(np.vstack(current_motif).shape)\n",
    "    # generate a histogram of the current motif's durations across trials\n",
    "    bar = hv.Bars((edges,frequencies))\n",
    "    bar.opts(xrotation=45)\n",
    "    bar_list.append(bar)\n",
    "print(median_duration)\n",
    "hv.Layout(bar_list, kdims='x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "herbal-techno",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the average progression for each motif\n",
    "\n",
    "# define the tolerance in the median\n",
    "# median_tol = \n",
    "# allocate a list of the visualizations\n",
    "motif_vis = []\n",
    "motif_average_list = []\n",
    "\n",
    "# for all the motifs\n",
    "for motif in np.arange(num_motifs):\n",
    "    # get the median\n",
    "    current_median = median_duration[motif]\n",
    "    # allocate a list per file\n",
    "    motif_list = []\n",
    "    # get the coordinates across trials\n",
    "    # for all the files\n",
    "    for idx, data in enumerate(data_list):\n",
    "        # get the labels\n",
    "        labels = label_list[idx]\n",
    "        \n",
    "        # find the starts \n",
    "        start_idx = np.argwhere(motif_duration[idx][motif]==current_median)\n",
    "        starts = motif_location[idx][motif][start_idx]\n",
    "        \n",
    "        # skip if no hits\n",
    "        if len(starts)==0:\n",
    "            continue\n",
    "        # get the coordinates\n",
    "        motif_list.append([data[:, int(np.round(el)):int(np.round(el)+np.round(current_median))] \\\n",
    "                           for el in starts[0]])\n",
    "        \n",
    "    # store\n",
    "    motif_vis.append(motif_list)\n",
    "    # average\n",
    "    motif_average = np.mean(np.concatenate(motif_list, axis=0), axis=0).T\n",
    "#     motif_average = np.array([el for el in motif_list[0]])[0, :, :].T\n",
    "#     print(motif_average.shape)\n",
    "    motif_average_list.append(np.stack((motif_average[0, :].T, motif_average[-1, :].T), axis=1))\n",
    "#     print(motif_average.shape)\n",
    "\n",
    "    def mouse_trajectory(time):\n",
    "#         x = motif_average[:time+1, [0, 2, 4, 6, 8]]\n",
    "#         y = motif_average[:time+1, [1, 3, 5, 7, 9]]\n",
    "        x = motif_average[time, [0, 2, 4, 6, 8, 10, 12, 14]]\n",
    "        y = motif_average[time, [1, 3, 5, 7, 9, 11, 13, 15]]\n",
    "        \n",
    "        return hv.Curve((x, y))\n",
    "\n",
    "    def cricket_trajectory(time):\n",
    "#         x = motif_average[:time+1, [10, 12]]\n",
    "#         y = motif_average[:time+1, [11, 13]]\n",
    "        \n",
    "        x = motif_average[time, [16, 18]]\n",
    "        y = motif_average[time, [17, 19]]\n",
    "\n",
    "    \n",
    "#     return hv.Curve((x, y))*arrow_head*arrow_body\n",
    "        return hv.Curve((x, y))\n",
    "\n",
    "    mouse_map = hv.DynamicMap(mouse_trajectory, kdims=['time'])\n",
    "    if 'prey_capture' in vame_type:\n",
    "        cricket_map = hv.DynamicMap(cricket_trajectory, kdims=['time'])\n",
    "\n",
    "\n",
    "# both_map = (mouse_map*cricket_map).opts(width=600, height=400, xlim=(0, 1280), ylim=(0, 1024))\n",
    "if 'prey_capture' in vame_type:\n",
    "    both_map = (mouse_map*cricket_map).opts(width=600, height=400, xlim=(-40, 40), ylim=(-40, 40))\n",
    "else:\n",
    "    both_map = (mouse_map).opts(width=600, height=400, xlim=(-40, 40), ylim=(-40, 40))\n",
    "        \n",
    "both_panel = pn.panel(both_map.redim.range(time=(0, motif_average.shape[0]-1)), \n",
    "                      center=True, widget_location='top')\n",
    "both_panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rocky-blogger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the motifs\n",
    "# average across files\n",
    "print(motif_average_list[0].shape)\n",
    "# average_motifs = np.nanmean(np.stack(motif_average_list, axis=2), axis=2)\n",
    "# sem_motifs = np.nanmean(np.stack(motif_sem, axis=3), axis=3)\n",
    "average_motifs = np.array(motif_average_list)\n",
    "# define the indexes to plot\n",
    "index_x = [0, 2, 4, 6, 8, 10, 12, 14]\n",
    "index_y = [1, 3, 5, 7, 9, 11, 13, 15]\n",
    "\n",
    "cricket_x = [16, 18]\n",
    "cricket_y = [17, 19]\n",
    "# plot\n",
    "# allocate memory for the plots\n",
    "plot_list = []\n",
    "\n",
    "# for all the motifs\n",
    "# for motif in np.arange(latent_list[0].shape[1]):\n",
    "for motif in motif_sort:\n",
    "#         line = hv.Curve((list(np.arange(average.shape[0])), average),label=bino, vdims='Goodness of fit').opts(\n",
    "#         width=400, height=400, shared_axes=False,xticks=x_labels, xrotation=45, padding=0.1, \n",
    "#         fontsize={'title': 16, 'labels': 14, 'xticks': 12, 'yticks': 12})\n",
    "#     shadow = hv.Spread((list(np.arange(average.shape[0])), average, errors)).opts(shared_axes=False) \n",
    "    curve1 = hv.Curve((average_motifs[motif, index_x, 0], average_motifs[motif, index_y, 0]))\n",
    "#     shadow1 = hv.Spread((average_motifs[motif, index_x, 0], average_motifs[motif, index_y, 0],\n",
    "#                                                                              sem_motifs[motif, index_y, 0]))\n",
    "    \n",
    "    curve2 = hv.Curve((average_motifs[motif, index_x, 1], average_motifs[motif, index_y, 1]))\n",
    "#     shadow2 = hv.Spread((average_motifs[motif, index_x, 1], average_motifs[motif, index_y, 1],\n",
    "#                                                                              sem_motifs[motif, index_y, 1]))\n",
    "    if 'prey_capture' in vame_type:\n",
    "        curve3 = hv.Curve((average_motifs[motif, [16, 18], 0], average_motifs[motif, [17, 19], 0]))\n",
    "        curve4 = hv.Curve((average_motifs[motif, [16, 18], 1], average_motifs[motif, [17, 19], 1]))\n",
    "#     plot_list.append(curve1*shadow1*curve2*shadow2)\n",
    "        plot_list.append(curve1*curve2*curve3*curve4)\n",
    "    else:\n",
    "        plot_list.append(curve1*curve2)\n",
    "hv.Layout(plot_list, kdims='time').cols(3)\n",
    "# print(average_motifs)\n",
    "# plot_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wicked-repeat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the transition and usage analysis\n",
    "\n",
    "# load the latent and labels\n",
    "usage_list = [np.load(os.path.join(target_folder,'results',el,'VAME','kmeans-15','motif_usage_'+el+'.npy')) \n",
    "              for el in result_list]\n",
    "# transition_list = [np.load(os.path.join(target_folder,'results',el,'VAME_prey_5_model','kmeans-15','behavior_quantification','transition_matrix.npy')) \n",
    "#                for el in result_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disturbed-lesson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and plot the average usage\n",
    "# compiled_usage = np.vstack(usage_list)\n",
    "\n",
    "# allocate memory for the output usages\n",
    "usage_all = np.zeros((len(label_list), latent_list[0].shape[1]))\n",
    "# for all the files\n",
    "for idx, labels in enumerate(label_list):\n",
    "    # get the unique numbers and their counts\n",
    "    unique_nums, unique_counts = np.unique(motif_revsort[labels], return_counts=True)\n",
    "    # fill in the corresponding indexes in the matrix\n",
    "    usage_all[idx, unique_nums] = unique_counts\n",
    "\n",
    "# average\n",
    "average_usage = np.mean(usage_all, axis=0)\n",
    "sem_usage = sem(usage_all, axis=0)\n",
    "\n",
    "motif_number = latent_list[0].shape[1]\n",
    "# plot\n",
    "def motif_usage_plot(data_in, std_in, axis_limits):\n",
    "    bars = hv.Bars(data_in, kdims=['Motif'], vdims=['Fraction'])\n",
    "    bars.opts(width=600, height=400, ylim=(0, 150))\n",
    "    errorbars = hv.ErrorBars((np.arange(axis_limits), data_in, std_in))\n",
    "\n",
    "    return bars*errorbars\n",
    "\n",
    "# calculate the succ and fail averages\n",
    "succ_usages = np.array([el for idx, el in enumerate(usage_all) if 'succ' in result_list[idx]])\n",
    "succ_average = np.mean(succ_usages, axis=0)\n",
    "succ_std = sem(succ_usages, axis=0)/np.max(succ_average)\n",
    "succ_average /= np.max(succ_average)\n",
    "\n",
    "succ_plot = motif_usage_plot(succ_average, succ_std, motif_number).opts(ylim=(0, 1.2))\n",
    "\n",
    "fail_usages = np.array([el for idx, el in enumerate(usage_all) if 'fail' in result_list[idx]])\n",
    "fail_average = np.mean(fail_usages, axis=0)\n",
    "fail_std = sem(fail_usages, axis=0)/np.max(fail_average)\n",
    "fail_average /= np.max(fail_average)\n",
    "\n",
    "fail_plot = motif_usage_plot(fail_average, fail_std, motif_number).opts(ylim=(0, 1.2))\n",
    "\n",
    "img = succ_plot+fail_plot\n",
    "img.opts(shared_axes=False).cols(1)\n",
    "img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranking-presentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Load the matching prey capture data\n",
    "\n",
    "# using the slug, perform serial calls to the database\n",
    "# (super inefficient, but this is temporary as the VAME data should be includedin the hdf5 file)\n",
    "\n",
    "# allocate memory for the data\n",
    "beh_data = []\n",
    "\n",
    "# for all the files\n",
    "for files in result_list:\n",
    "    # define the search string\n",
    "    search_string = 'slug:'+files\n",
    "    # query the database for data to plot\n",
    "    data_all = bd.query_database('analyzed_data', search_string)\n",
    "    data_path = data_all[0]['analysis_path']\n",
    "\n",
    "    # load the data\n",
    "    beh_data.append(pd.read_hdf(data_path, 'full_traces'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e8c4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate parameter profiles for the motifs\n",
    "# THIS IS WHERE MOTIF_SORT IS CALCULATED\n",
    "\n",
    "# define the target parameter\n",
    "target_parameter = 'cricket_0_mouse_distance'\n",
    "# allocate memory for all the motifs\n",
    "motif_parameter = []\n",
    "# allocate memory to store the max of the parameter for sorting purposes\n",
    "motif_max = []\n",
    "\n",
    "# for all the motifs\n",
    "# for motif in np.arange(num_motifs):\n",
    "for motif in motif_sort:\n",
    "    # get the median\n",
    "    current_median = median_duration[motif]\n",
    "    # allocate a list per file\n",
    "    motif_list = []\n",
    "    # get the coordinates across trials\n",
    "    # for all the files\n",
    "    for idx, data in enumerate(beh_data):\n",
    "        # get the labels\n",
    "        labels = label_list[idx]\n",
    "        \n",
    "        # find the starts \n",
    "        start_idx = np.argwhere(motif_duration[idx][motif]==current_median)\n",
    "        starts = motif_location[idx][motif][start_idx]\n",
    "        \n",
    "        # skip if no hits\n",
    "        if len(starts)==0 or (target_parameter not in data.keys()):\n",
    "            continue\n",
    "        # get the current set of motifs\n",
    "        current_motif = [data[target_parameter].to_numpy()\\\n",
    "                         [int(np.round(el)):int(np.round(el)+np.round(current_median))] \\\n",
    "                           for el in starts[0]]\n",
    "        # get the coordinates\n",
    "        motif_list.append(current_motif)\n",
    "    \n",
    "    \n",
    "    # turn into an array\n",
    "    average_parameter = np.mean(np.concatenate(motif_list, axis=0), axis=0).T\n",
    "    std_parameter = sem(np.concatenate(motif_list, axis=0), axis=0).T\n",
    "    # get the max and store\n",
    "    motif_max.append(np.max(average_parameter))\n",
    "    \n",
    "    # plot\n",
    "    x_coord = np.array(np.arange(average_parameter.shape[0]))\n",
    "    curve = hv.Curve((x_coord, average_parameter)).opts(width=250)\n",
    "    spread = hv.Spread((x_coord, average_parameter, std_parameter))\n",
    "    motif_parameter.append(curve*spread)\n",
    "\n",
    "# # get a new motif sorting index\n",
    "# motif_sort = np.argsort(motif_max)\n",
    "# motif_revsort = np.argsort(motif_sort)\n",
    "print(motif_sort)\n",
    "print(motif_revsort)\n",
    "# display the plots\n",
    "hv.Layout(motif_parameter).cols(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defined-foster",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the behavioral data\n",
    "\n",
    "# allocate memory for the distances\n",
    "distance_list = []\n",
    "target_key = 'cricket_0_delta_heading'\n",
    "\n",
    "# define the interval to take from the edges\n",
    "# edges = [7, -14] # for bins = 14\n",
    "edges = [15, -15] # for bins = 30\n",
    "\n",
    "def triangle_area(p1, p2, p3):\n",
    "    a1 = p1[:, 0]*(p2[:, 1] - p3[:, 1])\n",
    "    a2 = p2[:, 0]*(p3[:, 1] - p1[:, 1])\n",
    "    a3 = p3[:, 0]*(p1[:, 1] - p2[:, 1])\n",
    "    return np.abs(a1+a2+a3)/2\n",
    "\n",
    "# for all the files\n",
    "for files in beh_data:\n",
    "#     temp_values = np.log(files.index[edges[0]:edges[1]])\n",
    "#     temp_values = (files['time_vector'][edges[0]:edges[1]]/np.max(files['time_vector'][edges[0]:edges[1]]))\n",
    "#     temp_values = files['mouse_speed'][edges[0]:edges[1]]\n",
    "    if target_key in files.keys():\n",
    "        temp_values = files[target_key][edges[0]:edges[1]]\n",
    "#         temp_values = np.log(files[target_key][edges[0]:edges[1]])\n",
    "#         temp_values[temp_values>10] = 10\n",
    "\n",
    "        temp_values[np.isinf(temp_values)] = 0\n",
    "#         snout = files[['mouse_snout_x', 'mouse_snout_y']].to_numpy()[edges[0]:edges[1], :]\n",
    "#         tail = files[['mouse_base_x', 'mouse_base_y']].to_numpy()[edges[0]:edges[1], :]\n",
    "#         temp_values = fk.distance_calculation(snout, tail)\n",
    "#         temp_values[temp_values>10] = 10\n",
    "\n",
    "#         # calculate curvature\n",
    "#         snout = files[['mouse_head_x', 'mouse_head_y']].to_numpy()[edges[0]:edges[1], :]\n",
    "#         body = files[['mouse_body2_x', 'mouse_body2_y']].to_numpy()[edges[0]:edges[1], :]\n",
    "#         tail = files[['mouse_base_x', 'mouse_base_y']].to_numpy()[edges[0]:edges[1], :]\n",
    "        \n",
    "# #         triangle = triangle_area(snout, body, tail)\n",
    "#         side_1 = fk.distance_calculation(snout, body)\n",
    "#         side_2 = fk.distance_calculation(snout, tail)\n",
    "#         side_3 = fk.distance_calculation(tail, body)\n",
    "        \n",
    "# #         temp_values = 4*triangle/(side_1*side_2*side_3)\n",
    "#         temp_values = np.arccos((side_1**2 + side_3**2 - side_2**2)/(2*side_1*side_3))\n",
    "# #         temp_values = np.log(temp_values)\n",
    "#         temp_values[temp_values<2] = 2\n",
    "#         temp_values[np.isinf(temp_values)] = 0\n",
    "#         temp_values = (files['time_vector'][edges[0]:edges[1]]/\n",
    "#                        np.max(files['time_vector'][edges[0]:edges[1]]))\n",
    "    else:\n",
    "        temp_values = np.zeros_like(files['mouse_x'][edges[0]:edges[1]])\n",
    "#     temp_values[temp_values==0] = np.nan\n",
    "    distance_list.append(temp_values)\n",
    "    \n",
    "print(label_list[0].shape)\n",
    "print(distance_list[0].shape)\n",
    "print(beh_data[0].keys())\n",
    "\n",
    "# print(beh_data[0]['mouse_x'])\n",
    "# print(data_list[0][3, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collective-nurse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create categorical labels\n",
    "\n",
    "\n",
    "unique_mice, point_id = np.unique(['_'.join(el.split('_')[7:10]) for el in result_list], return_inverse=True)\n",
    "# _, point_id = np.unique(['_'.join(el.split('_')[0:3]) for el in result_list], return_inverse=True)\n",
    "\n",
    "distance_list = []\n",
    "\n",
    "for idx, files in enumerate(result_list):\n",
    "#     distance_list.append(np.zeros_like(beh_data[idx]['mouse_x'][edges[0]:edges[1]])+point_id[idx])\n",
    "    if 'succ' in files:\n",
    "        distance_list.append(np.zeros_like(beh_data[idx]['mouse_x'][edges[0]:edges[1]]))\n",
    "    elif 'fail' in files:\n",
    "        distance_list.append(np.zeros_like(beh_data[idx]['mouse_x'][edges[0]:edges[1]])+1)\n",
    "    else:\n",
    "        distance_list.append(np.zeros_like(beh_data[idx]['mouse_x'][edges[0]:edges[1]])+2)\n",
    "distance_list = np.array(distance_list)\n",
    "\n",
    "\n",
    "# print(unique_mice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atmospheric-heath",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the UMAP clusters\n",
    "\n",
    "# get the labels\n",
    "compiled_labels = np.expand_dims(np.hstack(label_list).T, axis=1)\n",
    "print(motif_sort)\n",
    "compiled_labels = motif_revsort[compiled_labels]\n",
    "\n",
    "# define the sampling ratio\n",
    "sampling_ratio = 10\n",
    "\n",
    "umap_data = np.concatenate((embedded_data[::sampling_ratio, :],\n",
    "                            compiled_labels[::sampling_ratio, :]), axis=1)\n",
    "\n",
    "print(umap_data.shape)\n",
    "                            \n",
    "                            \n",
    "umap_plot = hv.Scatter(umap_data, vdims=['Dim 2','cluster'], kdims=['Dim 1'])\n",
    "print(umap_plot)\n",
    "umap_plot.opts(color='cluster', colorbar=True, cmap='Spectral', tools=['hover'])\n",
    "umap_plot.opts(opts.Scatter(width=800, height=600))\n",
    "umap_plot\n",
    "\n",
    "# save the plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-palestinian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the UMAP embedding with another parameter\n",
    "\n",
    "# get the labels\n",
    "compiled_labels = np.expand_dims(np.hstack(distance_list), axis=1)\n",
    "\n",
    "# define the sampling ratio\n",
    "sampling_ratio = 10\n",
    "\n",
    "umap_data = np.concatenate((embedded_data[::sampling_ratio, :],compiled_labels[::sampling_ratio,:]), axis=1)\n",
    "\n",
    "print(umap_data.shape)\n",
    "                            \n",
    "                            \n",
    "umap_plot = hv.Scatter(umap_data, vdims=['Dim 2','parameter'], kdims=['Dim 1'])\n",
    "umap_plot.opts(color='parameter', colorbar=True, cmap='Spectral', tools=['hover'])\n",
    "umap_plot.opts(opts.Scatter(width=800, height=600))\n",
    "umap_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fedbc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate transition matrices\n",
    "\n",
    "# define the number of frames back to look\n",
    "back_frames = 5\n",
    "# get the number of motif\n",
    "motif_number = latent_list[0].shape[1]\n",
    "# allocate memory for the matrices\n",
    "transition_matrices = np.zeros((back_frames, len(label_list), motif_number, motif_number))\n",
    "# for all the label files\n",
    "for idx, files in enumerate(label_list):\n",
    "    # remove the consecutive repeats\n",
    "    clean_label = [iterator[0] for iterator in it.groupby(files)]\n",
    "    \n",
    "    # for all the frames\n",
    "    for idx_frame, frames in enumerate(clean_label):\n",
    "        # get the labels (coordinates)\n",
    "        x = motif_revsort[frames]\n",
    "        # for all the back frames\n",
    "        for bframes in np.arange(back_frames):\n",
    "            # skip if not further enough yet\n",
    "            if (idx_frame - bframes) < 0:\n",
    "                continue\n",
    "            y = motif_revsort[clean_label[idx_frame - bframes - 1]]\n",
    "            transition_matrices[bframes, idx, x, y] += 1\n",
    "#     # for all the frames except the first one\n",
    "#     for idx_frame, frames in enumerate(files[1:]):\n",
    "#         # get the coordinates (y is pre, x is post)\n",
    "# #         x = frames\n",
    "# #         y = files[idx_frame-1]\n",
    "\n",
    "#         x = np.argwhere(frames==motif_sort)\n",
    "#         y = np.argwhere(files[idx_frame-1]==motif_sort)\n",
    "#         if x != y:\n",
    "#             # increment the counter\n",
    "#             transition_matrices[idx, x, y] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cd8a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot average matrices\n",
    "\n",
    "# define the plotting function\n",
    "def plot_matrix(matrix_in, axis_lims):\n",
    "# plot the overall average\n",
    "    # get the labels\n",
    "    x_labels = [((idx+0.5)/(axis_lims)-0.5, el) for idx, el in enumerate(np.arange(axis_lims))]\n",
    "    y_labels = [((axis_lims-idx-0.5)/(axis_lims)-0.5, el) \n",
    "                for idx, el in enumerate(np.arange(axis_lims))]\n",
    "    # plot\n",
    "    target_matrix = hv.Image(matrix_in, kdims=['Post motif', 'Pre motif'])\n",
    "    target_matrix.opts(tools=['hover'], xticks=x_labels, yticks=y_labels, cmap='viridis')\n",
    "    return target_matrix\n",
    "\n",
    "# plot the average for succ vs fail\n",
    "# initialize a list for the plots\n",
    "plot_list = []\n",
    "\n",
    "# for all the back frames\n",
    "for bframes in np.arange(back_frames):\n",
    "    temp_matrix = transition_matrices[bframes, :, :, :]\n",
    "\n",
    "    # get the succ\n",
    "    succ_matrices = np.mean([el for idx, el in enumerate(temp_matrix) \n",
    "                             if 'succ' in result_list[idx]], axis=0)\n",
    "\n",
    "    fail_matrices = np.mean([el for idx, el in enumerate(temp_matrix) \n",
    "                             if 'fail' in result_list[idx]], axis=0)\n",
    "    \n",
    "    # get the normalization factor\n",
    "    if bframes == 0:\n",
    "        succ_norm = np.max(succ_matrices)\n",
    "        fail_norm = np.max(fail_matrices)\n",
    "        \n",
    "    succ = plot_matrix(succ_matrices/succ_norm, motif_number)\n",
    "    fail = plot_matrix(fail_matrices/fail_norm, motif_number)\n",
    "    \n",
    "    plot_list.append(succ)\n",
    "    plot_list.append(fail)\n",
    "\n",
    "# img = (succ+fail).opts(shared_axes=False)\n",
    "\n",
    "img = hv.Layout(plot_list).opts(shared_axes=True).cols(2)\n",
    "img\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-prey_capture] *",
   "language": "python",
   "name": "conda-env-.conda-prey_capture-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
