{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9b2a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "# suppress holoviews warning. Using warnings module didn't work\n",
    "import logging\n",
    "logging.getLogger(\"param.Dimension\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"param.ParameterizedMetaclass\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"param.HistogramPlot\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"param.AdjointLayout\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"param.OverlayPlot\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"param.HoloMap\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"param.CurvePlot\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"param.Layout\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"param.LayerPlot\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"param.RasterPlot\").setLevel(logging.CRITICAL)\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath(r'D:\\Code Repos\\prey_capture'))\n",
    "\n",
    "\n",
    "import panel as pn\n",
    "import holoviews as hv\n",
    "from holoviews import opts, dim\n",
    "hv.extension('bokeh')\n",
    "from bokeh.resources import INLINE\n",
    "\n",
    "import paths\n",
    "import functions_bondjango as bd\n",
    "import functions_misc as fm\n",
    "import functions_plotting as fp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.mixture as mix\n",
    "import sklearn.decomposition as decomp\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_predict, train_test_split\n",
    "from sklearn import svm, datasets\n",
    "from sklearn import preprocessing\n",
    "import sklearn.linear_model as lin\n",
    "import sklearn.metrics as smet\n",
    "import scipy.signal as ss\n",
    "\n",
    "import random\n",
    "import functions_data_handling as fd\n",
    "import importlib\n",
    "import processing_parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf798ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the desired files\n",
    "importlib.reload(processing_parameters)\n",
    "# get the data paths\n",
    "try: \n",
    "    data_path = snakemake.input[0]\n",
    "except NameError:\n",
    "    # define the search string\n",
    "    search_string = processing_parameters.search_string\n",
    "    # query the database for data to plot\n",
    "    data_all = bd.query_database('analyzed_data', search_string)\n",
    "    data_path = [el['analysis_path'] for el in data_all if 'preproc' in el['slug']]\n",
    "    print(data_path)\n",
    "    # load the calcium data\n",
    "    beh_data = []\n",
    "    data = []\n",
    "    # for all the files\n",
    "    for files in data_path:\n",
    "        # load the data\n",
    "        with pd.HDFStore(files) as h:\n",
    "            beh_data.append(h['full_traces'])\n",
    "            if '/matched_calcium' in h.keys():\n",
    "                data.append(h['matched_calcium'])\n",
    "    # concatenate the data\n",
    "    data = pd.concat(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827bf18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify a given variable\n",
    "\n",
    "# define the target variable\n",
    "target_behavior = 'cricket_0_mouse_distance'\n",
    "direct_label = 0\n",
    "# define the number of bins\n",
    "bin_number = 5;\n",
    "\n",
    "images = {}\n",
    "scores = []\n",
    "\n",
    "prediction = {}\n",
    "\n",
    "#     for day in ['2020_09_02']:\n",
    "# print(str(mouse)+str(day))\n",
    "# get the table\n",
    "# sub_data = data[mouse][day]\n",
    "sub_data = data\n",
    "# get the available columns\n",
    "labels = list(sub_data.columns)\n",
    "cells = [el for el in labels if 'cell' in el]\n",
    "not_cells = [el for el in labels if 'cell' not in el]\n",
    "# get the cell data\n",
    "calcium_data = np.array(sub_data[cells].copy())\n",
    "# get rid of the super small values\n",
    "calcium_data[np.isnan(calcium_data)] = 0\n",
    "\n",
    "# if calcium_data.shape[0] == 0:\n",
    "#     continue\n",
    "# scale (convert to float to avoid warning, potentially from using too small a dtype)\n",
    "calcium_data = preprocessing.StandardScaler().fit_transform(calcium_data)\n",
    "\n",
    "# get the distance to cricket\n",
    "# distance = ss.medfilt(sub_data.loc[:, target_behavior].to_numpy(), 21)\n",
    "distance = sub_data.loc[:, target_behavior].to_numpy()\n",
    "print(distance)\n",
    "\n",
    "# remove repeated values in a row\n",
    "# remove_vector = np.argwhere(np.diff(distance)!=0) + 1\n",
    "# remove_vector = np.array([el[0] for el in remove_vector])\n",
    "keep_vector = np.pad(np.diff(distance)!=0, (1, 0), mode='constant', constant_values=0)\n",
    "# keep_vector = (keep_vector) & (distance<3)\n",
    "\n",
    "distance = distance[keep_vector]\n",
    "calcium_data = calcium_data[keep_vector, :]\n",
    "\n",
    "# if direct bins are selected, just use the behavior vector directly\n",
    "if direct_label:\n",
    "    label_vector = np.round(distance)\n",
    "else:\n",
    "    # determine the speed bin edges\n",
    "    bins = np.percentile(distance,np.linspace(0, 100, bin_number+1))   \n",
    "    bins[-1] = np.ceil(bins[-1])\n",
    "    bins[0] = np.floor(bins[0])\n",
    "\n",
    "    # bin the distance to cricket\n",
    "    label_vector = np.digitize(distance,bins)-1\n",
    "\n",
    "# if np.min(calcium_data.flatten()) == 0 and np.max(calcium_data.flatten())== 0:\n",
    "#     continue\n",
    "print([np.min(calcium_data.flatten()), np.max(calcium_data.flatten())])\n",
    "\n",
    "# # shufle the label vector\n",
    "random.shuffle(label_vector)\n",
    "# random.shuffle(distance)\n",
    "\n",
    "# train the classifier\n",
    "linear = svm.SVC(kernel='linear', C=1, decision_function_shape='ovr', random_state=0, \\\n",
    "                 probability=False)\n",
    "\n",
    "# linear = lin.ElasticNet(positive=False, l1_ratio=0.5, alpha=0.1, normalize=False, fit_intercept=True)\n",
    "# # linear = lin.TweedieRegressor(alpha=.1, max_iter=5000, fit_intercept=True, power=2)\n",
    "# linear.fit(calcium_data, distance)\n",
    "# linear_pred = linear.predict(calcium_data)\n",
    "\n",
    "# linear = lin.PoissonRegressor(alpha=1000, max_iter=5000)\n",
    "linear_distance = distance/np.max(distance)\n",
    "print(linear_distance.shape)\n",
    "print(calcium_data.shape)\n",
    "# linear_pred = cross_val_predict(linear, calcium_data, distance, cv=5, verbose=3, \n",
    "#                                 n_jobs=None)\n",
    "\n",
    "# print('Exp variance:'+str(smet.r2_score(distance, linear_pred)))\n",
    "\n",
    "\n",
    "linear_pred_save = cross_val_predict(linear, calcium_data, label_vector, cv=5, verbose=3, \n",
    "                                n_jobs=None, method='decision_function')\n",
    "\n",
    "# # take the max of the decision function\n",
    "linear_pred = np.argmax(linear_pred_save, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98806a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_linear = confusion_matrix(label_vector, linear_pred)\n",
    "# normalize the confusion matrix to rows\n",
    "confusion_linear = confusion_linear/np.sum(confusion_linear, axis=1)\n",
    "print('Accuracy:'+str(smet.balanced_accuracy_score(label_vector, linear_pred)))\n",
    "\n",
    "# y_labels = [(-(idx+0.5)/(len(bins))+0.5, el) for idx, el in enumerate(bins)]\n",
    "y_labels = [((len(bins)-1) - idx - 0.5, el) for idx, el in enumerate(np.arange(len(bins)-1))]\n",
    "x_labels = [(idx + 0.5, el) for idx, el in enumerate(np.arange(len(bins)-1))]\n",
    "print(y_labels)\n",
    "\n",
    "# # plot\n",
    "delta_image = hv.Image(confusion_linear, kdims=['Predicted distance', 'Real distance'],\n",
    "                       bounds=[0, 0, confusion_linear.shape[1], confusion_linear.shape[0]])\n",
    "delta_image.opts(yticks=y_labels, xticks=x_labels, colorbar=True, cmap='viridis', width=400, height=300)\n",
    "\n",
    "# # images[mouse,day] = delta_image\n",
    "# scores.append(np.sum(np.diag(confusion_linear))/np.sum(confusion_linear.flatten()))\n",
    "# # prediction[mouse, day] = linear_pred_save\n",
    "\n",
    "# print(confusion_linear)\n",
    "# print(scores)\n",
    "\n",
    "delta_image        \n",
    "        # save the results\n",
    "# hv.Histogram((edges, frequencies))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a1a983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the prediction over time\n",
    "\n",
    "# get the number of time points\n",
    "time_number = linear_pred.shape[0]\n",
    "# print(time_number)\n",
    "\n",
    "normalized_param = distance\n",
    "# normalized_param = normalized_param/np.max(normalized_param)\n",
    "\n",
    "# normalized_prediction = ss.medfilt(bins[linear_pred], 21)\n",
    "# normalized_prediction = ss.medfilt(bins[linear_pred], 21)\n",
    "# normalized_prediction = ss.medfilt(linear_pred, 21)\n",
    "normalized_prediction = linear_pred.astype(float)\n",
    "\n",
    "# normalized_prediction = normalized_prediction/np.max(normalized_prediction)\n",
    "# normalized_prediction = ss.medfilt(linear_pred, 21)\n",
    "\n",
    "# normalized_labels = ss.medfilt(bins[label_vector], 21)\n",
    "\n",
    "quadrant_info = sub_data.loc[keep_vector, 'cricket_0_quadrant']\n",
    "# quadrant_info = ss.medfilt(quadrant_info/np.max(quadrant_info),21)\n",
    "\n",
    "# filter the prediction by the quadrant\n",
    "normalized_vis = normalized_prediction.copy()\n",
    "normalized_vis[quadrant_info>0] = np.nan\n",
    "normalized_nonvis = normalized_prediction.copy()\n",
    "normalized_nonvis[quadrant_info<1] = np.nan\n",
    "\n",
    "angle_info = sub_data.loc[keep_vector, 'cricket_0_visual_angle']\n",
    "# angle_info = ss.medfilt(angle_info/np.max(angle_info),21)\n",
    "\n",
    "speed_info = sub_data.loc[keep_vector, 'mouse_speed']\n",
    "# speed_info = ss.medfilt(speed_info/np.max(speed_info),21)\n",
    "\n",
    "prediction = hv.Curve((range(time_number), normalized_prediction)).opts(height=600, width=800)\n",
    "prediction_vis = hv.Curve((range(time_number), normalized_vis)).opts(height=600, width=800)\n",
    "prediction_nonvis = hv.Curve((range(time_number), normalized_nonvis)).opts(height=600, width=800)\n",
    "parameter = hv.Curve((range(time_number), normalized_param)).opts(height=600, width=800)\n",
    "# labels = hv.Curve((range(time_number), normalized_labels)).opts(height=600, width=800)\n",
    "\n",
    "# quadrant = hv.Curve((range(time_number), quadrant_info)).opts(height=600, width=800)\n",
    "angle = hv.Curve((range(time_number), angle_info)).opts(height=600, width=800, tools=['hover'])\n",
    "speed = hv.Curve((range(time_number), speed_info)).opts(height=600, width=800)\n",
    "\n",
    "# prediction+parameter\n",
    "# compiled = parameter*prediction#*angle*speed\n",
    "# compiled = parameter*prediction_vis*prediction_nonvis\n",
    "compiled = parameter*prediction\n",
    "compiled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573c8bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multivariate regression\n",
    "\n",
    "# define the target variable\n",
    "# target_behavior = ['cricket_0_x', 'cricket_0_y']\n",
    "# target_behavior = ['mouse_x', 'mouse_y']\n",
    "target_behavior = ['cricket_0_mouse_distance']\n",
    "direct_label = 0\n",
    "# define the number of bins\n",
    "bin_number = 5\n",
    "\n",
    "images = {}\n",
    "scores = []\n",
    "\n",
    "prediction = {}\n",
    "\n",
    "#     for day in ['2020_09_02']:\n",
    "# print(str(mouse)+str(day))\n",
    "# get the table\n",
    "# sub_data = data[mouse][day]\n",
    "sub_data = data\n",
    "# get the available columns\n",
    "labels = list(sub_data.columns)\n",
    "cells = [el for el in labels if 'cell' in el]\n",
    "not_cells = [el for el in labels if 'cell' not in el]\n",
    "# get the cell data\n",
    "calcium_data = np.array(sub_data[cells].copy())\n",
    "# get rid of the super small values\n",
    "calcium_data[np.isnan(calcium_data)] = 0\n",
    "\n",
    "# if calcium_data.shape[0] == 0:\n",
    "#     continue\n",
    "# scale (convert to float to avoid warning, potentially from using too small a dtype)\n",
    "calcium_data = preprocessing.StandardScaler().fit_transform(calcium_data)\n",
    "\n",
    "# get the distance to cricket\n",
    "# distance = ss.medfilt(sub_data.loc[:, target_behavior].to_numpy(), 21)\n",
    "parameter = sub_data.loc[:, target_behavior].to_numpy()\n",
    "distance_to_prey = sub_data.loc[:, 'cricket_0_mouse_distance'].to_numpy()\n",
    "print(distance.shape)\n",
    "keep_vector = np.pad(np.diff(parameter[:, 0])!=0, (1, 0), mode='constant', constant_values=0)\n",
    "distance_vector = distance_to_prey>3\n",
    "# random.shuffle(distance_vector)\n",
    "\n",
    "# keep_vector = (keep_vector) & (distance_vector)\n",
    "parameter = parameter[keep_vector, :]\n",
    "calcium_data = calcium_data[keep_vector, :]\n",
    "\n",
    "# random.shuffle(parameter)\n",
    "\n",
    "# if np.min(calcium_data.flatten()) == 0 and np.max(calcium_data.flatten())== 0:\n",
    "#     continue\n",
    "\n",
    "# # shufle the label vector\n",
    "# random.shuffle(label_vector)\n",
    "# random.shuffle(distance)\n",
    "\n",
    "# train the classifier\n",
    "\n",
    "calcium_train, calcium_test, parameter_train, parameter_test = \\\n",
    "    train_test_split(calcium_data, parameter, test_size=0.2)\n",
    "# linear = lin.ElasticNet(positive=False, l1_ratio=0.5, alpha=0.1, normalize=False, fit_intercept=True)\n",
    "# linear = lin.TweedieRegressor(alpha=.1, max_iter=5000, fit_intercept=True, power=2)\n",
    "# linear = lin.MultiTaskElasticNet(alpha=.1, max_iter=5000, l1_ratio=0.5)\n",
    "linear = lin.MultiTaskElasticNetCV(max_iter=5000, l1_ratio=[.1, .5, .7, .9, .95, .99, 1], n_jobs=7)\n",
    "linear.fit(calcium_train, parameter_train)\n",
    "linear_pred = linear.predict(calcium_test)\n",
    "\n",
    "# linear = lin.PoissonRegressor(alpha=1000, max_iter=5000)\n",
    "# linear_distance = parameter/np.max(parameter)\n",
    "print(calcium_data.shape)\n",
    "\n",
    "\n",
    "print('Exp variance:'+str(smet.r2_score(parameter_test, linear_pred)))\n",
    "print(linear.alpha_)\n",
    "print(linear.l1_ratio_)\n",
    "# print(linear.mse_path_)\n",
    "# print(linear.alphas_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac45e064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the prediction over time\n",
    "\n",
    "# get the number of time points\n",
    "time_number = linear_pred.shape[0]\n",
    "# define the x axis\n",
    "x_axis = np.array(range(time_number))\n",
    "# print(time_number)\n",
    "\n",
    "normalized_param = parameter_test\n",
    "# normalized_param = normalized_param/np.max(normalized_param)\n",
    "\n",
    "# normalized_prediction = ss.medfilt(bins[linear_pred], 21)\n",
    "# normalized_prediction = ss.medfilt(bins[linear_pred], 21)\n",
    "# normalized_prediction = ss.medfilt(linear_pred, 21)\n",
    "# normalized_prediction = linear_pred.astype(float)\n",
    "normalized_prediction = ss.medfilt(linear_pred.astype(float), (1, 1))\n",
    "\n",
    "# normalized_prediction = normalized_prediction/np.max(normalized_prediction)\n",
    "# normalized_prediction = ss.medfilt(linear_pred, 21)\n",
    "\n",
    "# normalized_labels = ss.medfilt(bins[label_vector], 21)\n",
    "\n",
    "# quadrant_info = sub_data.loc[keep_vector, 'cricket_0_quadrant']\n",
    "quadrant_info = sub_data.loc[:, 'cricket_0_quadrant'].to_numpy()\n",
    "quadrant_info = quadrant_info[keep_vector]\n",
    "# quadrant_info = ss.medfilt(quadrant_info/np.max(quadrant_info),21)\n",
    "\n",
    "distance_info = distance_to_prey[keep_vector]\n",
    "# # filter the prediction by the quadrant\n",
    "# normalized_vis = normalized_prediction.copy()\n",
    "# normalized_vis[quadrant_info>0] = np.nan\n",
    "# normalized_nonvis = normalized_prediction.copy()\n",
    "# normalized_nonvis[quadrant_info<1] = np.nan\n",
    "\n",
    "# angle_info = sub_data.loc[keep_vector, 'cricket_0_visual_angle']\n",
    "# angle_info = ss.medfilt(angle_info/np.max(angle_info),21)\n",
    "\n",
    "# speed_info = sub_data.loc[keep_vector, 'mouse_speed']\n",
    "# speed_info = ss.medfilt(speed_info/np.max(speed_info),21)\n",
    "\n",
    "heading_info = sub_data.loc[:, 'cricket_0_delta_heading'].to_numpy()\n",
    "heading_info = heading_info[keep_vector]\n",
    "\n",
    "# prediction = hv.Curve((range(time_number), normalized_prediction)).opts(height=600, width=800)\n",
    "# prediction_vis = hv.Curve((range(time_number), normalized_vis)).opts(height=600, width=800)\n",
    "# prediction_nonvis = hv.Curve((range(time_number), normalized_nonvis)).opts(height=600, width=800)\n",
    "parameter1 = hv.Curve((x_axis, normalized_param[:, 0])).opts(height=300, width=800)\n",
    "parameter2 = hv.Curve((x_axis, normalized_param[:, 1])).opts(height=300, width=800)\n",
    "\n",
    "prediction1 = hv.Curve((x_axis, normalized_prediction[:, 0])).opts(height=300, width=800)\n",
    "prediction2 = hv.Curve((x_axis, normalized_prediction[:, 1])).opts(height=300, width=800)\n",
    "# labels = hv.Curve((range(time_number), normalized_labels)).opts(height=600, width=800)\n",
    "mse_info = np.sqrt((normalized_param - normalized_prediction)**2)\n",
    "mse1 = hv.Curve((x_axis, mse_info[:, 0])).opts(height=300, width=800)\n",
    "# quadrant = hv.Curve((range(time_number), quadrant_info)).opts(height=600, width=800)\n",
    "# quadrant = hv.Spikes(x_axis[quadrant_info==0]).opts(alpha=.3, spike_length=40)\n",
    "# distance = hv.Curve((x_axis, distance_info))\n",
    "# heading = hv.Curve((x_axis, heading_info))\n",
    "# angle = hv.Curve((range(time_number), angle_info)).opts(height=600, width=800, tools=['hover'])\n",
    "# speed = hv.Curve((range(time_number), speed_info)).opts(height=600, width=800)\n",
    "\n",
    "# prediction+parameter\n",
    "# compiled = parameter*prediction#*angle*speed\n",
    "# compiled = parameter*prediction_vis*prediction_nonvis\n",
    "# compiled = mse1*quadrant*distance*heading + parameter2*prediction2*quadrant\n",
    "# compiled = parameter1*prediction1 + parameter2*prediction2\n",
    "scatter_x = hv.Scatter((normalized_param[:, 0], normalized_prediction[:, 0]), kdims=['Real', 'Predicted'])\n",
    "scatter_y = hv.Scatter((normalized_param[:, 1], normalized_prediction[:, 1]), kdims=['Real', 'Predicted'])\n",
    "compiled = parameter1*prediction1 + parameter2*prediction2 + scatter_x + scatter_y\n",
    "compiled.cols(1)\n",
    "compiled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9d5464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a histogram of the weights\n",
    "\n",
    "freq0, edges0 = np.histogram(linear.coef_[0, :], 20)\n",
    "freq1, edges1 = np.histogram(linear.coef_[1, :], 20)\n",
    "\n",
    "# bar0 = hv.Bars((edges0, freq0), kdims=['Weights'], vdims=['Counts']).opts(xrotation=45)\n",
    "# bar1 = hv.Bars((edges1, freq1), kdims=['Weights'], vdims=['Counts']).opts(xrotation=45)\n",
    "\n",
    "bar0 = hv.Curve((edges0, np.cumsum(freq0)))\n",
    "bar1 = hv.Curve((edges1, np.cumsum(freq1)))\n",
    "\n",
    "combined = bar0*bar1\n",
    "# combined.cols(2)\n",
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fbe712",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# calculate regression for prediction into past and future\n",
    "\n",
    "# define the target variable\n",
    "# target_behavior = ['cricket_0_x', 'cricket_0_y']\n",
    "target_behavior = ['mouse_x', 'mouse_y']\n",
    "# target_behavior = ['cricket_0_mouse_distance']\n",
    "# target_behavior = ['mouse_speed']\n",
    "# target_behavior = ['cricket_0_delta_heading']\n",
    "direct_label = 0\n",
    "# define the number of bins\n",
    "bin_number = 5;\n",
    "\n",
    "images = {}\n",
    "scores = []\n",
    "\n",
    "prediction = {}\n",
    "\n",
    "#     for day in ['2020_09_02']:\n",
    "# print(str(mouse)+str(day))\n",
    "# get the table\n",
    "# sub_data = data[mouse][day]\n",
    "sub_data = data\n",
    "# get the available columns\n",
    "labels = list(sub_data.columns)\n",
    "cells = [el for el in labels if 'cell' in el]\n",
    "not_cells = [el for el in labels if 'cell' not in el]\n",
    "# get the cell data\n",
    "calcium_data = np.array(sub_data[cells].copy())\n",
    "# get rid of the super small values\n",
    "calcium_data[np.isnan(calcium_data)] = 0\n",
    "\n",
    "# if calcium_data.shape[0] == 0:\n",
    "#     continue\n",
    "# scale (convert to float to avoid warning, potentially from using too small a dtype)\n",
    "calcium_data = preprocessing.StandardScaler().fit_transform(calcium_data)\n",
    "\n",
    "# get the distance to cricket\n",
    "# distance = ss.medfilt(sub_data.loc[:, target_behavior].to_numpy(), 21)\n",
    "parameter = sub_data.loc[:, target_behavior].to_numpy()\n",
    "distance_to_prey = sub_data.loc[:, 'cricket_0_mouse_distance'].to_numpy()\n",
    "keep_vector = np.pad(np.diff(parameter[:, 0])!=0, (1, 0), mode='constant', constant_values=0)\n",
    "distance_vector = distance_to_prey>3\n",
    "# random.shuffle(distance_vector)\n",
    "\n",
    "# keep_vector = (keep_vector) & (distance_vector)\n",
    "keep_vector = np.ones_like(distance_to_prey, dtype=bool)\n",
    "parameter = parameter[keep_vector, :]\n",
    "calcium_data = calcium_data[keep_vector, :]\n",
    "\n",
    "\n",
    "\n",
    "# random.shuffle(parameter)\n",
    "\n",
    "# if np.min(calcium_data.flatten()) == 0 and np.max(calcium_data.flatten())== 0:\n",
    "#     continue\n",
    "\n",
    "# # shufle the label vector\n",
    "# random.shuffle(label_vector)\n",
    "# random.shuffle(distance)\n",
    "\n",
    "# train the classifier\n",
    "\n",
    "# define the lag range\n",
    "lag_range = 20\n",
    "# define the number of shuffles\n",
    "shuffle_number = 50\n",
    "# get the set of lags\n",
    "lag_vector = np.array(np.arange(-lag_range, lag_range))\n",
    "# allocate memory for the coefficients\n",
    "r2_list = []\n",
    "\n",
    "# for all the shuffles\n",
    "for shuff in np.arange(shuffle_number):\n",
    "    # allocate memory for the within shuffle scores\n",
    "    within_list = []\n",
    "\n",
    "    # get a random vector to separate train and test set\n",
    "    # get the size\n",
    "    number_points = parameter.shape[0]\n",
    "    # generate an empty vector\n",
    "    shuffle_vector = np.zeros((number_points))\n",
    "    # turn 80% of them to 1\n",
    "    shuffle_vector[:int(np.round(number_points*0.8))] = 1\n",
    "    # shuffle the vector\n",
    "    random.shuffle(shuffle_vector)\n",
    "\n",
    "    # for all the lags\n",
    "    for lag in lag_vector:\n",
    "\n",
    "        if lag > 0:\n",
    "            parameter_lag = parameter[lag:, :]\n",
    "            calcium_data_lag = calcium_data[:-lag, :]\n",
    "            shuffle_lag = shuffle_vector[:-lag]\n",
    "        elif lag < 0:\n",
    "            parameter_lag = parameter[:lag, :]\n",
    "            calcium_data_lag = calcium_data[-lag:, :]\n",
    "            shuffle_lag = shuffle_vector[-lag:]\n",
    "        else:\n",
    "            parameter_lag = parameter\n",
    "            calcium_data_lag = calcium_data\n",
    "            shuffle_lag = shuffle_vector\n",
    "\n",
    "    #     calcium_train, calcium_test, parameter_train, parameter_test = \\\n",
    "    #         train_test_split(calcium_data_lag, parameter_lag, test_size=0.2)\n",
    "\n",
    "        calcium_train = calcium_data_lag[shuffle_lag==1, :]\n",
    "        parameter_train = parameter_lag[shuffle_lag==1]\n",
    "        calcium_test = calcium_data_lag[shuffle_lag==0, :]\n",
    "        parameter_test = parameter_lag[shuffle_lag==0]\n",
    "        \n",
    "#         # shuffle training labels\n",
    "#         random.shuffle(parameter_train)\n",
    "\n",
    "        # linear = lin.ElasticNet(positive=False, l1_ratio=0.5, alpha=0.1, normalize=False, fit_intercept=True)\n",
    "        # linear = lin.TweedieRegressor(alpha=.1, max_iter=5000, fit_intercept=True, power=2)\n",
    "        linear = lin.MultiTaskElasticNet(alpha=.1, max_iter=5000, l1_ratio=0.5)\n",
    "    #     linear = lin.MultiTaskElasticNetCV(max_iter=5000, l1_ratio=[.1, .5, .7, .9, .95, .99, 1], n_jobs=7)\n",
    "        linear.fit(calcium_train, parameter_train)\n",
    "        linear_pred = linear.predict(calcium_test)\n",
    "        within_list.append(smet.r2_score(parameter_test, linear_pred))\n",
    "    # save the scores for this shuffle\n",
    "    r2_list.append(within_list)\n",
    "\n",
    "# average across the shuffles\n",
    "r2_average = np.mean(np.array(r2_list), axis=0)\n",
    "r2_sem = np.std(np.array(r2_list), axis=0)/np.sqrt(shuffle_number)\n",
    "\n",
    "print(lag_vector[np.argmax(r2_average)])\n",
    "line = hv.Curve((lag_vector, r2_average), kdims=['Lags (frames)'], vdims=['r2'])\n",
    "shade = hv.Spread((lag_vector, r2_average, r2_sem))\n",
    "line * shade\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-prey_capture] *",
   "language": "python",
   "name": "conda-env-.conda-prey_capture-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
