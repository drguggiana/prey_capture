{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c90ca9d-44aa-4430-9761-62b57d52d3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import panel as pn\n",
    "import holoviews as hv\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import importlib\n",
    "import datetime\n",
    "import warnings\n",
    "import math\n",
    "import cmath\n",
    "import pycircstat as circ\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pprint import pprint\n",
    "from itertools import zip_longest\n",
    "from scipy.stats import sem, norm, binned_statistic, percentileofscore\n",
    "from scipy.optimize import least_squares\n",
    "\n",
    "from bokeh.resources import INLINE\n",
    "from bokeh.io import export_svgs, export_png\n",
    "from holoviews import opts, dim\n",
    "from holoviews.operation import histogram\n",
    "hv.extension('bokeh')\n",
    "# hv.extension('matplotlib')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(r'C:/Users/mmccann/repos/bonhoeffer/prey_capture/'))\n",
    "import paths\n",
    "import processing_parameters\n",
    "import functions_bondjango as bd\n",
    "import functions_plotting as fp\n",
    "import functions_data_handling as fdh\n",
    "import functions_kinematic as fk\n",
    "\n",
    "importlib.reload(fp)\n",
    "# set up the figure theme\n",
    "fp.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b473b1d-b0b4-494f-97a8-831501c6ff3a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fd87a4-4b51-4c25-b80d-f4df0d923a1e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a9acd9-1b8f-4f8e-9756-17d888531af2",
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def list_lists_to_array(list_of_lists):\n",
    "    \"\"\" Converts a list of lists into a 2D array\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    list_of_lists : list\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    new_array : np.array\n",
    "        Array where each row was an entry in the list of lists\n",
    "    \"\"\"\n",
    "\n",
    "    max_length = max([len(sublist) for sublist in list_of_lists])\n",
    "    new_array = np.empty((len(list_of_lists), max_length))\n",
    "    new_array[:] = np.NaN\n",
    "\n",
    "    for row, l in enumerate(list_of_lists):\n",
    "        new_array[row, :len(l)] = l\n",
    "\n",
    "    return new_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d051b8de-3be5-489d-8186-782d7ee0b9fd",
   "metadata": {
    "hidden": true,
    "init_cell": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def normalize_rows(data_in):\n",
    "    \n",
    "    for idx, el in enumerate(data_in):\n",
    "        data_in[idx, :] = (el-np.nanmin(el))/(np.nanmax(el)-np.nanmin(el))\n",
    "    return data_in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfca5cd5-ec14-4996-80dd-6070df519e35",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf77fe8",
   "metadata": {
    "init_cell": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "importlib.reload(processing_parameters)\n",
    "# get the search string\n",
    "search_string = processing_parameters.search_string\n",
    "\n",
    "\n",
    "# get the paths from the database\n",
    "file_path, paths_all, parsed_query, date_list, animal_list = fdh.fetch_preprocessing(processing_parameters.search_string)\n",
    "\n",
    "animal_idxs = [i for i,d in enumerate(animal_list) if d==parsed_query['mouse'].lower()]\n",
    "good_entries = [file_path[index] for index in animal_idxs]\n",
    "input_path = [paths_all[index] for index in animal_idxs]\n",
    "\n",
    "# # assemble the output path\n",
    "# out_path = os.path.join(paths.analysis_path, 'test_latentconsolidate.hdf5')\n",
    "print(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8276e6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# allocate memory for the data\n",
    "free_data = []\n",
    "fixed_data = []\n",
    "data = []\n",
    "exp_type = []\n",
    "\n",
    "for files in input_path:\n",
    "    print(files)\n",
    "    # load the data\n",
    "    with pd.HDFStore(files) as h:\n",
    "#         beh_data.append(h['full_traces'])\n",
    "        if '/matched_calcium' in h.keys():\n",
    "            \n",
    "            data.append(h['matched_calcium'])\n",
    "            \n",
    "            if \"VWheel\" in files:\n",
    "                fixed_data.append(h['matched_calcium'])\n",
    "                exp_type.append('fixed')\n",
    "            else:\n",
    "                free_data.append(h['matched_calcium'])\n",
    "                exp_type.append('free')\n",
    "\n",
    "                \n",
    "print(f'Number of files loaded: {len(data)}')\n",
    "\n",
    "# data = {'free': pd.concat(free_data), 'fixed': pd.concat(fixed_data)}\n",
    "\n",
    "# Calculate orientation for each dataset\n",
    "for ds in data:\n",
    "# for name, ds in data.items():\n",
    "    ds['orientation'] = ds['direction']\n",
    "    ds['orientation'][ds['orientation'] < 0] += 180\n",
    "    \n",
    "# Grab list of direction and orientations used\n",
    "directions = np.sort(ds.direction.unique()[1:])\n",
    "directions_wrapped = np.sort(fk.wrap(directions))\n",
    "orientations = np.sort(ds.orientation.unique()[1:])\n",
    "pprint(f'Directions used: {directions}')\n",
    "pprint(f'Orientations used: {orientations}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6045f02a-47b4-4f64-8df4-a277de8f5ac0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Remove baselines from the data for later processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533535d5-12c4-4de8-a99e-063160b3bef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = ['direction', 'orientation']\n",
    "\n",
    "# Allocate memory for cell property dataframes\n",
    "df_list = []\n",
    "\n",
    "for ds in data:\n",
    "\n",
    "    # get the number of cells\n",
    "    cells = [el for el in ds.columns if 'cell' in el]\n",
    "    \n",
    "    # allocate memory for the experiment\n",
    "    exp_list = []\n",
    "\n",
    "    # Get the 7th percentile of activity per cell for each stimulus\n",
    "    # Try 7th/8th percentile\n",
    "    percentiles = ds.groupby(['direction'])[cells].quantile(0.07)\n",
    "\n",
    "    # get the baselines - The first column is the inter-trial interval\n",
    "    baselines = percentiles.iloc[0, percentiles.columns.get_loc(cells[0]):percentiles.columns.get_loc(cells[-1])+1]\n",
    "\n",
    "    # Subtract baseline from everything\n",
    "    ds[cells].subtract(baselines, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01930bbe-241c-4a1e-8e79-6642db521b35",
   "metadata": {},
   "source": [
    "# Freely Moving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197b6030-79f7-4643-a05d-df5644a33d38",
   "metadata": {},
   "source": [
    "## Running traces and distributions\n",
    "Seems like 50th percentile is a decent cutoff for running vs not running periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17a64ff-04ab-40c2-88dd-8997c36d6246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make time trace of data\n",
    "trace = hv.Curve((data[0].time_vector, data[0].mouse_speed)).opts(xlabel='time', ylabel='m/s', width=600, height=300)\n",
    "percenitile = np.percentile(np.abs(data[0].mouse_speed), 50)\n",
    "percentile_line = hv.HLine(percenitile).opts(color='r', line_width=2)\n",
    "\n",
    "# Make histogram\n",
    "# Create time bins of 0.5sec length with mean running speed\n",
    "time_bins = np.arange(0, data[0].time_vector.max(), step=0.25)\n",
    "vals, _, _ = binned_statistic(data[0].time_vector, data[0].mouse_speed, bins=time_bins, statistic=np.nanmean)\n",
    "\n",
    "hist_bins = np.arange(0, np.max(vals), step=0.005)\n",
    "frequencies, edges = np.histogram(vals, hist_bins)\n",
    "print('Values: %s, Edges: %s' % (frequencies.shape[0], edges.shape[0]))\n",
    "hist = hv.Histogram((edges, frequencies)).opts(xlabel='m/s', width=600,)\n",
    "percentile_line2 = hv.VLine(percenitile).opts(color='r', line_width=2)\n",
    "\n",
    "(trace*percentile_line + hist*percentile_line2).opts(shared_axes=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e5f676-f98d-46e1-bdd5-b14278e7bf5b",
   "metadata": {},
   "source": [
    "## Angular speed trace and distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2804978f-86fe-45fa-8c77-a610d1a2cc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make time trace of data\n",
    "trace = hv.Curve((data[0].time_vector, np.abs(data[0].mouse_angular_speed))).opts(xlabel='time', ylabel='m/s', width=600, height=300)\n",
    "percenitile = np.percentile(np.abs(data[0].mouse_angular_speed), 50)\n",
    "percentile_line = hv.HLine(percenitile).opts(color='r', line_width=2)\n",
    "\n",
    "# Make histogram\n",
    "# Create time bins of 0.5sec length with mean running speed\n",
    "time_bins = np.arange(0, data[0].time_vector.max(), step=0.25)\n",
    "vals, _, _ = binned_statistic(data[0].time_vector, np.abs(data[0].mouse_angular_speed), bins=time_bins, statistic=np.nanmean)\n",
    "\n",
    "hist_bins = np.arange(0, np.max(vals), step=10)\n",
    "frequencies, edges = np.histogram(vals, hist_bins)\n",
    "print('Values: %s, Edges: %s' % (frequencies.shape[0], edges.shape[0]))\n",
    "hist = hv.Histogram((edges, frequencies)).opts(xlabel='m/s', width=600,)\n",
    "percentile_line2 = hv.VLine(percenitile).opts(color='r', line_width=2)\n",
    "\n",
    "(trace*percentile_line + hist*percentile_line2).opts(shared_axes=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f2674d-dfb6-497b-a4e9-c07336c91cd3",
   "metadata": {},
   "source": [
    "## Head angle traces\n",
    "Note here that y is the vertical axis (yaw), x is forward (roll), and z is side (pitch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7443f4-bf92-4949-a974-c1807bea4cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "overlay = hv.Curve((data[0].time_vector, data[0].mouse_yrot_m), label='y_rot') * \\\n",
    "            hv.Curve((data[0].time_vector, data[0].mouse_zrot_m+360), label='z_rot') * \\\n",
    "            hv.Curve((data[0].time_vector, data[0].mouse_xrot_m+720), label='x_rot')\n",
    "overlay.opts(xlabel='time', width=900, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2062f1-e2c0-4fa0-be92-92b99f8f5263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove jumps from traces - doesn't fix 0-360 jumps, but it doesn't need to\n",
    "rot_traces = ['mouse_xrot_m', 'mouse_yrot_m', 'mouse_zrot_m']\n",
    "data[0][rot_traces] = data[0][rot_traces].apply(fk.jump_killer, args=[50])\n",
    "data[0][rot_traces] = data[0][rot_traces].apply(fk.wrap)\n",
    "\n",
    "overlay = hv.Curve((data[0].time_vector, data[0].mouse_yrot_m), label='y_rot') * \\\n",
    "            hv.Curve((data[0].time_vector, data[0].mouse_zrot_m+360), label='z_rot') * \\\n",
    "            hv.Curve((data[0].time_vector, data[0].mouse_xrot_m+720), label='x_rot')\n",
    "overlay.opts(xlabel='time', width=900, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9add102b-4d0d-4f06-965e-e144dc7076b3",
   "metadata": {},
   "source": [
    "# Head fixed\n",
    "Seems like 80th percentile is a decent cutoff for running vs not running periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a417e60-10b5-46b3-9a62-374cff184f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get time trace\n",
    "hv.Curve((data[1].time_vector, np.abs(data[1].wheel_speed))).opts(xlabel='time', ylabel='m/s', width=600, height=300)\n",
    "percenitile = np.percentile(np.abs(data[1].wheel_speed), 80)\n",
    "percentile_line = hv.HLine(percenitile).opts(color='r', line_width=2)\n",
    "\n",
    "\n",
    "# Make histogram\n",
    "# Create time bins of 0.5sec length with mean running speed\n",
    "time_bins = np.arange(0, data[1].time_vector.max(), step=0.25)\n",
    "vals, _, _ = binned_statistic(data[1].time_vector, np.abs(data[1].wheel_speed), bins=time_bins, statistic=np.nanmean)\n",
    "\n",
    "hist_bins = np.arange(0, np.max(vals), step=0.005)\n",
    "frequencies, edges = np.histogram(vals, hist_bins)\n",
    "print('Values: %s, Edges: %s' % (frequencies.shape[0], edges.shape[0]))\n",
    "hist = hv.Histogram((edges, frequencies)).opts(xlabel='m/s', width=600,)\n",
    "percentile_line2 = hv.VLine(percenitile).opts(color='r', line_width=2)\n",
    "\n",
    "(trace*percentile_line + hist*percentile_line2).opts(shared_axes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e2f7ed-5097-4e4a-b16c-950c9a5d4668",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:prey_capture] *",
   "language": "python",
   "name": "conda-env-prey_capture-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
