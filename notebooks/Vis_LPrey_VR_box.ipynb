{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize differences between prey capture in box and in VR arena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, r'..\\..')\n",
    "import paths\n",
    "\n",
    "import panel as pn\n",
    "import holoviews as hv\n",
    "from holoviews import opts, dim\n",
    "hv.extension('bokeh')\n",
    "from bokeh.resources import INLINE\n",
    "\n",
    "import functions_bondjango as bd\n",
    "import functions_kinematic as fk\n",
    "import functions_plotting as fp\n",
    "import functions_misc as fm\n",
    "import functions_data_handling as fd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "from scipy.stats import sem\n",
    "import sklearn.decomposition as decomp\n",
    "import umap\n",
    "import sklearn.mixture as mix\n",
    "from scipy.stats import sem\n",
    "\n",
    "\n",
    "line_width = 5"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# define the name to be used for the saved figures\n",
    "save_name = 'VPrey_VRArena_box'"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define a data loading function\n",
    "\n",
    "def load_dataset(search_string, label=None, exclusion=None):\n",
    "    # load the data\n",
    "    # get the data paths\n",
    "    try:\n",
    "        data_path = snakemake.input[0]\n",
    "    except NameError:\n",
    "        # query the database for data to plot\n",
    "        data_all = bd.query_database('analyzed_data', search_string)\n",
    "\n",
    "        if exclusion is not None:\n",
    "            for ds in data_all:\n",
    "                if exclusion not in ds['analysis_path']:\n",
    "                    data_path = ds['analysis_path']\n",
    "                    data_date = ds['date']\n",
    "                    break\n",
    "        else:\n",
    "            data_path = data_all[0]['analysis_path']\n",
    "            data_date = data_all[0]['date']\n",
    "    print(data_path)\n",
    "    print(data_date)\n",
    "\n",
    "    # assemble a label for this data set\n",
    "    if label is None:\n",
    "        d = fd.parse_search_string(search_string)\n",
    "        label = '_'.join([d['rig'], d['lighting'], d['result'], d['notes']])\n",
    "    print('data label: ' + label + '\\n')\n",
    "\n",
    "    # load the data\n",
    "    return fd.aggregate_loader(data_path), label"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encounter analysis\n",
    "This section of the analysis relies on the aggregated encounters. Load this data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# create container for holding multiple data sets\n",
    "data_dict = {}\n",
    "\n",
    "# Load real prey capture in VR arena in the light\n",
    "search_string = 'result:succ, lighting:normal, rig:VR, analysis_type:aggEnc'\n",
    "ds, label = load_dataset(search_string, exclusion='obstacle', label=\"VR_light_succ\")\n",
    "data_dict[label] = ds\n",
    "\n",
    "# Load real prey capture in VR arena in the dark - successes\n",
    "search_string = 'result:succ, lighting:dark, rig:VR, analysis_type:aggEnc'\n",
    "ds, label = load_dataset(search_string, label=\"VR_dark_succ\")\n",
    "data_dict[label] = ds\n",
    "\n",
    "# Load real prey capture in VR arena in the dark - failures\n",
    "search_string = 'result:fail, lighting:dark, rig:VR, analysis_type:aggEnc'\n",
    "ds, label = load_dataset(search_string, label=\"VR_dark_fail\")\n",
    "data_dict[label] = ds\n",
    "\n",
    "# Load real prey capture in small box\n",
    "search_string = 'result:succ, lighting:normal, rig:miniscope, analysis_type:aggEnc'\n",
    "ds, label = load_dataset(search_string, label=\"Box_light_succ\")\n",
    "data_dict[label] = ds\n",
    "\n",
    "# Get rid of doubled data set\n",
    "del ds"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot the number of encounters per trial for each condition\n",
    "\n",
    "# allocate a list for the plots\n",
    "plot_list = []\n",
    "means = []\n",
    "enc_sem = []\n",
    "\n",
    "# Plots by trial type\n",
    "for name in data_dict:\n",
    "\n",
    "    data = data_dict[name]\n",
    "\n",
    "    # load the parameter\n",
    "    parameter = data[['event_id','trial_id']].copy()\n",
    "    # find the number of encounters\n",
    "    grouped_parameter = parameter.groupby(['trial_id']).agg(list)\n",
    "    encounters = np.array([el[-1] for el in grouped_parameter['event_id']]) + 1\n",
    "    means.append(encounters.mean())\n",
    "    enc_sem.append((name, encounters.mean(), sem(encounters)))\n",
    "\n",
    "    # plot the results\n",
    "    enc_plot = hv.Bars((np.arange(encounters.shape[0]), encounters)).opts(title=name, xlabel='trial', ylabel='# encounters') * \\\n",
    "        hv.HLine(encounters.mean()).opts(color='red', line_width=1)\n",
    "    plot_list.append(enc_plot)\n",
    "\n",
    "encounters_panel = hv.Layout(plot_list).opts(shared_axes=True)\n",
    "save_path = os.path.join(paths.figures_path, '_'.join([save_name, 'encounters']))\n",
    "hv.save(encounters_panel, save_path, fmt='png')\n",
    "\n",
    "# display the image\n",
    "encounters_panel"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Separately plot mean + sem of encounters\n",
    "\n",
    "# Plot of means\n",
    "enc_means = hv.Bars((list(data_dict.keys()), means)).opts(title='Mean # Encounters', ylabel='# encounters', ylim=(0,15), xrotation=45) \n",
    "enc_means = hv.ErrorBars(enc_sem) * enc_means\n",
    "save_path = os.path.join(paths.figures_path, '_'.join([save_name, 'encounter_means']))\n",
    "hv.save(enc_means, save_path, fmt='png')\n",
    "\n",
    "# dispaly the iamge\n",
    "enc_means"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA of Encounter types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# define the target parameter and PCA\n",
    "target_parameter = 'cricket_0_mouse_distance'\n",
    "\n",
    "# container for plots\n",
    "plot_list = []\n",
    "\n",
    "# container for PCA fit\n",
    "pca_transforms = []\n",
    "\n",
    "# container for target data\n",
    "target = []\n",
    "\n",
    "for name in data_dict:\n",
    "    data = data_dict[name]\n",
    "\n",
    "    # assemble the array with the parameters of choice\n",
    "    target_data = data[[target_parameter] + ['event_id', 'trial_id']].groupby(['trial_id', 'event_id']).agg(list).to_numpy()\n",
    "\n",
    "    # HACK REMOVE\n",
    "    if ('VR' or 'VPrey') in name:\n",
    "        target_data = np.array([el for sublist in target_data for el in sublist if len(el) == 594])\n",
    "    else:\n",
    "        target_data = np.array([el for sublist in target_data for el in sublist if len(el) == 74])\n",
    "    target.append(target_data)\n",
    "\n",
    "    # PCA the data before clustering\n",
    "    pca = decomp.PCA()\n",
    "    transformed_data = pca.fit_transform(target_data)\n",
    "    pca_transforms.append(transformed_data)\n",
    "\n",
    "    # fp.plot_2d([[pca.explained_variance_ratio_]])\n",
    "    exp_var = hv.Curve(pca.explained_variance_ratio_).opts(xlabel='PCs', ylabel='explained variance', title=name)\n",
    "    plot_list.append(exp_var)\n",
    "\n",
    "hv.Layout(plot_list)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guassian Mixture Model of clusters in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cluster the data using GMMs\n",
    "plot_list = []\n",
    "clusters = []\n",
    "\n",
    "for transformed_data, name in zip(pca_transforms, data_dict.keys()):\n",
    "    \n",
    "    # define the vector of components\n",
    "    component_vector = [2, 3, 4, 5, 10, 15]\n",
    "    # allocate memory for the results\n",
    "    gmms = []\n",
    "    # for all the component numbers\n",
    "    for comp in component_vector:\n",
    "        # # define the number of components\n",
    "        gmm = mix.GaussianMixture(n_components=comp, covariance_type='diag', n_init=50)\n",
    "        gmm.fit(transformed_data[:, :7])\n",
    "        gmms.append(gmm.bic(transformed_data[:, :7]))    # Pull the first 7 PCs and get the bayesian information criterion\n",
    "\n",
    "    # select the minimum bic number of components\n",
    "    n_components = np.array(component_vector)[np.argmin(gmms)]\n",
    "    # predict the cluster indexes\n",
    "    gmm = mix.GaussianMixture(n_components=n_components, covariance_type='diag', n_init=50)\n",
    "    cluster_idx = gmm.fit_predict(transformed_data[:, :7])\n",
    "\n",
    "    # discard singletons\n",
    "    # turn cluster_idx in a float\n",
    "    cluster_idx = cluster_idx.astype(float)\n",
    "    # get the IDs\n",
    "    clu_unique = np.unique(cluster_idx)\n",
    "    for clu in clu_unique:\n",
    "        # get the number of traces in the cluster\n",
    "        number_traces = sum(cluster_idx==clu)\n",
    "        # if it's less than 5, eliminate the cluster\n",
    "        if number_traces < 5:\n",
    "            cluster_idx[cluster_idx==clu] = np.nan\n",
    "    clusters.append(cluster_idx)\n",
    "        \n",
    "    # plot the BIC\n",
    "    BIC = hv.Curve((component_vector, gmms)).opts(title=name, xlabel='cluster', ylabel='BIC')\n",
    "    plot_list.append(BIC)\n",
    "\n",
    "hv.Layout(plot_list).opts(shared_axes=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# plot the clusters\n",
    "plot_list = []\n",
    "\n",
    "for target_data, cluster_idx, name in zip(target, clusters, data_dict):\n",
    "\n",
    "    # add the cluster indexes to the dataframe\n",
    "    cluster_data = np.array([np.mean(target_data[cluster_idx == el, :], axis=0) for el in np.arange(n_components)])\n",
    "    cluster_std = np.array([np.std(target_data[cluster_idx == el, :], axis=0)/np.sqrt(np.sum(cluster_idx == el))\n",
    "                            for el in np.arange(n_components)])\n",
    "    # plot the results\n",
    "    cluster_plot = hv.Overlay([hv.Curve(el, label=str(idx), kdims=['Time (s)'], vdims=[target_parameter.replace('_', ' ')+' (px)']) for idx, el in enumerate(cluster_data)] + \n",
    "                                [hv.Spread((np.arange(el.shape[0]),el,cluster_std[idx, :])) for idx, el in enumerate(cluster_data)])\n",
    "\n",
    "    cluster_plot.relabel('Clusters').opts({'Curve': dict(color=hv.Palette('Category20')), \n",
    "                                            'Spread': dict(color=hv.Palette('Category20'))})\n",
    "    \n",
    "    cluster_plot.opts(title=name)\n",
    "\n",
    "    # For publication-ready image\n",
    "    cluster_plot.opts(\n",
    "        opts.Curve(\n",
    "                    width=fp.pix(10.7), height=fp.pix(5), \n",
    "                    toolbar=None, hooks=[fp.margin], \n",
    "                    fontsize=fp.font_sizes['small'], \n",
    "                    line_width=12, xticks=3, yticks=3\n",
    "                    ),\n",
    "        opts.Overlay(legend_position='right', text_font='Arial')\n",
    "        )\n",
    "\n",
    "    # cluster_plot.opts(\n",
    "    #     opts.Curve(\n",
    "    #                 # width=fp.pix(10.7), height=fp.pix(5), \n",
    "    #                 # toolbar=None, hooks=[fp.margin], \n",
    "    #                 # fontsize=fp.font_sizes['small'], \n",
    "    #                 # line_width=12, \n",
    "    #                 xticks=3, yticks=3\n",
    "    #                 ),\n",
    "    #     opts.Overlay(legend_position='right', text_font='Arial')\n",
    "    # )\n",
    "\n",
    "    plot_list.append(cluster_plot)\n",
    "    # print(cluster_plot)\n",
    "\n",
    "cluster_trace_panel = hv.Layout(plot_list).opts(shared_axes=False)\n",
    "# assemble the save path\n",
    "save_path = os.path.join(paths.figures_path, '_'.join([save_name, target_parameter, 'cluster']))\n",
    "hv.save(cluster_trace_panel, save_path, fmt='png')\n",
    "\n",
    "# display the image\n",
    "cluster_trace_panel"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "grouped_parameter = data.loc[:, [target_parameter] + ['event_id', 'trial_id']].groupby(['trial_id', 'event_id']).agg(list).to_numpy()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "np.percentile(grouped_parameter, 95)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# define the target parameter and PCA\n",
    "target_parameter = 'cricket_0_mouse_distance'\n",
    "\n",
    "# plot the clusters as an image\n",
    "plot_list = []\n",
    "for name in data_dict:\n",
    "    data = data_dict[name]\n",
    "\n",
    "    # group the single traces\n",
    "    grouped_parameter = data.loc[:, [target_parameter] + ['event_id', 'trial_id']].groupby(['trial_id', 'event_id']).agg(list).to_numpy()\n",
    "\n",
    "    if 'VR' in name:\n",
    "        grouped_parameter = np.array([el for sublist in grouped_parameter for el in sublist if len(el) == 594])\n",
    "        grouped_parameter *= 100   # Convert to cm\n",
    "    elif ('Box' in name) and ('distance' in target_parameter):       \n",
    "        grouped_parameter = np.array([el for sublist in grouped_parameter for el in sublist]) \n",
    "\n",
    "    # plot all traces\n",
    "    [sorted_traces,_,_] = fp.sort_traces(grouped_parameter)\n",
    "\n",
    "    image = hv.Image(sorted_traces, ['Time','Trial #'], \n",
    "                    [target_parameter.replace('_', ' ')], \n",
    "                    bounds=[0, 0, target_data.shape[1], target_data.shape[0]]\n",
    "                    ).opts(title=name)\n",
    "\n",
    "    # For publication-ready image                \n",
    "    # image.opts(\n",
    "    #         width=fp.pix(5.8), \n",
    "    #         height=fp.pix(5.8), \n",
    "    #         toolbar=None, \n",
    "    #         hooks=[fp.margin], \n",
    "    #         fontsize=fp.font_sizes['small'], \n",
    "    #         xticks=3, yticks=3, \n",
    "    #         colorbar=True, cmap='viridis', \n",
    "    #         colorbar_opts={'major_label_text_align': 'left'}\n",
    "    #         )\n",
    "\n",
    "    image.opts(\n",
    "            # width=fp.pix(5.8), \n",
    "            # height=fp.pix(5.8), \n",
    "            toolbar=None, \n",
    "            hooks=[fp.margin], \n",
    "            #fontsize=fp.font_sizes['small'], \n",
    "            #xticks=3, yticks=3, \n",
    "            colorbar=True, cmap='viridis', \n",
    "            colorbar_opts={'major_label_text_align': 'left'}\n",
    "            )\n",
    "\n",
    "    plot_list.append(image)\n",
    "\n",
    "\n",
    "sorted_cluster_heatmap_panel = hv.Layout(plot_list).opts(shared_axes=False)\n",
    "\n",
    "# assemble the save path\n",
    "save_path = os.path.join(paths.figures_path,'_'.join([save_name, target_parameter]))\n",
    "hv.save(sorted_cluster_heatmap_panel, save_path, fmt='png')\n",
    "\n",
    "# display the image\n",
    "sorted_cluster_heatmap_panel"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UMAP embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# UMAP\n",
    "plot_list = []\n",
    "\n",
    "for transformed_data, cluster_idx, name in zip(pca_transforms, clusters, data_dict):\n",
    "    # Pull data from storage dictionary\n",
    "    data = data_dict[name]\n",
    "\n",
    "    # embed the data via UMAP\n",
    "    reducer = umap.UMAP(min_dist=0.5, n_neighbors=10)\n",
    "    embedded_data = reducer.fit_transform(transformed_data)\n",
    "\n",
    "    #--- Plot the embedding ---#\n",
    "\n",
    "    # use the cluster indexes\n",
    "    umap_data = np.concatenate((embedded_data,np.expand_dims(cluster_idx, axis=1)),axis=1)\n",
    "\n",
    "    # # use the trial ID\n",
    "    # # group the single traces\n",
    "    # grouped_parameter = data.loc[:, ['event_id', 'trial_id']].groupby(\n",
    "    #     ['trial_id']).agg(list)\n",
    "    # temp_parameter = []\n",
    "    # counter = 0\n",
    "    # for idx, el in enumerate(grouped_parameter['event_id']):\n",
    "    #     # get the event ids\n",
    "    #     event_ids = np.unique(el)\n",
    "    #     temp_parameter.append(idx*np.ones(event_ids.shape[0]))\n",
    "\n",
    "    # grouped_parameter = np.concatenate(temp_parameter, axis=0)\n",
    "    # umap_data = np.concatenate((embedded_data,np.expand_dims(grouped_parameter, axis=1)),axis=1)\n",
    "\n",
    "    # highlight the last encounter of every group\n",
    "    # allocate a list for that \n",
    "    winner_list = []\n",
    "    grouped_parameter = data.loc[:, ['event_id', 'trial_id']].groupby(['trial_id']).agg(list)\n",
    "\n",
    "    # for all the trials\n",
    "    for idx, el in enumerate(grouped_parameter['event_id']):\n",
    "        # get the event ids\n",
    "        encounter_list = np.zeros(np.unique(el).shape[0])\n",
    "        encounter_list[-1] = 1\n",
    "        winner_list.append(encounter_list)\n",
    "\n",
    "    grouped_parameter = np.concatenate(winner_list, axis=0)\n",
    "\n",
    "\n",
    "    umap_plot = hv.Scatter(umap_data, vdims=['Dim 2','cluster'], kdims=['Dim 1'])\n",
    "    umap_plot.opts(color='cluster', colorbar=True, cmap='Category10', size=5)\n",
    "\n",
    "    # # For publication-ready image      \n",
    "    # umap_plot.opts(color='cluster', colorbar=True, cmap='Category10', size=20)          \n",
    "    # umap_plot.opts(\n",
    "    #     opts.Scatter(\n",
    "    #         width=fp.pix(5.7), \n",
    "    #         height=fp.pix(7.8), \n",
    "    #         toolbar=None, \n",
    "    #         hooks=[fp.margin], \n",
    "    #         fontsize=fp.font_sizes['small'], \n",
    "    #         xticks=3, \n",
    "    #         yticks=3\n",
    "    #         )\n",
    "    #     )\n",
    "\n",
    "    umap_plot.opts(\n",
    "        opts.Scatter(\n",
    "            # width=fp.pix(5.7), \n",
    "            # height=fp.pix(7.8), \n",
    "            toolbar=None, \n",
    "            # hooks=[fp.margin], \n",
    "            fontsize=fp.font_sizes['small'], \n",
    "            xticks=3, \n",
    "            yticks=3\n",
    "            )\n",
    "        )\n",
    "\n",
    "    #             opts.Overlay(legend_position='right', text_font='Arial'))\n",
    "\n",
    "    # winner_data = embedded_data[grouped_parameter==1]\n",
    "\n",
    "    # winner_plot = hv.Scatter(winner_data, vdims=['Dim 2'], kdims=['Dim 1'])\n",
    "    # winner_plot.opts(width=fp.pix(5.7), height=fp.pix(7.8), toolbar=None, \n",
    "    #                         hooks=[fp.margin], fontsize=fp.font_sizes['small'], xticks=3, yticks=3, color='black', size=20)\n",
    "    # umap_overlay = umap_plot*winner_plot\n",
    "\n",
    "    plot_list.append(umap_plot)\n",
    "\n",
    "umap_panel = hv.Layout(plot_list)\n",
    "\n",
    "# assemble the save path\n",
    "save_path = os.path.join(paths.figures_path,'_'.join([save_name, 'umap']))\n",
    "# hv.save(umap_panel, save_path, fmt='png')\n",
    "\n",
    "# display the image\n",
    "umap_panel"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binned time analysis\n",
    "We move to using the binned time analysis to get an idea of what the overall kinematics of the scene are like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# create container for holding multiple data sets\n",
    "data_dict = {}\n",
    "\n",
    "# Load real prey capture in VR arena in the light\n",
    "search_string = 'result:succ, lighting:normal, rig:VR, analysis_type:aggBin, notes:crickets'\n",
    "ds, label = load_dataset(search_string, exclusion='obstacle', label=\"VR_light_succ\")\n",
    "data_dict[label] = ds\n",
    "\n",
    "# Load real prey capture in VR arena in the dark - successes\n",
    "search_string = 'result:succ, lighting:dark, rig:VR, analysis_type:aggBin'\n",
    "ds, label = load_dataset(search_string, label=\"VR_dark_succ\")\n",
    "data_dict[label] = ds\n",
    "\n",
    "# Load real prey capture in VR arena in the dark - failures\n",
    "search_string = 'result:fail, lighting:dark, rig:VR, analysis_type:aggBin'\n",
    "ds, label = load_dataset(search_string, label=\"VR_dark_fail\")\n",
    "data_dict[label] = ds\n",
    "\n",
    "# Load real prey capture in small box\n",
    "search_string = 'result:succ, lighting:normal, rig:miniscope, analysis_type:aggBin'\n",
    "ds, label = load_dataset(search_string, label=\"Box_light_succ\")\n",
    "data_dict[label] = ds\n",
    "\n",
    "# Get rid of doubled data set\n",
    "del ds"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot example encounter traces sorted\n",
    "\n",
    "# define the target parameter\n",
    "target_parameters = ['mouse_speed', 'cricket_0_mouse_distance', 'cricket_0_speed']\n",
    "\n",
    "\n",
    "# allocate a list for the plots\n",
    "plot_list = []\n",
    "\n",
    "keys = list(data_dict.keys())\n",
    "for name in data_dict:\n",
    "\n",
    "    data = data_dict[name]\n",
    "    cluster_idx = None\n",
    "\n",
    "    # for all the parameters\n",
    "    for target_param in target_parameters:\n",
    "\n",
    "        # load the parameter\n",
    "        try:\n",
    "            parameter = data[[target_param, 'trial_id']].copy()\n",
    "        except KeyError:\n",
    "            if ('cricket' in target_param) and ('vr' not in target_param):\n",
    "                hmap = hv.Empty()\n",
    "                plot_list.append(hmap)\n",
    "                continue\n",
    "\n",
    "        # group the single traces\n",
    "        grouped_parameter = parameter.groupby(['trial_id']).agg(list)\n",
    "        grouped_parameter = np.array([el for el in grouped_parameter[target_param]])\n",
    "        if np.argwhere(np.isinf(grouped_parameter)).size != 0:\n",
    "            grouped_parameter[grouped_parameter == np.inf] = 0\n",
    "        \n",
    "        # get the clustering for first parameter, and preserve that sorting for all other target parameters tested\n",
    "        if cluster_idx is None:\n",
    "                [sorted_traces, cluster_idx, clusters] = fp.sort_traces(grouped_parameter, nclusters=min((10, len(grouped_parameter))))\n",
    "        else:\n",
    "                sorted_traces = grouped_parameter[cluster_idx, :]\n",
    "        \n",
    "        # plot all traces\n",
    "        hmap = hv.Image(sorted_traces, ['Binned Time','Trial #'],\n",
    "                        [target_param.replace('_', ' ')], \n",
    "                        bounds=[0, 0, grouped_parameter.shape[1], grouped_parameter.shape[0]],\n",
    "                        group=name, \n",
    "                        label=target_param)\n",
    "\n",
    "        # # For publication-ready image                \n",
    "        # hmap.opts(\n",
    "        #         width=fp.pix(5.8), \n",
    "        #         height=fp.pix(5.8), \n",
    "        #         toolbar=None, \n",
    "        #         hooks=[fp.margin], \n",
    "        #         fontsize=fp.font_sizes['small'], \n",
    "        #         xticks=3, yticks=3, \n",
    "        #         colorbar=True, cmap='viridis', \n",
    "        #         colorbar_opts={'major_label_text_align': 'left'}\n",
    "        #         )\n",
    "\n",
    "        hmap.opts(\n",
    "                width=fp.pix(1.5), \n",
    "                height=fp.pix(1.5), \n",
    "                toolbar=None, \n",
    "                # hooks=[fp.margin], \n",
    "                #fontsize=fp.font_sizes['small'], \n",
    "                xticks=3, yticks=3, \n",
    "                colorbar=True, cmap='viridis', \n",
    "                colorbar_opts={'major_label_text_align': 'left'}\n",
    "                )\n",
    "\n",
    "        plot_list.append(hmap)\n",
    "\n",
    "heatmaps = hv.Layout(plot_list).cols(len(target_parameters))\n",
    "save_path = os.path.join(paths.figures_path, '_'.join([save_name, 'binned_kinematics']))\n",
    "hv.save(heatmaps, save_path, fmt='png')\n",
    "heatmaps"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot example encounter traces sorted by cluster\n",
    "\n",
    "# define the target parameter\n",
    "target_parameters = ['mouse_speed', 'cricket_0_speed', 'cricket_0_mouse_distance']\n",
    "\n",
    "# allocate a list for the plots\n",
    "plot_list = []\n",
    "\n",
    "for name in data_dict:\n",
    "\n",
    "    data = data_dict[name]\n",
    "\n",
    "    # for all the parameters\n",
    "    for target_param in target_parameters:\n",
    "\n",
    "        # load the parameter\n",
    "        try:\n",
    "            parameter = data[[target_param, 'trial_id']].copy()\n",
    "        except KeyError:\n",
    "            # handle cases where you have VR only and no real crickets\n",
    "            if ('cricket' in target_param) and ('vr' not in target_param):\n",
    "                hmap = hv.Empty()\n",
    "                plot_list.append(hmap)\n",
    "                continue\n",
    "            # handle cases when you have real only and Vr only comparisons\n",
    "            elif ('vr' in target_param) and ('VPrey' not in name):\n",
    "                hmap = hv.Empty()\n",
    "                plot_list.append(hmap)\n",
    "                continue\n",
    "\n",
    "        # group the single traces\n",
    "        grouped_parameter = parameter.groupby(['trial_id']).agg(list)\n",
    "        grouped_parameter = np.array([el for el in grouped_parameter[target_param]])\n",
    "        if np.argwhere(np.isinf(grouped_parameter)).size != 0:\n",
    "            grouped_parameter[grouped_parameter == np.inf] = 0\n",
    "\n",
    "        # For all of these, we have units of m/s or meters. Convert to cm/s or cm\n",
    "        if 'VR' in name:\n",
    "            grouped_parameter *= 100\n",
    "\n",
    "        # get the statistics of the cluster so we set the same bins for all plots of the same variable\n",
    "        # HACK: this gets rid of zero values, need to find a way to show them in the speed plots\n",
    "        # sorted_traces += 1e-10\n",
    "        # lower = np.log10(np.percentile(grouped_parameter[np.nonzero(grouped_parameter)], 1))\n",
    "        # # upper = np.log10(np.percentile(grouped_parameter, 99))\n",
    "        # # lower = np.log10(np.min(grouped_parameter[np.nonzero(grouped_parameter)]))\n",
    "        # upper = np.log10(np.floor(np.max(grouped_parameter)))\n",
    "        lower = -2\n",
    "        upper = 2.5\n",
    "\n",
    "        bin_edges = np.logspace(lower, upper, 50)\n",
    "\n",
    "        # generate histogram\n",
    "        freq, edges = np.histogram(grouped_parameter, bin_edges, density=False)\n",
    "        freq = freq / np.sum(freq)\n",
    "        # generate CDF of histogram\n",
    "        cdf = np.cumsum(freq)\n",
    "\n",
    "        # plot histogram\n",
    "        hist = hv.Histogram((edges, freq), \n",
    "                group=': '.join((name, target_param)), \n",
    "                ).opts(logx=True)\n",
    "\n",
    "        # Add cdf\n",
    "        # cum_dist = hv.Curve((edges[1:], cdf)).opts(color='green', logx=True)\n",
    "        # hist = hist * cum_dist\n",
    "\n",
    "        # Make a reference line at 1 or 10 (10cm for distance, 10cm/s for velocity)\n",
    "        # v_line = 1 if 'cricket' in target_param else 10\n",
    "        # vline = hv.VLine(v_line).opts(color='red', line_width=1, line_dash='dashed')\n",
    "        # hist = (hist * vline)\n",
    "\n",
    "        # Addx axis labels\n",
    "        if 'speed' in target_param:\n",
    "            hist.opts(xlabel='cm/s')\n",
    "        elif 'distance' in target_param:\n",
    "            hist.opts(xlabel='cm')\n",
    "\n",
    "        plot_list.append(hist)\n",
    "\n",
    "\n",
    "param_hists = hv.Layout(plot_list).opts(shared_axes=True).cols(len(target_parameters))\n",
    "save_path = os.path.join(paths.figures_path, '_'.join([save_name, 'histogram_kinematics']))\n",
    "hv.save(param_hists, save_path, fmt='png')\n",
    "param_hists"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full aggregate analysis\n",
    "Here we look at the full trace for each experiment. This is useful for getting the trial duration, but also a different view of the kinematics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# create container for holding multiple data sets\n",
    "data_dict = {}\n",
    "\n",
    "# Load real prey capture in VR arena in the light\n",
    "search_string = 'result:succ, lighting:normal, rig:VR, analysis_type:aggFull, notes:crickets'\n",
    "ds, label = load_dataset(search_string, exclusion='obstacle', label=\"VR_light_succ\")\n",
    "data_dict[label] = ds\n",
    "\n",
    "# Load real prey capture in VR arena in the dark - successes\n",
    "search_string = 'result:succ, lighting:dark, rig:VR, analysis_type:aggFull'\n",
    "ds, label = load_dataset(search_string, label=\"VR_dark_succ\")\n",
    "data_dict[label] = ds\n",
    "\n",
    "# Load real prey capture in VR arena in the dark - failures\n",
    "search_string = 'result:fail, lighting:dark, rig:VR, analysis_type:aggFull'\n",
    "ds, label = load_dataset(search_string, label=\"VR_dark_fail\")\n",
    "data_dict[label] = ds\n",
    "\n",
    "# Load real prey capture in small box\n",
    "search_string = 'result:succ, lighting:normal, rig:miniscope, analysis_type:aggFull'\n",
    "ds, label = load_dataset(search_string, label=\"Box_light_succ\")\n",
    "data_dict[label] = ds\n",
    "\n",
    "# Get rid of doubled data set\n",
    "del ds"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "np.unique(data['trial_id'], return_counts=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Get a sense of how many trials there are per data set\n",
    "for name in data_dict:\n",
    "    data = data_dict[name]\n",
    "    "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot historgrams of trial duration\n",
    "\n",
    "# allocate a list for the plots\n",
    "plot_list = []\n",
    "means = []\n",
    "dur_sem = []\n",
    "\n",
    "for name in data_dict:\n",
    "\n",
    "    data = data_dict[name]\n",
    "\n",
    "    times = data[['time_vector', 'trial_id']].copy()\n",
    "    times = times.groupby(['trial_id']).agg(list)\n",
    "    duration = np.array([trial[-1] for trial in times['time_vector']])\n",
    "\n",
    "    means.append(duration.mean())\n",
    "    dur_sem.append((name, duration.mean(), sem(duration)))\n",
    "\n",
    "    # plot the results\n",
    "    duration_histogram = hv.Bars(duration).opts(title=name, xlabel='trial', ylabel='duration')\n",
    "    plot_list.append(duration_histogram)\n",
    "\n",
    "hv.Layout(plot_list)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Separately plot mean + sem of trial duration\n",
    "\n",
    "# Plot of means\n",
    "trial_means = hv.Bars((list(data_dict.keys()), means)).opts(title='Mean Trial Duration', ylabel='Duration (s)', ylim=(0,300), xrotation=45) \n",
    "trial_means = hv.ErrorBars(dur_sem) * trial_means\n",
    "save_path = os.path.join(paths.figures_path, '_'.join([save_name, 'duration_means']))\n",
    "hv.save(trial_means, save_path, fmt='png')\n",
    "\n",
    "# dispaly the iamge\n",
    "enc_means"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('prey_capture': conda)",
   "metadata": {
    "interpreter": {
     "hash": "a87906ed62866a44251634d622def361af2400f308708d93dff50a5577b9f27c"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
