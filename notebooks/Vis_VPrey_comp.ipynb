{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Visualize comparisons between real and virtual prey capture attempts\n",
    "This notebook only deals with purely virtual prey capture - there are no mixed VR+Real cricket experiments here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, r'..\\..')\n",
    "import paths\n",
    "\n",
    "import panel as pn\n",
    "import holoviews as hv\n",
    "from holoviews import opts, dim\n",
    "hv.extension('bokeh')\n",
    "from bokeh.resources import INLINE\n",
    "\n",
    "import functions_bondjango as bd\n",
    "import functions_kinematic as fk\n",
    "import functions_plotting as fp\n",
    "import functions_misc as fm\n",
    "import functions_data_handling as fd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "from scipy.stats import sem\n",
    "import sklearn.decomposition as decomp\n",
    "import umap\n",
    "import sklearn.mixture as mix\n",
    "from scipy.stats import sem\n",
    "\n",
    "\n",
    "line_width = 5"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Define a data loading function\n",
    "\n",
    "def load_dataset(search_string, label=None, exclusion=None):\n",
    "    # load the data\n",
    "    # get the data paths\n",
    "    try:\n",
    "        data_path = snakemake.input[0]\n",
    "    except NameError:\n",
    "        # query the database for data to plot\n",
    "        data_all = bd.query_database('analyzed_data', search_string)\n",
    "\n",
    "        if exclusion is not None:\n",
    "            for ds in data_all:\n",
    "                if exclusion not in ds['analysis_path']:\n",
    "                    data_path = ds['analysis_path']\n",
    "                    data_date = ds['date']\n",
    "                    break\n",
    "        else:\n",
    "            data_path = data_all[0]['analysis_path']\n",
    "            data_date = data_all[0]['date']\n",
    "    print(data_path)\n",
    "    print(data_date)\n",
    "\n",
    "    # assemble a label for this data set\n",
    "    if label is None:\n",
    "        d = fd.parse_search_string(search_string)\n",
    "        label = '_'.join([d['rig'], d['lighting'], d['result'], d['notes']])\n",
    "    print('data label: ' + label + '\\n')\n",
    "\n",
    "    # load the data\n",
    "    return fd.aggregate_loader(data_path), label"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# define the name to be used for the saved figures\n",
    "save_name = 'VPrey_crickets_0_vrcrickets_1'"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Encounter analysis\n",
    "This analysis gets us an idea of the number of encounters, as well as encounter types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Load the encounter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load new data\n",
    "# create container for holding multiple data sets\n",
    "data_dict = {}\n",
    "\n",
    "# Load real prey capture in the light - this is a baseline comparison\n",
    "search_string = 'result:succ, lighting:normal, rig:VR, analysis_type:aggEnc'\n",
    "ds, label = load_dataset(search_string, exclusion='obstacle', label=\"VR_light_succ\")\n",
    "data_dict[label] = ds\n",
    "\n",
    "# Load VR prey capture with VR blackCr\n",
    "search_string = 'result:test, lighting:normal, rig:VPrey, analysis_type:aggEnc, notes:blackCr_rewarded_crickets_0_vrcrickets_1'\n",
    "ds, label = load_dataset(search_string, label=\"VPrey_blackCr_lightBG\")\n",
    "data_dict[label] = ds\n",
    "\n",
    "# Load VR prey capture with VR whiteCr_blackBG\n",
    "search_string = 'result:test, lighting:normal, rig:VPrey, analysis_type:aggEnc, notes:whiteCr_blackBG_rewarded_crickets_0_vrcrickets_1'\n",
    "ds, label = load_dataset(search_string, label=\"VPrey_whiteCr_blackBG\")\n",
    "data_dict[label] = ds\n",
    "\n",
    "# Load VR  prey capture with VR blackCr_grayBG\n",
    "search_string = 'result:test, lighting:normal, rig:VPrey, analysis_type:aggEnc, notes:blackCr_grayBG_rewarded_crickets_0_vrcrickets_1'\n",
    "ds, label = load_dataset(search_string, label=\"VPrey_blackCr_grayBG\")\n",
    "data_dict[label] = ds\n",
    "\n",
    "# Load VR prey capture with VR whiteCr_grayBG\n",
    "search_string = 'result:test, lighting:normal, rig:VPrey, analysis_type:aggEnc, notes:whiteCr_grayBG_rewarded_crickets_0_vrcrickets_1'\n",
    "ds, label = load_dataset(search_string, label=\"VPrey_whiteCr_grayBG\")\n",
    "data_dict[label] = ds\n",
    "\n",
    "# Get rid of doubled data set\n",
    "del ds"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Encounter averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "source": [
    "plot_dict = {}\n",
    "\n",
    "# encounter variables for real cricket\n",
    "encounter_angle_variables = ['mouse_heading', 'cricket_0_heading', 'cricket_0_delta_heading']\n",
    "encounter_nonangle_variables = ['cricket_0_mouse_distance', 'mouse_speed', 'mouse_acceleration', 'cricket_0_speed', 'cricket_0_acceleration']\n",
    "\n",
    "# encounter variables for VR cricket(s)\n",
    "encounter_angle_variables_VR = ['mouse_heading', 'vrcricket_0_heading', 'vrcricket_0_delta_heading']\n",
    "encounter_nonangle_variables_VR = ['vrcricket_0_mouse_distance', 'mouse_speed', 'mouse_acceleration', 'vrcricket_0_speed', 'vrcricket_0_acceleration']\n",
    "\n",
    "for name in data_dict:\n",
    "    print(name)\n",
    "    data = data_dict[name]\n",
    "    plot_container = {}\n",
    "\n",
    "    enc_ang_var = encounter_angle_variables\n",
    "    enc_nang_var = encounter_nonangle_variables\n",
    "\n",
    "    try:\n",
    "        angled_params = data[enc_ang_var]\n",
    "        angled_params['event_index'] = data.index\n",
    "\n",
    "        nonangled_params = data[enc_nang_var]\n",
    "        nonangled_params['event_index'] = data.index\n",
    "    except KeyError:\n",
    "        if ('VPrey' in name):\n",
    "            enc_ang_var = encounter_angle_variables_VR\n",
    "            enc_nang_var = encounter_nonangle_variables_VR\n",
    "\n",
    "            angled_params = data[enc_ang_var]\n",
    "            angled_params['event_index'] = data.index\n",
    "\n",
    "            nonangled_params = data[enc_nang_var]\n",
    "            nonangled_params['event_index'] = data.index\n",
    "\n",
    "\n",
    "    angled_average = fk.wrap(angled_params.groupby('event_index').agg(lambda x: 180 + fk.circmean_deg(x)))\n",
    "    angled_std = pd.DataFrame(fk.unwrap(angled_params.groupby('event_index').agg(lambda x: fk.circstd_deg(x)/np.sqrt(x.shape[0]))), columns=enc_ang_var)\n",
    "\n",
    "    nonangled_average = nonangled_params.groupby('event_index').mean()\n",
    "    nonangled_std = pd.DataFrame(nonangled_params.groupby('event_index').sem(), columns=enc_nang_var)\n",
    "\n",
    "    encounter_average = pd.concat((angled_average, nonangled_average), axis=1)\n",
    "    encounter_sem = pd.concat((angled_std, nonangled_std), axis=1)\n",
    "\n",
    "    # plot the results\n",
    "    # define the variables to plot from\n",
    "    encounter_variables = enc_ang_var + enc_nang_var\n",
    "    # get the trials\n",
    "    trial_list = data['trial_id'].unique()\n",
    "    # get the time vector\n",
    "    time_vector = data.loc[(data['event_id'] == 0) & (data['trial_id'] == trial_list[0]), 'time_vector'].to_numpy()\n",
    "\n",
    "    # for each of the variables\n",
    "    for var_count, variable in enumerate(encounter_variables):\n",
    "        x = np.arange(encounter_average[variable].size)\n",
    "        y = encounter_average[variable].to_numpy()\n",
    "        yerr = encounter_sem[variable].to_numpy()\n",
    "\n",
    "        plot_container[variable] = hv.Spread((list(x), list(y), list(yerr)), label=name).opts(title=variable) * hv.Curve((list(x), list(y))).opts(color='black')\n",
    "\n",
    "\n",
    "    plot_dict[('variables', name)] = hv.GridSpace(plot_container, kdims=['variable'])\n",
    "\n",
    "encounters = hv.GridSpace(plot_dict, kdims=['variables', 'dataset']).opts(plot_size=300)\n",
    "full_panel = pn.panel(encounters, center=True, widget_location='top')\n",
    "full_panel\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Number of encounters per trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "source": [
    "# Plot the number of encounters per trial for each condition\n",
    "\n",
    "# allocate a list for the plots\n",
    "plot_list = []\n",
    "means = []\n",
    "enc_sem = []\n",
    "\n",
    "# Plots by trial type\n",
    "for name in data_dict:\n",
    "    data = data_dict[name]\n",
    "\n",
    "    # load the parameter\n",
    "    parameter = data[['event_id','trial_id']].copy()\n",
    "    # find the number of encounters\n",
    "    grouped_parameter = parameter.groupby(['trial_id']).agg(list)\n",
    "    encounters = np.array([el[-1] for el in grouped_parameter['event_id']]) + 1\n",
    "    means.append(encounters.mean())\n",
    "    enc_sem.append((name, encounters.mean(), sem(encounters)))\n",
    "\n",
    "    # plot the results\n",
    "    enc_plot = hv.Bars((np.arange(encounters.shape[0]), encounters)).opts(title=name, xlabel='trial', ylabel='# encounters') * \\\n",
    "        hv.HLine(encounters.mean()).opts(color='red', line_width=1)\n",
    "    plot_list.append(enc_plot)\n",
    "\n",
    "encounters_panel = hv.Layout(plot_list).opts(shared_axes=True)\n",
    "save_path = os.path.join(paths.figures_path, '_'.join([save_name, 'encounters']))\n",
    "# hv.save(encounters_panel, save_path, fmt='png')\n",
    "\n",
    "# display the image\n",
    "encounters_panel"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "source": [
    "# Separately plot mean + sem of encounters\n",
    "\n",
    "# Plot of means\n",
    "enc_means = hv.Bars((list(data_dict.keys()), means)).opts(title='Mean # Encounters', ylabel='# encounters', ylim=(0,8), xrotation=45) \n",
    "enc_means = hv.ErrorBars(enc_sem) * enc_means\n",
    "save_path = os.path.join(paths.figures_path, '_'.join([save_name, 'encounter_means']))\n",
    "hv.save(enc_means, save_path, fmt='png')\n",
    "\n",
    "# dispaly the iamge\n",
    "enc_means"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### PCA of encounter types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# define the target parameter and PCA\n",
    "target_parameter = 'cricket_0_mouse_distance'\n",
    "\n",
    "# container for plots\n",
    "plot_list = []\n",
    "\n",
    "# container for PCA fit\n",
    "pca_transforms = []\n",
    "\n",
    "# container for target data\n",
    "target = []\n",
    "\n",
    "for name in data_dict:\n",
    "    data = data_dict[name]\n",
    "    \n",
    "    # assemble the array with the parameters of choice\n",
    "    try:\n",
    "        target_data = data[[target_parameter] + ['event_id', 'trial_id']].groupby(['trial_id', 'event_id']).agg(list).to_numpy()\n",
    "    except KeyError:\n",
    "        target_parameter = 'vr' + target_parameter\n",
    "        target_data = data[[target_parameter] + ['event_id', 'trial_id']].groupby(['trial_id', 'event_id']).agg(list).to_numpy()\n",
    "\n",
    "    # HACK Find a way to fix\n",
    "    target_data = np.array([el for sublist in target_data for el in sublist if len(el) == 594])\n",
    "    target.append(target_data)\n",
    "\n",
    "    # Remove nans or infs\n",
    "    np.nan_to_num(target_data,  0)\n",
    "    if np.argwhere(np.isinf(target_data)).size != 0:\n",
    "            target_data[target_data == np.inf] = 0\n",
    "\n",
    "    # PCA the data before clustering\n",
    "    pca = decomp.PCA()\n",
    "    transformed_data = pca.fit_transform(target_data)\n",
    "    pca_transforms.append(transformed_data)\n",
    "\n",
    "    # fp.plot_2d([[pca.explained_variance_ratio_]])\n",
    "    exp_var = hv.Curve(pca.explained_variance_ratio_).opts(xlabel='PCs', ylabel='explained variance', title=name)\n",
    "    plot_list.append(exp_var)\n",
    "\n",
    "hv.Layout(plot_list).cols(len(data_dict.keys()))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Guassian mixture model of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Cluster the data using GMMs\n",
    "plot_list = []\n",
    "clusters = []\n",
    "\n",
    "for transformed_data, name in zip(pca_transforms, data_dict):\n",
    "    \n",
    "    # define the vector of components\n",
    "    component_vector = [2, 3, 4, 5, 10, 15]\n",
    "    # allocate memory for the results\n",
    "    gmms = []\n",
    "    # for all the component numbers\n",
    "    for comp in component_vector:\n",
    "        # # define the number of components\n",
    "        gmm = mix.GaussianMixture(n_components=comp, covariance_type='diag', n_init=50)\n",
    "        gmm.fit(transformed_data[:, :7])                 # Pull the first 7 PCs and fit a GMM\n",
    "        gmms.append(gmm.bic(transformed_data[:, :7]))    # Get the bayesian information criterion\n",
    "\n",
    "    # select the minimum bic number of components\n",
    "    n_components = np.array(component_vector)[np.argmin(gmms)]\n",
    "    # predict the cluster indexes\n",
    "    gmm = mix.GaussianMixture(n_components=n_components, covariance_type='diag', n_init=50)\n",
    "    cluster_idx = gmm.fit_predict(transformed_data[:, :7])\n",
    "\n",
    "    # discard singletons\n",
    "    # turn cluster_idx in a float\n",
    "    cluster_idx = cluster_idx.astype(float)\n",
    "    # get the IDs\n",
    "    clu_unique = np.unique(cluster_idx)\n",
    "    for clu in clu_unique:\n",
    "        # get the number of traces in the cluster\n",
    "        number_traces = sum(cluster_idx==clu)\n",
    "        # if it's less than 5, eliminate the cluster\n",
    "        if number_traces < 5:\n",
    "            cluster_idx[cluster_idx==clu] = np.nan\n",
    "    clusters.append(cluster_idx)\n",
    "        \n",
    "    # plot the BIC\n",
    "    BIC = hv.Curve((component_vector, gmms)).opts(title=name, xlabel='cluster', ylabel='BIC')\n",
    "    plot_list.append(BIC)\n",
    "\n",
    "hv.Layout(plot_list).cols(len(data_dict.keys()))\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "source": [
    "# plot the clusters\n",
    "plot_list = []\n",
    "\n",
    "for target_data, cluster_idx, name in zip(target, clusters, data_dict):\n",
    "\n",
    "    # add the cluster indexes to the dataframe\n",
    "    cluster_data = np.array([np.mean(target_data[cluster_idx == el, :], axis=0) for el in np.arange(n_components)])\n",
    "    cluster_std = np.array([np.std(target_data[cluster_idx == el, :], axis=0) / np.sqrt(np.sum(cluster_idx == el))\n",
    "                            for el in np.arange(n_components)])\n",
    "    # plot the results\n",
    "    cluster_plot = hv.Overlay(\n",
    "        [hv.Curve(el, label=str(idx), kdims=['Time (s)'], vdims=[target_parameter.replace('_', ' ')+' (px)']) for idx, el in enumerate (cluster_data)] + \n",
    "        [hv.Spread((np.arange(el.shape[0]),el,cluster_std[idx, :])) for idx, el in enumerate(cluster_data)]\n",
    "        )\n",
    "\n",
    "    cluster_plot.relabel('Clusters').opts({'Curve': dict(color=hv.Palette('Category20')), \n",
    "                                           'Spread': dict(color=hv.Palette('Category20'))})\n",
    "    \n",
    "    cluster_plot.opts(title=name)\n",
    "\n",
    "    # For publication-ready image\n",
    "    cluster_plot.opts(\n",
    "        opts.Curve(\n",
    "                    width=fp.pix(10.7), height=fp.pix(5), \n",
    "                    toolbar=None, hooks=[fp.margin], \n",
    "                    fontsize=fp.font_sizes['small'], \n",
    "                    line_width=12, xticks=3, yticks=3\n",
    "                    ),\n",
    "        opts.Overlay(legend_position='right', text_font='Arial')\n",
    "        )\n",
    "\n",
    "    # cluster_plot.opts(\n",
    "    #     opts.Curve(\n",
    "    #                 # width=fp.pix(10.7), height=fp.pix(5), \n",
    "    #                 # toolbar=None, hooks=[fp.margin], \n",
    "    #                 # fontsize=fp.font_sizes['small'], \n",
    "    #                 # line_width=12, \n",
    "    #                 xticks=3, yticks=3\n",
    "    #                 ),\n",
    "    #     opts.Overlay(legend_position='right', text_font='Arial')\n",
    "    # )\n",
    "\n",
    "    plot_list.append(cluster_plot)\n",
    "    # print(cluster_plot)\n",
    "\n",
    "cluster_trace_panel = hv.Layout(plot_list).opts(shared_axes=False).cols(1)\n",
    "# assemble the save path\n",
    "save_path = os.path.join(paths.figures_path, '_'.join([save_name, target_parameter, 'cluster']))\n",
    "hv.save(cluster_trace_panel, save_path, fmt='png')\n",
    "\n",
    "# display the image\n",
    "cluster_trace_panel"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "source": [
    "# plot the clusters as an image\n",
    "plot_list = []\n",
    "target_parameter = 'cricket_0_mouse_distance'\n",
    "# target_parameter = 'mouse_speed'\n",
    "\n",
    "keys = list(data_dict.keys())\n",
    "\n",
    "for name in data_dict:\n",
    "    data = data_dict[name]\n",
    "\n",
    "    # group the single traces\n",
    "    try:\n",
    "        target_data = data[[target_parameter] + ['event_id', 'trial_id']].groupby(['trial_id', 'event_id']).agg(list).to_numpy()\n",
    "    except KeyError:\n",
    "        target_parameter = 'vr' + target_parameter\n",
    "        target_data = data[[target_parameter] + ['event_id', 'trial_id']].groupby(['trial_id', 'event_id']).agg(list).to_numpy()\n",
    "\n",
    "    # print(grouped_parameter)\n",
    "    target_data = np.array([el for sublist in target_data for el in sublist if len(el) == 594])\n",
    "\n",
    "    # Remove nans or infs\n",
    "    # print(np.where(np.isnan(grouped_parameter))[0].shape)\n",
    "    np.nan_to_num(target_data,  0.0)\n",
    "    if np.argwhere(np.isinf(target_data)).size != 0:\n",
    "        # print(np.argwhere(np.isinf(grouped_parameter)).size)\n",
    "        target_data[target_data == np.inf] = 0\n",
    "\n",
    "    # Trim to the 1st and 99th percentiles to get rid of massive outliers\n",
    "    lower = np.percentile(target_data, 1)\n",
    "    upper = np.percentile(target_data, 99)\n",
    "    target_data[target_data > upper] = upper\n",
    "\n",
    "    # normalize\n",
    "    target_data /= np.max(target_data)\n",
    "        \n",
    "    # plot all traces\n",
    "    [sorted_traces,_,_] = fp.sort_traces(target_data)\n",
    "\n",
    "    image = hv.Image(sorted_traces, ['Time','Encounter'], \n",
    "                    [target_parameter.replace('_', ' ')], \n",
    "                    bounds=[0, 0, target_data.shape[1], target_data.shape[0]]\n",
    "                    ).opts(title=name)\n",
    "                    \n",
    "    # For publication-ready image                \n",
    "    # image.opts(\n",
    "    #         width=fp.pix(5.8), \n",
    "    #         height=fp.pix(5.8), \n",
    "    #         toolbar=None, \n",
    "    #         hooks=[fp.margin], \n",
    "    #         fontsize=fp.font_sizes['small'], \n",
    "    #         xticks=3, yticks=3, \n",
    "    #         colorbar=True, cmap='viridis', \n",
    "    #         colorbar_opts={'major_label_text_align': 'left'}\n",
    "    #         )\n",
    "\n",
    "    image.opts(\n",
    "            # width=fp.pix(5.8), \n",
    "            # height=fp.pix(5.8), \n",
    "            toolbar=None, \n",
    "            hooks=[fp.margin], \n",
    "            #fontsize=fp.font_sizes['small'], \n",
    "            #xticks=3, yticks=3, \n",
    "            colorbar=True, cmap='viridis', \n",
    "            colorbar_opts={'major_label_text_align': 'left'}\n",
    "            )\n",
    "\n",
    "    vline = hv.VLine(300).opts(color='red', line_width=1, line_dash='dashed')\n",
    "    image = (image * vline)\n",
    "\n",
    "    plot_list.append(image)\n",
    "    \n",
    "\n",
    "sorted_cluster_heatmap_panel = hv.Layout(plot_list).opts(shared_axes=False).cols(len(data_dict.keys()))\n",
    "\n",
    "# assemble the save path\n",
    "save_path = os.path.join(paths.figures_path,'_'.join([save_name, target_parameter, 'heatmap']))\n",
    "hv.save(sorted_cluster_heatmap_panel, save_path, fmt='png')\n",
    "\n",
    "# display the image\n",
    "sorted_cluster_heatmap_panel"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "source": [
    "# UMAP\n",
    "plot_list = []\n",
    "\n",
    "for transformed_data, cluster_idx, name in zip(pca_transforms, clusters, data_dict):\n",
    "\n",
    "    # embed the data via UMAP\n",
    "    reducer = umap.UMAP(min_dist=0.25, n_neighbors=15)\n",
    "    embedded_data = reducer.fit_transform(transformed_data)\n",
    "\n",
    "    #--- Plot the embedding ---#\n",
    "\n",
    "    # use the cluster indexes\n",
    "    umap_data = np.concatenate((embedded_data, np.expand_dims(cluster_idx, axis=1)), axis=1)\n",
    "\n",
    "    # # use the trial ID\n",
    "    # # group the single traces\n",
    "    # grouped_parameter = data.loc[:, ['event_id', 'trial_id']].groupby(['trial_id']).agg(list)\n",
    "    # temp_parameter = []\n",
    "    # counter = 0\n",
    "    # for idx, el in enumerate(grouped_parameter['event_id']):\n",
    "    #     # get the event ids\n",
    "    #     event_ids = np.unique(el)\n",
    "    #     temp_parameter.append(idx * np.ones(event_ids.shape[0]))\n",
    "\n",
    "    # grouped_parameter = np.concatenate(temp_parameter, axis=0)\n",
    "    # umap_data = np.concatenate((embedded_data,np.expand_dims(grouped_parameter, axis=1)),axis=1)\n",
    "\n",
    "    # highlight the last encounter of every group\n",
    "    # allocate a list for that \n",
    "    winner_list = []\n",
    "    grouped_parameter = data.loc[:, ['event_id', 'trial_id']].groupby(['trial_id']).agg(list)\n",
    "\n",
    "    # for all the trials\n",
    "    for idx, el in enumerate(grouped_parameter['event_id']):\n",
    "        # get the event ids\n",
    "        encounter_list = np.zeros(np.unique(el).shape[0])\n",
    "        encounter_list[-1] = 1\n",
    "        winner_list.append(encounter_list)\n",
    "\n",
    "    grouped_parameter = np.concatenate(winner_list, axis=0)\n",
    "\n",
    "\n",
    "    umap_plot = hv.Scatter(umap_data, vdims=['Dim 2','cluster'], kdims=['Dim 1'])\n",
    "    # umap_plot.opts(color='cluster', colorbar=True, cmap='Category10', size=5)\n",
    "\n",
    "    # # For publication-ready image      \n",
    "    umap_plot.opts(color='cluster', colorbar=True, cmap='Category10', size=20, title=name)          \n",
    "    umap_plot.opts(\n",
    "        opts.Scatter(\n",
    "            width=fp.pix(5.7), \n",
    "            height=fp.pix(7.8), \n",
    "            toolbar=None, \n",
    "            hooks=[fp.margin], \n",
    "            fontsize=fp.font_sizes['small'], \n",
    "            xticks=3, \n",
    "            yticks=3\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # umap_plot.opts(\n",
    "    #     opts.Scatter(\n",
    "    #         # width=fp.pix(5.7), \n",
    "    #         # height=fp.pix(7.8), \n",
    "    #         toolbar=None, \n",
    "    #         # hooks=[fp.margin], \n",
    "    #         fontsize=fp.font_sizes['small'], \n",
    "    #         xticks=3, \n",
    "    #         yticks=3\n",
    "    #         )\n",
    "    #     )\n",
    "\n",
    "    #             opts.Overlay(legend_position='right', text_font='Arial'))\n",
    "\n",
    "    # winner_data = embedded_data[grouped_parameter==1]\n",
    "\n",
    "    # winner_plot = hv.Scatter(winner_data, vdims=['Dim 2'], kdims=['Dim 1'])\n",
    "    # winner_plot.opts(width=fp.pix(5.7), height=fp.pix(7.8), toolbar=None, \n",
    "    #                         hooks=[fp.margin], fontsize=fp.font_sizes['small'], xticks=3, yticks=3, color='black', size=20)\n",
    "    # umap_overlay = umap_plot*winner_plot\n",
    "\n",
    "    plot_list.append(umap_plot)\n",
    "\n",
    "umap_panel = hv.Layout(plot_list).opts(shared_axes=True).cols(len(data_dict))\n",
    "\n",
    "# assemble the save path\n",
    "save_path = os.path.join(paths.figures_path,'_'.join([save_name, 'umap']))\n",
    "hv.save(umap_panel, save_path, fmt='png')\n",
    "\n",
    "# display the image\n",
    "umap_panel"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Binned time analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load new data\n",
    "# create container for holding multiple data sets\n",
    "data_dict = {}\n",
    "\n",
    "# Load real prey capture in the light - this is a baseline comparison\n",
    "search_string = 'result:succ, lighting:normal, rig:VR, analysis_type:aggBin'\n",
    "ds, label = load_dataset(search_string, exclusion='obstacle', label=\"VR_light_succ\")\n",
    "data_dict[label] = ds\n",
    "\n",
    "# Load VR prey capture with VR blackCr\n",
    "search_string = 'result:test, lighting:normal, rig:VPrey, analysis_type:aggBin, notes:blackCr_rewarded_crickets_0_vrcrickets_1'\n",
    "ds, label = load_dataset(search_string, label=\"VPrey_blackCr_lightBG\")\n",
    "data_dict[label] = ds\n",
    "\n",
    "# # Load VR prey capture with VR whiteCr_blackBG\n",
    "# search_string = 'result:test, lighting:normal, rig:VPrey, analysis_type:aggBin, notes:whiteCr_blackBG_rewarded_crickets_0_vrcrickets_1'\n",
    "# ds, label = load_dataset(search_string, label=\"VPrey_whiteCr_blackBG\")\n",
    "# data_dict[label] = ds\n",
    "\n",
    "# # Load VR  prey capture with VR blackCr_grayBG\n",
    "# search_string = 'result:test, lighting:normal, rig:VPrey, analysis_type:aggBin, notes:blackCr_grayBG_rewarded_crickets_0_vrcrickets_1'\n",
    "# ds, label = load_dataset(search_string, label=\"VPrey_blackCr_grayBG\")\n",
    "# data_dict[label] = ds\n",
    "\n",
    "# # Load VR prey capture with VR whiteCr_grayBG\n",
    "# search_string = 'result:test, lighting:normal, rig:VPrey, analysis_type:aggBin, notes:whiteCr_grayBG_rewarded_crickets_0_vrcrickets_1'\n",
    "# ds, label = load_dataset(search_string, label=\"VPrey_whiteCr_grayBG\")\n",
    "# data_dict[label] = ds\n",
    "\n",
    "# Get rid of doubled data set\n",
    "del ds"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Heatmaps of encounter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "source": [
    "# Plot example encounter traces sorted\n",
    "\n",
    "# define the target parameter\n",
    "# target_parameters = ['mouse_speed', 'vrcricket_0_mouse_distance', 'vrcricket_0_speed', 'cricket_0_mouse_distance', 'cricket_0_speed']\n",
    "target_parameters = ['mouse_speed', 'cricket_0_mouse_distance', 'cricket_0_speed']\n",
    "\n",
    "\n",
    "# allocate a list for the plots\n",
    "plot_list = []\n",
    "\n",
    "keys = list(data_dict.keys())\n",
    "for name in data_dict:\n",
    "\n",
    "    data = data_dict[name]\n",
    "    cluster_idx = None\n",
    "\n",
    "    # for all the parameters\n",
    "    for target_param in target_parameters:\n",
    "\n",
    "        # load the parameter\n",
    "        try:\n",
    "            parameter = data[[target_param, 'trial_id']].copy()\n",
    "        except KeyError:\n",
    "            target_param = 'vr' + target_param\n",
    "            parameter = data[[target_param, 'trial_id']].copy()\n",
    "\n",
    "        # group the single traces\n",
    "        grouped_parameter = parameter.groupby(['trial_id']).agg(list)\n",
    "        grouped_parameter = np.array([el for el in grouped_parameter[target_param]])\n",
    "\n",
    "        # Remove nans or infs\n",
    "        np.nan_to_num(grouped_parameter, 0)\n",
    "        if np.argwhere(np.isinf(grouped_parameter)).size != 0:\n",
    "            grouped_parameter[grouped_parameter == np.inf] = 0\n",
    "\n",
    "    # Trim to the 1st and 99th percentiles to get rid of massive outliers\n",
    "        lower = np.percentile(grouped_parameter, 1)\n",
    "        upper = np.percentile(grouped_parameter, 99)\n",
    "        grouped_parameter[grouped_parameter > upper] = upper\n",
    "\n",
    "        # normalize to 1\n",
    "        # grouped_parameter /= np.max(grouped_parameter)\n",
    "        \n",
    "        # get the clustering for first parameter, and preserve that sorting for all other target parameters tested\n",
    "        if cluster_idx is None:\n",
    "                [sorted_traces, cluster_idx, clusters] = fp.sort_traces(grouped_parameter, nclusters=min((10, len(grouped_parameter))))\n",
    "        else:\n",
    "                sorted_traces = grouped_parameter[cluster_idx, :]\n",
    "        \n",
    "        # plot all traces\n",
    "        hmap = hv.Image(sorted_traces, ['Binned Time','Trial #'],\n",
    "                        [target_param.replace('_', ' ')], \n",
    "                        bounds=[0, 0, grouped_parameter.shape[1], grouped_parameter.shape[0]],\n",
    "                        group=name, \n",
    "                        label=target_param)\n",
    "\n",
    "        # # For publication-ready image                \n",
    "        # hmap.opts(\n",
    "        #         width=fp.pix(5.8), \n",
    "        #         height=fp.pix(5.8), \n",
    "        #         toolbar=None, \n",
    "        #         hooks=[fp.margin], \n",
    "        #         fontsize=fp.font_sizes['small'], \n",
    "        #         xticks=3, yticks=3, \n",
    "        #         colorbar=True, cmap='viridis', \n",
    "        #         colorbar_opts={'major_label_text_align': 'left'}\n",
    "        #         )\n",
    "\n",
    "        hmap.opts(\n",
    "                width=fp.pix(1.5), \n",
    "                height=fp.pix(1.5), \n",
    "                toolbar=None, \n",
    "                # hooks=[fp.margin], \n",
    "                #fontsize=fp.font_sizes['small'], \n",
    "                xticks=3, yticks=3, \n",
    "                colorbar=True, cmap='viridis', \n",
    "                colorbar_opts={'major_label_text_align': 'left'}\n",
    "                )\n",
    "\n",
    "        plot_list.append(hmap)\n",
    "\n",
    "heatmaps = hv.Layout(plot_list).cols(len(target_parameters)).opts(shared_axes=False)\n",
    "save_path = os.path.join(paths.figures_path, '_'.join([save_name, 'binned_kinematics']))\n",
    "hv.save(heatmaps, save_path, fmt='png')\n",
    "heatmaps"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "from scipy.stats import lognorm"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "source": [
    "# Plot example encounter traces sorted by cluster\n",
    "\n",
    "# define the target parameter\n",
    "target_parameters = ['mouse_speed', 'cricket_0_speed', 'cricket_0_mouse_distance']\n",
    "# target_parameters = ['vrcricket_0_speed']\n",
    "\n",
    "# allocate a list for the plots\n",
    "plot_list = []\n",
    "\n",
    "keys = list(data_dict.keys())\n",
    "\n",
    "for name in data_dict:\n",
    "\n",
    "    data = data_dict[name]\n",
    "\n",
    "    # for all the parameters\n",
    "    for target_param in target_parameters:\n",
    "\n",
    "       # load the parameter\n",
    "        try:\n",
    "            parameter = data[[target_param, 'trial_id']].copy()\n",
    "        except KeyError:\n",
    "            target_param = 'vr' + target_param\n",
    "            parameter = data[[target_param, 'trial_id']].copy()\n",
    "\n",
    "        # group the single traces\n",
    "        grouped_parameter = parameter.groupby(['trial_id']).agg(list)\n",
    "        grouped_parameter = np.array([el for el in grouped_parameter[target_param]])\n",
    "\n",
    "        # remove nans or infs\n",
    "        np.nan_to_num(grouped_parameter, 0)\n",
    "        if np.argwhere(np.isinf(grouped_parameter)).size != 0:\n",
    "            grouped_parameter[grouped_parameter == np.inf] = 0\n",
    "            # idxs = np.argwhere(np.isinf(grouped_parameter))\n",
    "            # for idx in idxs:\n",
    "            #     grouped_parameter[idx[0], idx[1]] = 0\n",
    "\n",
    "        # For all of these, we have units of m/s or meters. Convert to cm/s or cm\n",
    "        grouped_parameter *= 100\n",
    "\n",
    "        # get the statistics of the cluster so we set the same bins for all plots of the same variable\n",
    "        # HACK: this gets rid of zero values, need to find a way to show them in the speed plots\n",
    "        # sorted_traces += 1e-10\n",
    "        # lower = np.log10(np.percentile(grouped_parameter[np.nonzero(grouped_parameter)], 1))\n",
    "        # # upper = np.log10(np.percentile(grouped_parameter, 99))\n",
    "        # # lower = np.log10(np.min(grouped_parameter[np.nonzero(grouped_parameter)]))\n",
    "        # upper = np.log10(np.floor(np.max(grouped_parameter)))\n",
    "        lower = -2\n",
    "        upper = 2.5\n",
    "\n",
    "        bin_edges = np.logspace(lower, upper, 50)\n",
    "        \n",
    "\n",
    "        # Go through each cluster and plot on a histogram\n",
    "        # 23.09.20202 - DO not need clustering here, makes display bad\n",
    "        # plot all traces\n",
    "        # overlay_list = []\n",
    "        # for clu in np.unique(clusters):\n",
    "        #     idxs = cluster_idx[clusters == clu]\n",
    "        #     cluster_traces = sorted_traces[idxs, :]\n",
    "            \n",
    "        #     freq, edges = np.histogram(cluster_traces, bin_edges)\n",
    "            \n",
    "        #     hist = hv.Histogram((edges, freq), \n",
    "        #         group=': '.join((name, target_param)), \n",
    "        #         label=str(clu+1)\n",
    "        #         )\n",
    "        #     hist.opts(logx=True, alpha=0.3)\n",
    "\n",
    "        #     # If we are the last cluster, add a reference line\n",
    "        #     if clu == max(clusters):\n",
    "        #         vline = hv.VLine(1e-2).opts(color='red', line_width=1, line_dash='dashed')\n",
    "        #         hist = (hist * vline)\n",
    "            \n",
    "        #     overlay_list.append(hist)\n",
    "\n",
    "        # # Create an overlay of all the cluster histograms\n",
    "        # full_overlay = hv.Overlay(overlay_list)\n",
    "        # full_overlay.opts(legend_position='right', width=fp.pix(1.5), height=fp.pix(1.5))\n",
    "\n",
    "        # plot_list.append(full_overlay)\n",
    "\n",
    "        # generate histogram\n",
    "        freq, edges = np.histogram(grouped_parameter, bin_edges, density=False)\n",
    "        freq = freq / np.sum(freq)\n",
    "        # generate CDF of histogram\n",
    "        cdf = np.cumsum(freq)\n",
    "\n",
    "        # plot histogram\n",
    "        hist = hv.Histogram((edges, freq), \n",
    "                group=': '.join((name, target_param)), \n",
    "                ).opts(logx=True)\n",
    "\n",
    "        # Add cdf\n",
    "        # cum_dist = hv.Curve((edges[1:], cdf)).opts(color='green', logx=True)\n",
    "        # hist = hist * cum_dist\n",
    "\n",
    "        # Make a reference line at 1 or 10 (10cm for distance, 10cm/s for velocity)\n",
    "        # v_line = 1 if 'cricket' in target_param else 10\n",
    "        # vline = hv.VLine(v_line).opts(color='red', line_width=1, line_dash='dashed')\n",
    "        # hist = (hist * vline)\n",
    "\n",
    "        # Addx axis labels\n",
    "        if 'speed' in target_param:\n",
    "            hist.opts(xlabel='cm/s')\n",
    "        elif 'distance' in target_param:\n",
    "            hist.opts(xlabel='cm')\n",
    "\n",
    "\n",
    "        # These parameters depend on the specific type of input\n",
    "        if 'black' in name:\n",
    "            hist.opts(fill_color='gray')\n",
    "        elif 'white' in name:\n",
    "            hist.opts(fill_color='white',\n",
    "                       line_color='black')\n",
    "\n",
    "        # if target_param == 'vrcricket_0_speed':\n",
    "        #     hist.opts(ylim=(0, 0.51), \n",
    "        #           yticks=[0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "        #           )  \n",
    "        # else:\n",
    "        hist.opts(ylim=(0, 0.21), \n",
    "                yticks=[0, 0.05, 0.1, 0.15, 0.2],\n",
    "                )    \n",
    "\n",
    "        # For publication-ready image      \n",
    "        hist.opts(\n",
    "            opts.Histogram(\n",
    "                width=fp.pix(7.8), \n",
    "                height=fp.pix(7.8), \n",
    "                toolbar=None, \n",
    "                hooks=[fp.margin], \n",
    "                fontsize=fp.font_sizes['small'], \n",
    "                xticks=[10e-2, 10e-1, 10e0, 10e1, 10e2], \n",
    "                # yticks=[10e-1, 10e0, 10e1, 10e2],\n",
    "                padding=0.01\n",
    "                )\n",
    "            )\n",
    "\n",
    "        plot_list.append(hist)\n",
    "\n",
    "\n",
    "\n",
    "param_hists = hv.Layout(plot_list).opts(shared_axes=False, toolbar=None).cols(len(target_parameters))\n",
    "save_path = os.path.join(paths.figures_path, '_'.join([save_name, 'histogram_kinematics']))\n",
    "hv.save(param_hists, save_path, fmt='png')\n",
    "param_hists"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Full aggregate analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "### Load new data\n",
    "# create container for holding multiple data sets\n",
    "data_dict = {}\n",
    "\n",
    "# Load real prey capture in the light - this is a baseline comparison\n",
    "search_string = 'result:succ, lighting:normal, rig:VR, analysis_type:aggFull'\n",
    "ds, label = load_dataset(search_string, exclusion='obstacle', label=\"VR_light_succ\")\n",
    "data_dict[label] = ds\n",
    "\n",
    "# Load VR prey capture with VR blackCr\n",
    "search_string = 'result:test, lighting:normal, rig:VPrey, analysis_type:aggFull, notes:blackCr_rewarded_crickets_0_vrcrickets_1'\n",
    "ds, label = load_dataset(search_string, label=\"VPrey_blackCr_lightBG\")\n",
    "data_dict[label] = ds\n",
    "\n",
    "# Load VR prey capture with VR blackCr\n",
    "search_string = 'result:succ, lighting:normal, rig:VPrey, analysis_type:aggFull, notes:blackCr_crickets_1'\n",
    "ds, label = load_dataset(search_string, label=\"VPrey_blackCr_lightBG_multi\")\n",
    "data_dict[label] = ds\n",
    "\n",
    "# Load VR prey capture with VR whiteCr_blackBG\n",
    "search_string = 'result:test, lighting:normal, rig:VPrey, analysis_type:aggFull, notes:whiteCr_blackBG_rewarded_crickets_0_vrcrickets_1'\n",
    "ds, label = load_dataset(search_string, label=\"VPrey_whiteCr_blackBG\")\n",
    "data_dict[label] = ds\n",
    "\n",
    "# Load VR prey capture with VR whiteCr_blackBG\n",
    "search_string = 'result:succ, lighting:normal, rig:VPrey, analysis_type:aggFull, notes:whiteCr_blackBG_crickets_1'\n",
    "ds, label = load_dataset(search_string, label=\"VPrey_whiteCr_blackBG_multi\")\n",
    "data_dict[label] = ds\n",
    "\n",
    "# Load VR  prey capture with VR blackCr_grayBG\n",
    "search_string = 'result:test, lighting:normal, rig:VPrey, analysis_type:aggFull, notes:blackCr_grayBG_rewarded_crickets_0_vrcrickets_1'\n",
    "ds, label = load_dataset(search_string, label=\"VPrey_blackCr_grayBG\")\n",
    "data_dict[label] = ds\n",
    "\n",
    "# Load VR prey capture with VR whiteCr_grayBG\n",
    "search_string = 'result:test, lighting:normal, rig:VPrey, analysis_type:aggFull, notes:whiteCr_grayBG_rewarded_crickets_0_vrcrickets_1'\n",
    "ds, label = load_dataset(search_string, label=\"VPrey_whiteCr_grayBG\")\n",
    "data_dict[label] = ds\n",
    "\n",
    "# Load VR prey capture with VR whiteCr_grayBG\n",
    "search_string = 'result:test, lighting:normal, rig:VPrey, analysis_type:aggFull, gt_date:2020-06-23T00-00-00, lt_date:2020-07-06T00-00-00, notes:rewarded_crickets_0_vrcrickets_1'\n",
    "ds, label = load_dataset(search_string, label=\"VPrey_original\")\n",
    "data_dict[label] = ds\n",
    "\n",
    "# Load VR prey capture with VR whiteCr_grayBG\n",
    "search_string = 'result:succ, lighting:normal, rig:VPrey, analysis_type:aggFull, gt_date:2020-06-23T00-00-00, lt_date:2020-07-06T00-00-00,notes:crickets_1_vrcrickets_1'\n",
    "ds, label = load_dataset(search_string, label=\"VPrey_original_multi\")\n",
    "data_dict[label] = ds\n",
    "\n",
    "# Get rid of doubled data set\n",
    "del ds"
   ],
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "source": [
    "for name in data_dict:\n",
    "\n",
    "    data = data_dict[name]\n",
    "    print(name)\n",
    "    print(data[['trial_id']].max())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "source": [
    "# Plot historgrams of trial duration\n",
    "\n",
    "# allocate a list for the plots\n",
    "plot_list = []\n",
    "means = []\n",
    "dur_sem = []\n",
    "\n",
    "for name in data_dict:\n",
    "\n",
    "    data = data_dict[name]\n",
    "\n",
    "    times = data[['time_vector', 'trial_id']].copy()\n",
    "    times = times.groupby(['trial_id']).agg(list)\n",
    "    duration = np.array([trial[-1] for trial in times['time_vector']])\n",
    "\n",
    "    means.append(duration.mean())\n",
    "    dur_sem.append((name, duration.mean(), sem(duration)))\n",
    "\n",
    "    # plot the results\n",
    "    duration_histogram = hv.Bars(duration).opts(title=name, xlabel='trial', ylabel='duration')\n",
    "    plot_list.append(duration_histogram)\n",
    "\n",
    "hv.Layout(plot_list)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "source": [
    "# Separately plot mean + sem of trial duration\n",
    "\n",
    "# Plot of means\n",
    "enc_means = hv.Bars((list(data_dict.keys()), means)).opts(title='Mean Trial Duration', ylabel='Duration (s)', ylim=(0,100), xrotation=45) \n",
    "enc_means = hv.ErrorBars(dur_sem) * enc_means\n",
    "save_path = os.path.join(paths.figures_path, '_'.join([save_name, 'duration_means']))\n",
    "hv.save(enc_means, save_path, fmt='png')\n",
    "\n",
    "# dispaly the iamge\n",
    "enc_means"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.3 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "a87906ed62866a44251634d622def361af2400f308708d93dff50a5577b9f27c"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
