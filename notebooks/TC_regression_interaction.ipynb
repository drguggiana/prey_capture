{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d17414",
   "metadata": {},
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath(r'D:\\Code Repos\\prey_capture'))\n",
    "sys.path.insert(0, os.path.abspath(r'D:\\Code Repos\\prey_capture\\mine_pub'))\n",
    "\n",
    "import panel as pn\n",
    "import holoviews as hv\n",
    "from holoviews import opts, dim\n",
    "hv.extension('bokeh')\n",
    "from bokeh.resources import INLINE\n",
    "from bokeh import palettes\n",
    "\n",
    "import paths\n",
    "import importlib\n",
    "import functions_plotting as fp\n",
    "import functions_bondjango as bd\n",
    "import functions_loaders as fl\n",
    "import processing_parameters\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import scipy.stats as stat\n",
    "import datetime\n",
    "import umap\n",
    "from pprint import pprint\n",
    "from sklearn import metrics\n",
    "import sklearn.preprocessing as preproc\n",
    "import sklearn.cluster as cluster\n",
    "from mine import MineData"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c221ff7b",
   "metadata": {},
   "source": [
    "# set up the figure config\n",
    "importlib.reload(fp)\n",
    "importlib.reload(processing_parameters)\n",
    "# define the target saving path\n",
    "save_path = os.path.join(paths.figures_path, 'TC_regression_interaction')\n",
    "\n",
    "# define the printing mode\n",
    "save_mode = True\n",
    "# define the target document\n",
    "target_document = 'paper'\n",
    "# set up the figure theme\n",
    "fp.set_theme()\n",
    "# load the label dict\n",
    "label_dict = processing_parameters.label_dictionary\n",
    "variable_list = processing_parameters.variable_list\n",
    "units_dict = processing_parameters.tc_units"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d196e59e",
   "metadata": {},
   "source": [
    "# load the desired files and their associated regressions\n",
    "\n",
    "# Load the desired files\n",
    "importlib.reload(processing_parameters)\n",
    "\n",
    "# load the constants from the regression calculation\n",
    "time_shifts = processing_parameters.time_shifts\n",
    "shift_dict = {el: idx for idx, el in enumerate(time_shifts)}\n",
    "shift_number = len(time_shifts)\n",
    "# shuffles = processing_parameters.regression_shuffles\n",
    "\n",
    "# assemble the dataframe columns\n",
    "reals = ['real_'+str(el) for el in time_shifts]\n",
    "shuffle_means = ['smean_'+str(el) for el in time_shifts]\n",
    "columns = reals + shuffle_means + ['mouse', 'day']\n",
    "\n",
    "# get the search list\n",
    "search_list = processing_parameters.search_list\n",
    "\n",
    "# allocate a list for all paths (need to preload to get the dates)\n",
    "all_paths = []\n",
    "all_results = []\n",
    "# for all the search strings\n",
    "for search_string in search_list:\n",
    "\n",
    "    # query the database for data to plot\n",
    "    data_all = bd.query_database('analyzed_data', search_string)\n",
    "    data_path = [el['analysis_path'] for el in data_all if '_combinedanalysis' in el['slug']]\n",
    "    data_result = [el['result'] for el in data_all if '_combinedanalysis' in el['slug']]\n",
    "    all_paths.append(data_path)\n",
    "    all_results.append(data_result)\n",
    "# get the dates present\n",
    "data_dates = np.unique([os.path.basename(el)[:10] for el in np.concatenate(all_paths)])\n",
    "print(f'Dates present: {data_dates}')\n",
    "\n",
    "# allocate memory for the resulting dataframe\n",
    "data = {}\n",
    "weights = {}\n",
    "# predictions = {}\n",
    "day_list = []\n",
    "animal_list = []\n",
    "joint_list = []\n",
    "# for all the list items\n",
    "for idx0, data_path in enumerate(all_paths):\n",
    "\n",
    "    # for all the files\n",
    "    for idx1, files in enumerate(data_path):\n",
    "        \n",
    "        # if a habi trial, skip\n",
    "        if 'habi' in files:\n",
    "            continue\n",
    "        \n",
    "        # get the animal and date from the slug\n",
    "        name_parts = os.path.basename(files).split('_')\n",
    "        animal = '_'.join(name_parts[7:10])\n",
    "        day_s = '_'.join(name_parts[:3])\n",
    "        day = datetime.datetime.strptime(day_s, '%m_%d_%Y')\n",
    "        # skip if the animal and day are already evaluated, \n",
    "        # since the CC is the same for the whole day\n",
    "        if animal+'_'+day_s in joint_list:\n",
    "            continue\n",
    "        else:\n",
    "            animal_list.append(animal)\n",
    "            day_list.append(day)\n",
    "            joint_list.append(animal+'_'+day_s)\n",
    "        # assemble the preproc path\n",
    "        files_preproc = files.replace('_combinedanalysis', '_preproc')\n",
    "        # open the file\n",
    "        with pd.HDFStore(files_preproc, 'r') as preprocf:\n",
    "            if '/cell_matches' in preprocf.keys():\n",
    "                # get the matches\n",
    "                cell_matches = preprocf['cell_matches']\n",
    "                \n",
    "                print(animal, day_s, files_preproc)\n",
    "                # get the idx for this file\n",
    "                current_matches = cell_matches[datetime.datetime.strftime(day, '%m_%d_%Y')].to_numpy()\n",
    "                current_idx = np.argsort(current_matches).astype(float)\n",
    "                # remove the nan entries\n",
    "                current_idx = current_idx[~np.isnan(np.sort(current_matches))]\n",
    "                # get the roi info\n",
    "                roi_info = preprocf['roi_info']\n",
    "        \n",
    "        # load the data and the cell matches (wasteful, but cleaner I think)\n",
    "        with h5py.File(files, 'r') as h:\n",
    "            \n",
    "            # for all the target variables\n",
    "            for target_variable in variable_list:\n",
    "                # create an empty list only if it's the same time this variable runs\n",
    "                if target_variable not in data.keys():\n",
    "                    data[target_variable] = []\n",
    "                    weights[target_variable] = []\n",
    "                # allocate memory for the real and shuffled regressions\n",
    "                real_array = np.zeros((shift_number))\n",
    "                shuffle_array = np.zeros((shift_number))\n",
    "                real_weight = []\n",
    "                shuffle_weight = []\n",
    "\n",
    "                if 'regression' not in h.keys():\n",
    "                    continue\n",
    "\n",
    "                # for all the keys (will iterate through shifts and reps for shuffle)\n",
    "                for key in h['/regression'].keys():\n",
    "\n",
    "                    # skip if it's not a cc key or is not the target variable\n",
    "                    if (target_variable not in key) | ('_std' in key):\n",
    "                        continue\n",
    "                    # get the time shift \n",
    "                    key_parts = key.split('_')\n",
    "                    shift = int([el[5:] for el in key_parts if 'shift' in el][0])\n",
    "                    if 'cc' in key:\n",
    "\n",
    "                        if 'real' in key_parts:\n",
    "                             # save the values\n",
    "                            real_array[shift_dict[shift]] = np.array(h['/regression/'+key])\n",
    "                        else:\n",
    "                            shuffle_array[shift_dict[shift]] = np.array(h['/regression/'+key])\n",
    "\n",
    "                    elif ('coefficients' in key and shift == 0):\n",
    "                        if 'real' in key_parts:\n",
    "                            real_weight = np.array(h['/regression/'+key])\n",
    "                        else:\n",
    "                            shuffle_weight = np.array(h['/regression/'+key])\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "                # add the columns to the main list\n",
    "                data[target_variable].append(list(real_array) + list(shuffle_array) + [animal, day])\n",
    "\n",
    "                # take only the non time shift (need to check)\n",
    "                if isinstance(real_weight, list):\n",
    "                    continue\n",
    "\n",
    "                temp_df = pd.DataFrame(np.vstack((real_weight, shuffle_weight)).T, columns=['weight', 'shuffle_weight'])\n",
    "                temp_df['match_id'] = current_idx \n",
    "                temp_df['animal'] = animal\n",
    "                temp_df['day'] = day\n",
    "                temp_df = pd.concat((temp_df, roi_info), axis=1)\n",
    "                \n",
    "                # add a cell id field\n",
    "                cell_id = np.arange(temp_df.shape[0])\n",
    "                temp_df['cell_id'] = cell_id\n",
    "\n",
    "                weights[target_variable].append(temp_df)\n",
    "\n",
    "            \n",
    "# for all the variables once more\n",
    "for target_variable in variable_list:\n",
    "    # turn the overall list into a dataframe\n",
    "    data[target_variable] = pd.DataFrame(data[target_variable], columns=columns).sort_values(['mouse', 'day'], axis=0)\n",
    "    # turn the weights into a dictionary\n",
    "    weights[target_variable] = pd.concat(weights[target_variable], axis=0)\n",
    "\n",
    "    print(f'Shape of the data dictionary: {data[target_variable].shape}')\n",
    "    print(f'Shape of the weights dataframe: {weights[target_variable].shape}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a683bb",
   "metadata": {},
   "source": [
    "%%time\n",
    "# load the regressions with a loader\n",
    "importlib.reload(fl)\n",
    "importlib.reload(processing_parameters)\n",
    "\n",
    "# get the search list\n",
    "search_list = processing_parameters.search_list\n",
    "# get the time shifts \n",
    "time_shifts = processing_parameters.time_shifts\n",
    "\n",
    "# allocate a list for all paths (need to preload to get the dates)\n",
    "all_paths = []\n",
    "# all_results = []\n",
    "# for all the search strings\n",
    "for search_string in search_list:\n",
    "\n",
    "    # query the database for data to plot\n",
    "    data_all = bd.query_database('analyzed_data', search_string)\n",
    "    data_path = [el['analysis_path'] for el in data_all if '_combinedanalysis' in el['slug']]\n",
    "#     data_result = [el['result'] for el in data_all if '_combinedanalysis' in el['slug']]\n",
    "    all_paths.append(data_path)\n",
    "#     all_results.append(data_result)\n",
    "# get the dates present\n",
    "data_dates = np.unique([os.path.basename(el)[:10] for el in np.concatenate(all_paths)])\n",
    "print(f'Dates present: {data_dates}')\n",
    "\n",
    "# only load correlations and weights, predictions are not implemented\n",
    "correlations, regression_df = fl.load_regression(all_paths, variable_list, time_shifts)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9862fa5c",
   "metadata": {},
   "source": [
    "%%time\n",
    "# load the TCs with a loader\n",
    "importlib.reload(fl)\n",
    "importlib.reload(processing_parameters)\n",
    "\n",
    "# get the search list\n",
    "search_list = processing_parameters.search_list\n",
    "\n",
    "# allocate a list for all paths (need to preload to get the dates)\n",
    "all_paths = []\n",
    "# all_results = []\n",
    "# for all the search strings\n",
    "for search_string in search_list:\n",
    "\n",
    "    # query the database for data to plot\n",
    "    data_all = bd.query_database('analyzed_data', search_string)\n",
    "    data_path = [el['analysis_path'] for el in data_all if '_tcday' in el['slug']]\n",
    "#     data_result = [el['result'] for el in data_all if '_combinedanalysis' in el['slug']]\n",
    "    all_paths.append(data_path)\n",
    "#     all_results.append(data_result)\n",
    "# get the dates present\n",
    "data_dates = np.unique([os.path.basename(el)[:10] for el in np.concatenate(all_paths)])\n",
    "print(f'Dates present: {data_dates}')\n",
    "\n",
    "# load the tc data\n",
    "tc_whole = fl.load_tc(all_paths, variable_list)\n",
    "print(f'Shape of the tuning curve dataframe: {tc_whole.shape}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7275c2c",
   "metadata": {},
   "source": [
    "# Assemble the main regression dataframe\n",
    "\n",
    "# put all the cells in a single dataframe with variables vs cells\n",
    "\n",
    "# need to iterate through the variables, and concatenate the weights in order per day\n",
    "\n",
    "# allocate memory for the output dataframe\n",
    "regression_df = []\n",
    "\n",
    "# for all the variables\n",
    "for idx, target_feature in enumerate(weights.keys()):\n",
    "    # get the data\n",
    "    current_feature = weights[target_feature]\n",
    "\n",
    "    if idx == 0:\n",
    "\n",
    "        df = current_feature.loc[:, ['animal', 'day', 'match_id', 'cell_id', 'weight', 'centroid_x', 'centroid_y',\n",
    "                                               'bbox_left', 'bbox_top', 'bbox_width', 'bbox_height', 'area']]\n",
    "        df = df.rename(columns={'weight': target_feature})\n",
    "\n",
    "    else:\n",
    "        df = current_feature.loc[:, 'weight']\n",
    "        df = df.rename(target_feature)\n",
    "\n",
    "    regression_df.append(df)\n",
    "# concatenate into a single dataframe\n",
    "regression_df = pd.concat(regression_df, axis=1)\n",
    "\n",
    "\n",
    "# eliminate rows with nans\n",
    "regression_df = regression_df.iloc[~np.any(np.isnan(regression_df.drop(['animal', 'day'], axis=1).to_numpy()), axis=1), :]\n",
    "print(regression_df.columns)\n",
    "print(regression_df.shape)\n",
    "    "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985b9471",
   "metadata": {},
   "source": [
    "# Load the tc_consolidate file\n",
    "importlib.reload(processing_parameters)\n",
    "\n",
    "# get the search query\n",
    "search_consolidate = processing_parameters.search_consolidate\n",
    "\n",
    "# query the database for data to plot\n",
    "data_path = bd.query_database('analyzed_data', search_consolidate)\n",
    "# data_path = [el['analysis_path'] for el in data_path if 'test' not in el['analysis_path']][0]\n",
    "data_path = [el['analysis_path'] for el in data_path if 'test' not in el['analysis_path']]\n",
    "pprint(data_path)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da6d464",
   "metadata": {},
   "source": [
    "# Get the relevant fields from the TC data\n",
    "\n",
    "tc_whole = []\n",
    "target_features = variable_list\n",
    "index_columns = ['Resp_index', 'Cons_index', 'Qual_index', 'Resp_test', 'Cons_test', 'Qual_test']\n",
    "# for all the targets\n",
    "for idx, target_feature in enumerate(target_features):\n",
    "    # load the data\n",
    "    data = []\n",
    "#     meta_data = []\n",
    "    for file in data_path:\n",
    "        \n",
    "        try:\n",
    "            data.append(pd.read_hdf(file, target_feature))\n",
    "            meta_data = pd.read_hdf(file, 'meta_data')\n",
    "            \n",
    "            # filter out habi trials\n",
    "            for idx1, el in enumerate(meta_data.to_numpy()):\n",
    "\n",
    "                if ('habi' in el):\n",
    "                    # get a vector with the rows to keep\n",
    "                    keep_vector = ~(data[-1].loc[:, 'id'] == meta_data.loc[idx1, 'id']).to_numpy()\n",
    "                    data[-1] = data[-1].iloc[keep_vector, :]\n",
    "\n",
    "        except KeyError:\n",
    "            continue\n",
    "    data = pd.concat(data, axis=0)\n",
    "    \n",
    "    # load the relevant columns\n",
    "    if idx == 0:\n",
    "        target_columns = ['day', 'animal', 'cell_id'] + index_columns + [el for el in data.columns if ('bin' in el) & ('half' not in el)]\n",
    "    else:\n",
    "        target_columns = index_columns + [el for el in data.columns if ('bin' in el) & ('half' not in el)]\n",
    "    \n",
    "    data = data.loc[:, target_columns]\n",
    "    \n",
    "#     if idx == 0:\n",
    "#         # convert the day from string to date format\n",
    "#         data.loc[:, 'day'] = np.array([datetime.datetime.strptime(el, '%m_%d_%Y') for el in data.loc[:, 'day']])\n",
    "    \n",
    "    # change the column names\n",
    "    new_names = {el: target_feature+'_'+el if ('bin' in el) | ('index' in el) | ('test' in el) else el for el in target_columns}\n",
    "    data = data.rename(columns=new_names)\n",
    "    # save in the list\n",
    "    tc_whole.append(data)\n",
    "\n",
    "# concatenate    \n",
    "tc_whole = pd.concat(tc_whole, axis=1).reset_index(drop=True)\n",
    "# print(tc_whole['day'])\n",
    "# reformat the date\n",
    "# tc_whole.loc[:, 'day'] = np.array([datetime.datetime.strptime(el, '%m_%d_%Y') for el in tc_whole.loc[:, 'day']])\n",
    "\n",
    "print(tc_whole.shape)\n",
    "# exclude all rows with nans\n",
    "cleanup_columns = [el for el in tc_whole.columns if 'Resp' in el]\n",
    "cleanup_data = tc_whole.loc[:, cleanup_columns].to_numpy()\n",
    "cleanup_data[np.isnan(cleanup_data)] = 0\n",
    "cleanup_data[np.isinf(cleanup_data)] = 0\n",
    "tc_whole.loc[:, cleanup_columns] = cleanup_data\n",
    "\n",
    "print(tc_whole.shape)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b561ae1",
   "metadata": {},
   "source": [
    "# sort the TCs to match the regressions\n",
    "print(tc_whole.columns)\n",
    "\n",
    "tc_whole = tc_whole.sort_values(['animal', 'day', 'cell_id'], axis=0).reset_index(drop=True)\n",
    "# trim the regression(only for this)\n",
    "regression_df = regression_df.iloc[((regression_df['rvs']=='real')&(regression_df['regressor']=='linear')).values, :]\n",
    "regression_df = regression_df.sort_values(['animal', 'day', 'cell_id'], axis=0).reset_index(drop=True)\n",
    "\n",
    "# # add the match_id to the TC\n",
    "# tc_whole['match_id'] = regression_df['match_id']\n",
    "\n",
    "# save copies of the original for subselection\n",
    "tc_whole_ori = tc_whole.copy()\n",
    "regression_df_ori = regression_df.copy()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38820b89",
   "metadata": {},
   "source": [
    "# load the mine data\n",
    "# define the base directory\n",
    "base_path = r'Z:\\test_mine'\n",
    "# define the list of fields\n",
    "field_list = ['correlations_trained', 'correlations_test', 'taylor_scores', 'taylor_true_change', 'taylor_full_prediction', 'taylor_by_predictor', 'nl_probs', 'me_scores', 'jacobians']\n",
    "\n",
    "# get the files in the directory\n",
    "files = os.listdir(base_path)\n",
    "# allocate memory for the mine data\n",
    "mine_list = []\n",
    "mouse_date_list = []\n",
    "cell_ids = []\n",
    "# for all the files\n",
    "for file in files[1:]:\n",
    "    \n",
    "    # get the mouse and day\n",
    "    name = file.split('_')\n",
    "    mouse = '_'.join(name[0:3])\n",
    "    day = name[3]\n",
    "    filepath = os.path.join(base_path, file)\n",
    "    # load the file\n",
    "    with h5py.File(filepath, mode='r') as f:    \n",
    "        current_mine = [np.array(f[el]) for el in field_list]\n",
    "        temp_ids = np.array(f['cell_ids'])\n",
    "        temp_ids = [str(el).replace('[', '').replace(']', '').replace(\"'\", '').split(' ') for el in temp_ids]\n",
    "        # very ugly fix to get rid of the b\n",
    "        temp_list = []\n",
    "        for el in temp_ids:\n",
    "            temp = [el2[1:] for el2 in el]\n",
    "            temp_list.append(temp)\n",
    "        temp_ids = temp_list\n",
    "        cell_ids.append(temp_ids)\n",
    "    # create the mine object\n",
    "    current_mine = MineData(*current_mine)\n",
    "    \n",
    "    # store on a list\n",
    "    mine_list.append(current_mine)\n",
    "    mouse_date_list.append((mouse, day))\n",
    "\n",
    "# load a single file to get the predictor columns variable (crappy crappy hacky code)\n",
    "predictor_columns = variable_list\n",
    "# # get the paths from the database using search_list\n",
    "# all_paths, all_queries = fl.query_search_list()\n",
    "# # print(all_paths)\n",
    "\n",
    "# # data_list = []\n",
    "# # load the data\n",
    "# for path, queries in zip(all_paths[:1], all_queries[:1]):\n",
    "    \n",
    "#     data, _, _  = fl.load_preprocessing(path[:1], queries[:1])\n",
    "# #     data_list.append(data)\n",
    "    \n",
    "# # print(data)\n",
    "# predictor_columns = data[0].columns\n",
    "# predictor_columns = [el for el in predictor_columns if (el not in ['motifs', 'mouse', 'datetime']) & ('cell_' not in el)]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a8e8a1",
   "metadata": {},
   "source": [
    "# create a dataframe with the relevant parts of the mine data\n",
    "# get the number of predictors from MINE\n",
    "predictor_number = len(predictor_columns)\n",
    "print(predictor_number)\n",
    "scores_number = ((predictor_number**2) - predictor_number)/2 +predictor_number\n",
    "\n",
    "# allocate for the current trial\n",
    "mine_cells = []\n",
    "# for all date/mice\n",
    "for current_mine, (mouse, date), ids in zip(mine_list, mouse_date_list, cell_ids):\n",
    "    \n",
    "    # for all the cells\n",
    "    for cell, _ in enumerate(current_mine.correlations_trained):\n",
    "#         print(current_mine.taylor_scores[cell, :, 0].shape)\n",
    "#         print(mouse, date, ids[cell])\n",
    "        taylor_scores = current_mine.taylor_scores[cell, :, 0].flatten()\n",
    "        if taylor_scores.shape[0] < scores_number:\n",
    "            taylor_scores = np.concatenate([taylor_scores, np.zeros((int(scores_number - taylor_scores.shape[0])))*np.nan], axis=0)\n",
    "#         raise ValueError\n",
    "        temp_list = [current_mine.correlations_test[cell], *taylor_scores, current_mine.nl_probabilities[cell], current_mine.mean_exp_scores[cell], mouse, date, ids[cell][0]]\n",
    "#         print(temp_list)\n",
    "#         raise ValueError\n",
    "#         print(len(temp_list))\n",
    "        mine_cells.append(np.array(temp_list))\n",
    "\n",
    "# generate the column names\n",
    "interaction_names = []\n",
    "idx = 1\n",
    "for name1 in predictor_columns:\n",
    "    for name2 in predictor_columns[idx:]:\n",
    "#         if name1 == name2:\n",
    "#             continue\n",
    "        interaction_names.append(f'{name1}_{name2}')\n",
    "    idx += 1\n",
    "taylor_columns = predictor_columns + interaction_names\n",
    "columns = ['correlation_test', *taylor_columns, 'nl_probability', 'mean_exp_score', 'animal', 'day', 'cell_ids']\n",
    "# turn the output into a dataframe\n",
    "mine_cells = pd.DataFrame(mine_cells, columns=columns)\n",
    "date_temp = []\n",
    "for el in mine_cells.loc[:, 'day']:\n",
    "    name = el.split('-')\n",
    "    new_date = '_'.join([name[1], name[2], name[0]])\n",
    "    date_temp.append(new_date)\n",
    "mine_cells.loc[:, 'day'] = date_temp"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daeccff1",
   "metadata": {},
   "source": [
    "print(len(mouse_date_list))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928bd72f",
   "metadata": {},
   "source": [
    "# tc_whole = tc_whole_ori\n",
    "# regression_df = regression_df_ori"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9755e16",
   "metadata": {},
   "source": [
    "# print(mine_cells['mouse_speed'])\n",
    "\n",
    "cells = mine_cells['mouse_speed'].to_numpy().astype(float)\n",
    "cells = cells[~np.isnan(cells)]\n",
    "cells = np.log10(np.abs(cells))\n",
    "# cells = cells[cells>-5]\n",
    "print(cells.shape)\n",
    "freq, location = np.histogram(cells, bins=100)\n",
    "\n",
    "plot = hv.Scatter((location, freq))\n",
    "plot.opts(width=600, height=600, tools=['hover'])\n",
    "plot"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759f3389",
   "metadata": {},
   "source": [
    "# match cells across TCs, regression and mine\n",
    "\n",
    "# group by the animal and day \n",
    "for idx, ((mouse, day), data) in enumerate(mine_cells.groupby(['animal', 'day'])):\n",
    "    "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5b21c0",
   "metadata": {},
   "source": [
    "# select only the cells that are present in regression, mine and tc\n",
    "\n",
    "# allocate a list for the reordered MINE data\n",
    "mine_accumulator = []\n",
    "tc_accumulator = []\n",
    "reg_accumulator = []\n",
    "# and also an empty selection vector\n",
    "selection_vector = np.zeros((tc_whole.shape[0]))\n",
    "# cycle through the days and mice\n",
    "for idx, ((mouse, day), data) in enumerate(mine_cells.groupby(['animal', 'day'])):\n",
    "    # get the current mouse and day in the tuning curves\n",
    "    temp_selection = ((tc_whole['animal'].values == mouse) & (tc_whole['day'].values == day))\n",
    "    print(temp_selection)\n",
    "    raise ValueError\n",
    "#     # if there are cells in these conditions\n",
    "#     if temp_selection.sum() > 0:\n",
    "#         # check if the selection vector is empty\n",
    "#         if selection_vector.sum() == 0:\n",
    "#             # if yes, make the current vector the selection vector\n",
    "#             selection_vector = temp_selection\n",
    "#         else:\n",
    "#             # otherwise, OR it with the existing one\n",
    "#             selection_vector = selection_vector | temp_selection\n",
    "    tc_accumulator.append(tc_whole.iloc[temp_selection, :])\n",
    "    reg_accumulator.append(regression_df.iloc[temp_selection, :])\n",
    "    print(data.columns)\n",
    "    # also save the target mine fields in a preallocated list\n",
    "    mine_accumulator.append(data[variable_list + ['animal', 'day', 'cell_ids', 'nl_probability', 'mean_exp_score', 'correlation_test']])        \n",
    "    \n",
    "#     print(mouse, day, data)\n",
    "#     break\n",
    "        \n",
    "# tc_whole = tc_whole.iloc[selection_vector, :]\n",
    "# regression_df = regression_df.iloc[selection_vector, :]\n",
    "tc_whole = pd.concat(tc_accumulator)\n",
    "regression_df = pd.concat(reg_accumulator)\n",
    "mine_cells = pd.concat(mine_accumulator)\n",
    "print(f'TC shape: {tc_whole.shape}, Reg shape: {regression_df.shape}, MINE shape: {mine_cells.shape}')\n",
    "    \n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e868abf",
   "metadata": {},
   "source": [
    "print(mine_cells.columns)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b25bf44",
   "metadata": {},
   "source": [
    "print(tc_whole.loc[:, ['cell_id', 'animal', 'day']], mine_cells.loc[:, ['cell_ids', 'animal', 'day']])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f02b991",
   "metadata": {},
   "source": [
    "# Prepare the array of indexes for comparative calculations\n",
    "\n",
    "# get the tuning indexes\n",
    "tc_index_columns = [el for el in tc_whole.columns if '_Qual_index' in el]\n",
    "tc_index = tc_whole[tc_index_columns].copy()\n",
    "\n",
    "tc_test_columns = [el for el in tc_whole.columns if '_Qual_test' in el]\n",
    "tc_test = tc_whole[tc_test_columns].copy()\n",
    "\n",
    "tc_index = tc_index * tc_test.values\n",
    "\n",
    "# tc_index = mine_cells[variable_list]\n",
    "regression_index = mine_cells[variable_list]\n",
    "# print((tc_test.values!=0).sum())\n",
    "# get the regression importances\n",
    "# regression_index = regression_df.loc[:, variable_list].iloc[((regression_df['rvs']=='real')&(regression_df['regressor']=='linear')).values, :]\n",
    "\n",
    "print(f'TC shape: {tc_index.shape}, Reg shape: {regression_index.shape}')\n",
    "# combine them\n",
    "all_indexes = pd.concat([tc_index, regression_index], axis=1)\n",
    "all_columns = all_indexes.columns\n",
    "all_indexes = all_indexes.to_numpy().astype(float)\n",
    "# normalize\n",
    "# all_indexes = preproc.StandardScaler().fit_transform(all_indexes)\n",
    "all_indexes[np.isnan(all_indexes)] = 0"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7929862d",
   "metadata": {},
   "source": [
    "%%time\n",
    "# embed\n",
    "reducer = umap.UMAP(min_dist=0.5, n_neighbors=30)\n",
    "embedded_data = reducer.fit_transform(all_indexes[:, :11])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e664b9d",
   "metadata": {},
   "source": [
    "# calculate labels\n",
    "\n",
    "# define the target variable\n",
    "target_parameter = 'mouse_speed'\n",
    "perc = 99\n",
    "\n",
    "column_idx = [idx for idx, el in enumerate(all_columns) if target_parameter in el]\n",
    "\n",
    "idx0 = all_indexes[:, column_idx[0]].copy()\n",
    "idx1 = all_indexes[:, column_idx[1]].copy()\n",
    "\n",
    "# idx0 = np.abs(idx0)\n",
    "# idx1 = np.abs(idx1)\n",
    "\n",
    "# idx0 = (idx0 - idx0.min())/(idx0.max() - idx0.min())\n",
    "# idx1 = (idx1 - idx1.min())/(idx1.max() - idx1.min())\n",
    "\n",
    "# raw_labels = (idx0-idx1)/(idx0+idx1)\n",
    "# label = np.abs(idx1)\n",
    "# raw_labels = idx1\n",
    "# raw_labels = np.abs(idx1)\n",
    "\n",
    "# label = np.log(np.abs(idx1))\n",
    "raw_labels = [idx0.copy(), idx1.copy()]\n",
    "umap_list = []\n",
    "for label in raw_labels:\n",
    "    \n",
    "    label[label>np.percentile(label, perc)] = np.percentile(label, perc)\n",
    "    label[label<np.percentile(label, 100-perc)] = np.percentile(label, 100-perc)\n",
    "\n",
    "    plot_data = np.concatenate([embedded_data, label.reshape((-1, 1))], axis=1)\n",
    "\n",
    "    # plot the trials\n",
    "    umap_plot = hv.Scatter(plot_data, vdims=['Dim 2', 'Parameter'], kdims=['Dim 1'])\n",
    "    # umap_plot = hv.HexTiles(umap_data, kdims=['Dim 1', 'Dim 2'])\n",
    "    umap_plot.opts(colorbar=True, color='Parameter', cmap='Spectral', tools=['hover'], alpha=1)\n",
    "    umap_plot.opts(width=600, height=700, size=5)\n",
    "    umap_list.append(umap_plot)\n",
    "hv.Layout(umap_list).cols(2).opts(shared_axes=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3c2e5d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# ticks = [(idx+0.5, el) for idx, el in enumerate(variable_list)]\n",
    "# plot = hv.Raster(all_indexes[:, :11].T)\n",
    "# plot.opts(width=1200, height=600, colorbar=True, cmap='RdBu', tools=['hover'], yticks=ticks, xrotation=0, xlabel='')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f635c17",
   "metadata": {},
   "source": [
    "x = idx0.copy()\n",
    "y = idx1.copy()\n",
    "\n",
    "# x = np.abs(x)\n",
    "# y = np.abs(y)\n",
    "selection_vector = (x!=0)&(y!=0)\n",
    "x = x[selection_vector]\n",
    "y = y[selection_vector]\n",
    "\n",
    "plot = hv.Scatter((x, y)).opts(xlabel='Tuning index', ylabel='Taylor metric', title=f'{stat.spearmanr(x, y)[0]:0.3f}')\n",
    "plot.opts(width=600, height=600, tools=['hover'], ylim=(-1, 1.5))#, xlim=(0, 0.7))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2daa4f",
   "metadata": {},
   "source": [
    "freq, loc = np.histogram(raw_labels, bins=30)\n",
    "\n",
    "hv.Bars((loc, freq)).opts(width=600, xrotation=45)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083142ba",
   "metadata": {},
   "source": [
    "# explore the roi info\n",
    "\n",
    "# # plot are distributions\n",
    "# freq, edges = np.histogram(np.log10(regression_df['area'].to_numpy()), bins=40)\n",
    "# areas = hv.Histogram((edges, freq), kdims='ROI area (log(px))')\n",
    "# areas.opts(xrotation=45, width=400)\n",
    "\n",
    "# # calculate the distance between matches\n",
    "# # annotate a list to accumulate distances\n",
    "# distance_list = []\n",
    "# # run through all the mice\n",
    "# for mouse_name, mouse_data in regression_df.groupby(['animal']):\n",
    "#     for cell_name, cell_data in mouse_data.groupby(['match_id']):\n",
    "#         # skip the singles\n",
    "#         if cell_data.shape[0] == 1:\n",
    "#             continue\n",
    "#         # get the features\n",
    "#         features = cell_data[['centroid_x', 'centroid_y']]\n",
    "\n",
    "#         # get the distance matrix\n",
    "#         distance_matrix = metrics.pairwise_distances(features.to_numpy())\n",
    "# #         distance_list.append(np.diagonal(distance_matrix, offset=1))\n",
    "#         distance_list.append(np.mean(distance_matrix, axis=0))\n",
    "# # concatenate\n",
    "# distance_list = np.hstack(distance_list)\n",
    "# # calculate and plot the histogram\n",
    "# freq_dist, edges_dist = np.histogram(distance_list, bins=40)\n",
    "# distances = hv.Histogram((edges_dist, freq_dist), kdims='Cortical distance (px)')\n",
    "# distances.opts(xrotation=45, width=400)\n",
    "\n",
    "# (areas+distances).opts(shared_axes=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08599225",
   "metadata": {},
   "source": [
    "# plot the roi centroid for matching pairs\n",
    "\n",
    "# # initialize the plot list\n",
    "# plot_list = []\n",
    "# # define the number of matches to take per mouse\n",
    "# target_matches = 10\n",
    "# # generate a palette accordingly\n",
    "# colors = palettes.brewer['Spectral'][target_matches]\n",
    "\n",
    "# # run through all the mice\n",
    "# for mouse_name, mouse_data in regression_df.groupby(['animal']):\n",
    "    \n",
    "#     # get the number of matched cells per id\n",
    "#     match_numbers = mouse_data.groupby(['match_id'], as_index=False)['match_id'].count()\n",
    "\n",
    "#     # exclude the non-matched\n",
    "#     match_numbers = match_numbers.iloc[match_numbers['match_id'].to_numpy()>1, :]\n",
    "\n",
    "#     # if there aren't enough matches, just use all\n",
    "#     target_ids = np.array(match_numbers.index)\n",
    "#     if match_numbers.shape[0] > target_matches:\n",
    "#         target_ids = np.random.choice(target_ids, size=target_matches)\n",
    "#     # initialize a list for this roi's plot\n",
    "#     cell_plot = []\n",
    "#     # for all the ids    \n",
    "#     for idx, ids in enumerate(target_ids):\n",
    "\n",
    "#         # get the data and plot\n",
    "#         current_idx = mouse_data['match_id'].to_numpy() == ids\n",
    "# #         current_data = mouse_data.loc[current_idx, ['centroid_x', 'centroid_y']]\n",
    "# #         plot = hv.Scatter(current_data)\n",
    "# #         cell_plot.append(plot)\n",
    "#         current_data = mouse_data.loc[current_idx, ['bbox_left', 'bbox_top', 'bbox_width', 'bbox_height']]\n",
    "#         # for all the cells\n",
    "#         for index, cells in current_data.iterrows():\n",
    "#             # assemble the bounding box for plotting\n",
    "#             bbox = np.array([[cells['bbox_left'], cells['bbox_top']], [cells['bbox_left']+cells['bbox_width'], cells['bbox_top']],\n",
    "#                             [cells['bbox_left']+cells['bbox_width'], cells['bbox_top']+cells['bbox_height']], [cells['bbox_left'], cells['bbox_top']+cells['bbox_height']], \n",
    "#                             [cells['bbox_left'], cells['bbox_top']]])\n",
    "#             plot = hv.Curve(bbox)\n",
    "#             plot.opts(color=colors[idx], width=400, height=400, title=mouse_name)\n",
    "#             cell_plot.append(plot)\n",
    "#     # overlay and store\n",
    "#     plot_list.append(hv.Overlay(cell_plot))\n",
    "# # plot\n",
    "# hv.Layout(plot_list).cols(3)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61639ff4",
   "metadata": {},
   "source": [
    "# sub-select cells\n",
    "\n",
    "# # define a target feature to sort by\n",
    "# target_feature = 'mouse_x'\n",
    "# # define the percentage of cells to select\n",
    "# target_percentage = 100\n",
    "\n",
    "# # exclude cells that don't pass an area criterion\n",
    "\n",
    "# # define the area criterion (based on the area histogram across the dataset)\n",
    "# # area_min = 30\n",
    "# # area_max = 300\n",
    "# distance = 10\n",
    "\n",
    "# # keep_vector = (regression_df_ori['area'].to_numpy() > area_min) & (regression_df_ori['area'].to_numpy() < area_max)\n",
    "\n",
    "# # get the distances for the distance criterion\n",
    "# # annotate a list to accumulate distances\n",
    "# distance_list = []\n",
    "# # run through all the mice\n",
    "# for mouse_name, mouse_data in regression_df_ori.groupby(['animal']):\n",
    "#     for cell_name, cell_data in mouse_data.groupby(['match_id']):\n",
    "#         # skip the singles\n",
    "#         if cell_data.shape[0] == 1:\n",
    "#             average_distance = np.nan\n",
    "#         else:\n",
    "#             # get the features\n",
    "#             features = cell_data[['centroid_x', 'centroid_y']]\n",
    "\n",
    "#             # get the distance matrix\n",
    "#             distance_matrix = metrics.pairwise_distances(features.to_numpy())\n",
    "#     #         distance_list.append(np.diagonal(distance_matrix, offset=1))\n",
    "#             average_distance = np.mean(distance_matrix, axis=0)\n",
    "#         # assemble a dataframe\n",
    "#         distance_df = cell_data[['animal', 'day', 'cell_id']].copy()\n",
    "#         distance_df.loc[:, 'distance'] = average_distance\n",
    "        \n",
    "#         distance_list.append(distance_df)\n",
    "# # concatenate\n",
    "# distance_list = pd.concat(distance_list, axis=0)\n",
    "# distance_list = distance_list.sort_values(['animal', 'day', 'cell_id'], axis=0).reset_index(drop=True)\n",
    "\n",
    "# # threshold\n",
    "# keep_vector = (keep_vector) & (distance_list['distance'].to_numpy() < 10)\n",
    "# # replace in the data\n",
    "# regression_df = regression_df_ori.iloc[keep_vector, :]\n",
    "# tc_whole = tc_whole_ori.iloc[keep_vector, :]\n",
    "\n",
    "# # leave only cells with matches\n",
    "\n",
    "# # allocate the list for accumulation\n",
    "# accumulation_list = []\n",
    "# # also allocate a list for the cell coordinates to save\n",
    "# idx_list = []\n",
    "# # run through all the mice\n",
    "# for mouse_name, mouse_data in regression_df.groupby(['animal']):\n",
    "    \n",
    "#     for cell_name, cell_data in mouse_data.groupby(['match_id']):\n",
    "        \n",
    "#         # skip the singles\n",
    "#         if cell_data.shape[0] == 1:\n",
    "#             continue\n",
    "#         # accumulate the cells\n",
    "#         accumulation_list.append(cell_data)\n",
    "#         # accumulate the idx\n",
    "#         idx_list.append(cell_data.index.to_numpy())\n",
    "# # concatenate them\n",
    "# regression_df = pd.concat(accumulation_list)\n",
    "# # also sub-select from the TCs\n",
    "# tc_whole = tc_whole.loc[np.hstack(idx_list), :]\n",
    "\n",
    "# # sort by a target feature\n",
    "\n",
    "# # get the actual target number\n",
    "# target_number = int(np.round(tc_whole.shape[0]*target_percentage/100))\n",
    "\n",
    "# # get the feature sorting\n",
    "# feature = tc_whole.loc[:, target_feature+'_Cons_index'].to_numpy()\n",
    "# # feature = regression_df_ori.loc[:, target_feature].to_numpy()\n",
    "# feature_idx = np.argsort(feature)\n",
    "# sorted_feature = feature[feature_idx]\n",
    "# # exclude the nans\n",
    "# feature_idx = feature_idx[~np.isnan(sorted_feature)]\n",
    "# feature_idx = feature_idx[-target_number:]\n",
    "\n",
    "# # get the cells from both \n",
    "# tc_whole = tc_whole.iloc[feature_idx, :]\n",
    "# regression_df = regression_df.iloc[feature_idx, :]\n",
    "# # resort\n",
    "# tc_whole = tc_whole.sort_values(['animal', 'day', 'cell_id'], axis=0).reset_index(drop=True)\n",
    "# regression_df = regression_df.sort_values(['animal', 'day', 'cell_id'], axis=0).reset_index(drop=True)\n",
    "# print(feature[feature_idx])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f68c8e7",
   "metadata": {},
   "source": [
    "# calculate the correlation matrix\n",
    "# # get the columns for both methods\n",
    "# resp_columns = [el for el in tc_whole.columns if 'Resp_index' in el]\n",
    "# cons_columns = [el for el in tc_whole.columns if 'Cons_index' in el]\n",
    "# qual_columns = [el for el in tc_whole.columns if 'Qual_index' in el]\n",
    "# # get the TCs\n",
    "# tc_resp = tc_whole.loc[:, resp_columns].to_numpy()\n",
    "# tc_cons = tc_whole.loc[:, cons_columns].to_numpy()\n",
    "# tc_qual = tc_whole.loc[:, qual_columns].to_numpy()\n",
    "# tc_indexes = tc_qual\n",
    "\n",
    "# allocate memory for the matrices\n",
    "matrix_list = []\n",
    "\n",
    "# for all the mice\n",
    "for mouse_name, mouse_data in tc_whole.groupby(['animal']):\n",
    "    # get the relevant columns from the TCs\n",
    "    target_columns = [el for el in tc_whole.columns if 'Qual_index' in el]\n",
    "    tc_matrix = mouse_data[target_columns].to_numpy()\n",
    "    test_columns = [el for el in tc_whole.columns if 'Qual_test' in el]\n",
    "    tc_test = mouse_data[test_columns].to_numpy()\n",
    "    tc_matrix = tc_matrix * tc_test\n",
    "# for mouse_name, mouse_data in mine_cells.groupby(['animal']):\n",
    "#     tc_matrix = mouse_data.loc[:, variable_list].to_numpy()\n",
    "    \n",
    "    # get the regressions\n",
    "#     regression_matrix = mine_cells.iloc[mine_cells.loc[:, 'animal'].to_numpy() == mouse_name, :]\n",
    "    regression_matrix = regression_df.iloc[regression_df.loc[:, 'animal'].to_numpy() == mouse_name, :]\n",
    "    regression_matrix = regression_matrix.loc[:, variable_list].to_numpy()\n",
    "    \n",
    "    print(tc_matrix.shape, regression_matrix.shape)\n",
    "\n",
    "# take the abs\n",
    "# tc_indexes = np.abs(tc_indexes)\n",
    "# regression_matrix = np.abs(regression_matrix)\n",
    "\n",
    "    # calculate correlation\n",
    "    correlation_matrix, pvalue_matrix = stat.spearmanr(tc_matrix, regression_matrix, nan_policy='omit')\n",
    "    \n",
    "    matrix_list.append(correlation_matrix)\n",
    "\n",
    "matrix_list = np.array(matrix_list)\n",
    "\n",
    "correlation_average = np.mean(matrix_list, axis=0)[len(variable_list):, :len(variable_list)]\n",
    "correlation_error = stat.sem(matrix_list, axis=0)[len(variable_list):, :len(variable_list)]\n",
    "# print(tc_indexes.shape)\n",
    "# print(regression_matrix.shape)\n",
    "print(matrix_list.shape)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "df27706f",
   "metadata": {},
   "source": [
    "# Interaction correlation plot  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccd9e0d",
   "metadata": {},
   "source": [
    "# plot the correlation matrix\n",
    "# plot_list = []\n",
    "\n",
    "# ticks = [(idx+0.5, el[:-11]) for idx, el in enumerate(resp_columns+resp_columns)]\n",
    "# raster = hv.Raster(correlation_matrix, kdims=['Regression', 'TCs'])\n",
    "# raster.opts(width=1000, height=800, yticks=ticks, xticks=ticks, xrotation=45, colorbar=True, cmap='RdBu', clim=(-1, 1), tools=['hover'])\n",
    "# plot_list.append(raster)\n",
    "\n",
    "# ticks = [(idx+0.5, el[:-11]) for idx, el in enumerate(resp_columns)]\n",
    "# raster = hv.Raster(correlation_matrix[len(resp_columns):, :len(resp_columns)], kdims=['Regression', 'TCs'])\n",
    "# raster.opts(width=1000, height=800, yticks=ticks, xticks=ticks, xrotation=45, colorbar=True, cmap='RdBu', clim=(-1, 1), tools=['hover'])\n",
    "# plot_list.append(raster)\n",
    "\n",
    "ticks = [(idx+0.5, label_dict[el]) for idx, el in enumerate(variable_list)]\n",
    "# ticks = [(idx+0.5, idx) for idx, el in enumerate(variable_list)]\n",
    "plot_matrix = correlation_average.copy()\n",
    "# plot_matrix = np.tril(plot_matrix, k=0)\n",
    "plot_matrix[plot_matrix==0] = np.nan\n",
    "# hv.Raster(correlation_matrix)\n",
    "raster = hv.Raster(plot_matrix)\n",
    "# format the plot\n",
    "raster.opts(width=950, height=800, yticks=ticks, xticks=ticks, colorbar=True, cmap='RdBu', clim=(-1, 1), xrotation=45, \n",
    "            xlabel='Importance', ylabel='Tuning')\n",
    "\n",
    "# assemble the file name\n",
    "save_name = os.path.join(save_path, '_'.join((target_document, 'Interaction_correlation')) + '.png')\n",
    "# save the figure\n",
    "fig = fp.save_figure(raster, save_name, fig_width=15, dpi=1200, fontsize=target_document, target='screen')\n",
    "\n",
    "# ticks = [(idx+0.5, el[:-11]) for idx, el in enumerate(resp_columns)]\n",
    "# raster = hv.Raster(correlation_matrix[:len(resp_columns), :len(resp_columns)], kdims=['TCs', 'TCs'])\n",
    "# raster.opts(width=1000, height=800, yticks=ticks, xticks=ticks, xrotation=45, colorbar=True, cmap='RdBu', clim=(-1, 1), tools=['hover'])\n",
    "# plot_list.append(raster)\n",
    "\n",
    "# ticks = [(idx+0.5, el[:-11]) for idx, el in enumerate(resp_columns)]\n",
    "# raster = hv.Raster(correlation_matrix[len(resp_columns):, len(resp_columns):], kdims=['Regression', 'Regression'])\n",
    "# raster.opts(width=1000, height=800, yticks=ticks, xticks=ticks, xrotation=45, colorbar=True, cmap='RdBu', clim=(-1, 1), tools=['hover'])\n",
    "# plot_list.append(raster)\n",
    "\n",
    "# hv.Layout(plot_list).cols(1).opts(shared_axes=False)\n",
    "# print(ticks)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "3477f3c0",
   "metadata": {},
   "source": [
    "# Interaction diagonal plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba588c4",
   "metadata": {},
   "source": [
    "# plot the diagonal of the interaction matrix across animals\n",
    "importlib.reload(fp)\n",
    "# get the diagonals\n",
    "diagonal_average = np.diagonal(correlation_average)\n",
    "diagonal_error = np.diagonal(correlation_error)\n",
    "# get the x coordinate\n",
    "x = [label_dict[el] for el in variable_list]\n",
    "\n",
    "# plot\n",
    "dots = hv.Scatter((x, diagonal_average))\n",
    "dots.opts(size=10, ylabel='Tuning-to-Importance CC', xlabel='')\n",
    "error = hv.ErrorBars((x, diagonal_average, diagonal_error))\n",
    "error.opts(width=600, height=600, xrotation=45, line_width=1)\n",
    "\n",
    "plot = dots*error\n",
    "# assemble the file name\n",
    "save_name = os.path.join(save_path, '_'.join((target_document, 'Interaction_diagonal')) + '.png')\n",
    "# save the figure\n",
    "plot = fp.save_figure(plot, save_name, fig_width=10, dpi=1200, fontsize=target_document, target='screen')\n",
    "\n",
    "# plot"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "80ab3021",
   "metadata": {},
   "source": [
    "# Top cells scatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129be44b",
   "metadata": {},
   "source": [
    "# plot the top and worst cells for each feature\n",
    "\n",
    "# allocate memory for the plot list\n",
    "plot_list = []\n",
    "# define the percentages to plot\n",
    "percentage_target = 10\n",
    "number_target = int(np.round(tc_whole.shape[0]*(percentage_target/100)))\n",
    "# for all the features\n",
    "for feature in variable_list[:4]:\n",
    "    # get the TC and regression data\n",
    "#     print(tc_whole.columns)\n",
    "    tc_resp = tc_whole.loc[:, feature+'_Resp_index'].to_numpy().copy()\n",
    "    tc_cons = tc_whole.loc[:, feature+'_Cons_index'].to_numpy().copy()\n",
    "    tc_quals = tc_whole.loc[:, feature+'_Qual_index'].to_numpy().copy()\n",
    "    tc_data = tc_quals\n",
    "#     tc_data = np.abs(tc_data)\n",
    "#     tc_data[tc_data > 100] = 0\n",
    "    regression_data = regression_df.loc[:, feature].to_numpy().copy()\n",
    "    \n",
    "    # exclude nan values\n",
    "    keep_vector = ~np.isnan(tc_data)\n",
    "    tc_data = tc_data[keep_vector]\n",
    "    regression_data = regression_data[keep_vector]\n",
    "    # get the indexes of the top and bottom n percent\n",
    "    idx = np.argsort(regression_data)\n",
    "    \n",
    "    # get the top and bottom\n",
    "    tc_top = tc_data[idx[-number_target:]]\n",
    "    regression_top = regression_data[idx[-number_target:]]\n",
    "    \n",
    "    tc_bottom = tc_data[idx[:number_target]]\n",
    "    regression_bottom = regression_data[idx[:number_target]]\n",
    "    \n",
    "#     # artificially sort them\n",
    "#     tc_top.sort()\n",
    "#     regression_top.sort()\n",
    "    \n",
    "    correlation = stat.spearmanr(tc_top, regression_top)[0]\n",
    "    print(correlation)\n",
    "    \n",
    "    # plot\n",
    "    top_scatter = hv.Scatter((regression_top, tc_top), kdims=['Imp. index'], vdims='TC index')\n",
    "#     top_scatter.opts(xrotation=45, title=f'{feature}: {correlation:0.2f}', width=300, height=300)\n",
    "    top_scatter.opts(xrotation=45, frame_width=300, frame_height=300, title=label_dict[feature], yticks=3, xticks=3)\n",
    "    bottom_scatter = hv.Scatter((tc_bottom, regression_bottom))\n",
    "    \n",
    "    # assemble the file name\n",
    "    save_name = os.path.join(save_path, '_'.join((target_document, 'Top_weights', feature)) + '.png')\n",
    "    # save the figure\n",
    "    top_scatter = fp.save_figure(top_scatter, save_name, fig_width=2.8, dpi=1200, fontsize=target_document, target='screen')\n",
    "    \n",
    "    plot_list.append(top_scatter)\n",
    "    \n",
    "# hv.Layout(plot_list).cols(4).opts(shared_axes=False)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b12a75",
   "metadata": {},
   "source": [
    "# normalize the weights between 0-1\n",
    "\n",
    "regression_normalized = regression_df.copy()\n",
    "\n",
    "# regression_normalized.loc[:, variable_list] = (regression_normalized.loc[:, variable_list] - regression_normalized.loc[:, variable_list].min(axis=0)) / \\\n",
    "#     (regression_normalized.loc[:, variable_list].max(axis=0) - regression_normalized.loc[:, variable_list].min(axis=0))\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e86c5fd",
   "metadata": {},
   "source": [
    "# plot the cell matching based info (can use match_id for both methods since they are already sorted)\n",
    "\n",
    "print(regression_normalized.columns)\n",
    "# allocate memory for the correlations\n",
    "correlation_list = []\n",
    "# run through all the mice\n",
    "for mouse_name, mouse_data in regression_normalized.groupby(['animal']):\n",
    "    # if the mouse is empty, skip\n",
    "#     print(mouse_data.shape)\n",
    "#     if mouse_data.shape[0] == 0:\n",
    "#         continue\n",
    "    #     print(mouse)\n",
    "    mouse_list = []\n",
    "    \n",
    "    for cell_name, cell_data in mouse_data.groupby(['match_id']):\n",
    "        \n",
    "        # skip the singles\n",
    "        if cell_data.shape[0] == 1:\n",
    "            continue\n",
    "        # get the features\n",
    "        features = cell_data[variable_list].to_numpy()\n",
    "        # calculate the correlation matrix\n",
    "        correlation = stat.spearmanr(features.T, nan_policy='omit')[0]\n",
    "        # get the average of the off diagonal elements\n",
    "#         print(correlation)\n",
    "        \n",
    "        if isinstance(correlation, np.ndarray):\n",
    "            triu = np.triu(correlation, k=1)\n",
    "            average = np.mean(triu[triu != 0])\n",
    "        else:\n",
    "            average = correlation\n",
    "#         print(type(average))\n",
    "#         average = np.float(average)\n",
    "        # store\n",
    "        mouse_list.append([mouse_name, cell_name, average])\n",
    "#         print(type(mouse_list[-1][-1]))\n",
    "#         print(np.vstack(mouse_list))\n",
    "#         raise ValueError\n",
    "    # if no matches were found, skip\n",
    "    if len(mouse_list) == 0:\n",
    "        continue\n",
    "    correlation_list.append(pd.DataFrame(np.vstack(mouse_list), columns=['animal', 'match_id', 'correlation']))\n",
    "#     print(correlation_list[0])\n",
    "#     raise ValueError\n",
    "    \n",
    "correlation_list = pd.concat(correlation_list)\n",
    "correlation_list['correlation'] = correlation_list['correlation'].to_numpy().astype(float)\n",
    "print(correlation_list)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128601a0",
   "metadata": {},
   "source": [
    "# plot the distribution of correlations\n",
    "\n",
    "plot_list = []\n",
    "# # for all the mice\n",
    "# for mouse_name, mouse_data in correlation_list.groupby(['animal']):\n",
    "#     plot = hv.Scatter(mouse_data['correlation'])\n",
    "# plot_array = correlation_list[['animal', 'correlation']].copy().reset_index(drop=True)\n",
    "plot_array = correlation_list\n",
    "# plot_array[plot_array.isna()] = 0\n",
    "# print(type(plot_array.iloc[1, 1]))\n",
    "# print(type(float(plot_array.iloc[1, 1])))\n",
    "# plot_array[np.isinf(plot_array)] = 0\n",
    "box = hv.BoxWhisker(plot_array, ['animal'], ['correlation'])\n",
    "box.opts(width=800, height=600, xrotation=45)\n",
    "box"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c4dca1",
   "metadata": {},
   "source": [
    "# calculate averages per mouse\n",
    "\n",
    "# allocate memory for the correlations\n",
    "correlation_interval = []\n",
    "# run through all the mice\n",
    "for mouse_name, mouse_data in regression_normalized.groupby(['animal']):\n",
    "    \n",
    "    mouse_list = []    \n",
    "    random_list = []\n",
    "    for cell_name, cell_data in mouse_data.groupby(['match_id']):\n",
    "        # skip the singles\n",
    "        if cell_data.shape[0] == 1:\n",
    "            continue\n",
    "        # get the features\n",
    "        features = cell_data[variable_list]\n",
    "        \n",
    "        # calculate the correlation matrix\n",
    "        correlation = stat.spearmanr(features.T, nan_policy='omit')[0]\n",
    "        \n",
    "        # get a set of random cells of this size\n",
    "        random_idx = np.random.randint(0, mouse_data.shape[0]-1, cell_data.shape[0])\n",
    "        random_cells = mouse_data.iloc[random_idx, :]\n",
    "        \n",
    "        random_correlation = stat.spearmanr(random_cells[variable_list].T, nan_policy='omit')[0]\n",
    "        \n",
    "        if isinstance(random_correlation, float) & isinstance(correlation, np.ndarray):\n",
    "            random_correlation = np.zeros_like(correlation)*np.nan\n",
    "        elif isinstance(correlation, float) & isinstance(random_correlation, np.ndarray):\n",
    "            random_correlation = np.nan\n",
    "        # get the delta days\n",
    "        day_data = cell_data.loc[:, 'day'].to_numpy()\n",
    "        delta_days = [(el-day_data[0]) for el in day_data]\n",
    "        delta_days = (delta_days/np.timedelta64(1, 'D')).astype(int)\n",
    "\n",
    "        # get the average of the off diagonal elements\n",
    "        if isinstance(correlation, np.ndarray):\n",
    "            # for all the off diagonals\n",
    "            for el in np.arange(1, correlation.shape[0]):\n",
    "                diag = np.diagonal(correlation, offset=el)\n",
    "                average = np.mean(diag)\n",
    "                \n",
    "                random_diag = np.diagonal(random_correlation, offset=el)\n",
    "                random_average = np.mean(random_diag)\n",
    "                # store\n",
    "                mouse_list.append([mouse_name, cell_name, delta_days[el], average])   \n",
    "                random_list.append([mouse_name+'_random', cell_name, delta_days[el], random_average])   \n",
    "        else:\n",
    "            average = correlation\n",
    "            random_average = random_correlation\n",
    "            # store\n",
    "            mouse_list.append([mouse_name, cell_name, delta_days[1], average])\n",
    "            random_list.append([mouse_name+'_random', cell_name, delta_days[1], random_average])\n",
    "    # if no matches were found, skip\n",
    "    if len(mouse_list) == 0:\n",
    "        continue\n",
    "    correlation_interval.append(pd.DataFrame(np.vstack(mouse_list), columns=['animal', 'match_id', 'interval', 'correlation']))\n",
    "    correlation_interval.append(pd.DataFrame(np.vstack(random_list), columns=['animal', 'match_id', 'interval', 'correlation']))\n",
    "\n",
    "    \n",
    "correlation_interval = pd.concat(correlation_interval)\n",
    "correlation_interval['correlation'] = correlation_interval['correlation'].to_numpy().astype(float)\n",
    "\n",
    "print(correlation_interval.shape)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cde4ca",
   "metadata": {},
   "source": [
    "# plot the correlations per animal\n",
    "\n",
    "# allocate a list for the plots\n",
    "plot_list = []\n",
    "# calculate the mean and sem\n",
    "grouped_data = correlation_interval.groupby(['animal'], as_index=False)[['interval', 'correlation']]\n",
    "\n",
    "# for all the animals\n",
    "for mouse_name, mouse_data in grouped_data:\n",
    "    \n",
    "    mouse_mean = mouse_data.groupby(['interval'], as_index=False)['correlation'].mean()\n",
    "    mouse_mean.loc[:, 'interval'] = mouse_mean.loc[:, 'interval'].astype(float)\n",
    "    mouse_mean = mouse_mean.sort_values(['interval'])\n",
    "    \n",
    "    mouse_sem = mouse_data.groupby(['interval'], as_index=False)['correlation'].sem()\n",
    "    mouse_sem.loc[:, 'interval'] = mouse_sem.loc[:, 'interval'].astype(float)\n",
    "    mouse_sem = mouse_sem.sort_values(['interval'])\n",
    "    \n",
    "    mean_plot = hv.Curve((mouse_mean['interval'], mouse_mean['correlation']), kdims=['Interval'], vdims='Correlation', label=mouse_name)\n",
    "    sem_plot = hv.Spread((mouse_mean['interval'], mouse_mean['correlation'], mouse_sem['correlation']))\n",
    "    mean_plot.opts(height=600, width=800, xrotation=45)\n",
    "    \n",
    "    plot_list.append(mean_plot*sem_plot)\n",
    "hv.Overlay(plot_list)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b268fb",
   "metadata": {},
   "source": [
    "# calculate averages per feature\n",
    "\n",
    "# allocate memory for the correlations\n",
    "feature_diffs = {}\n",
    "\n",
    "# for all the features\n",
    "for feature in variable_list:\n",
    "    feature_df = []\n",
    "    # run through all the mice\n",
    "    for mouse_name, mouse_data in regression_normalized.groupby(['animal']):\n",
    "        mouse_list = []\n",
    "        random_list = []\n",
    "        for cell_name, cell_data in mouse_data.groupby(['match_id']):\n",
    "            # skip the singles\n",
    "            if cell_data.shape[0] == 1:\n",
    "                continue\n",
    "            # get the features\n",
    "            features = cell_data[feature]\n",
    "\n",
    "            # get the distance matrix\n",
    "            distances = metrics.pairwise_distances(features.to_numpy().reshape(-1, 1))\n",
    "#             distances[distances == 0] = np.nan\n",
    "#             distances = 1 - distances\n",
    "    \n",
    "            # get a set of random cells of this size\n",
    "            random_idx = np.random.randint(0, mouse_data.shape[0]-1, cell_data.shape[0])\n",
    "            random_cells = mouse_data.iloc[random_idx, :]\n",
    "            random_distances = metrics.pairwise_distances(random_cells[feature].to_numpy().reshape(-1, 1))\n",
    "#             random_distances = 1 - random_distances\n",
    "            \n",
    "            # get the delta days\n",
    "            day_data = cell_data.loc[:, 'day'].to_numpy()\n",
    "            delta_days = [(el-day_data[0]) for el in day_data]\n",
    "            delta_days = (delta_days/np.timedelta64(1, 'D')).astype(int)\n",
    "            # get the average of the off diagonal elements\n",
    "            if isinstance(distances, np.ndarray):\n",
    "                # for all the off diagonals\n",
    "                for el in np.arange(1, distances.shape[0]):\n",
    "                    diag = np.diagonal(distances, offset=el)\n",
    "                    average = np.mean(diag)\n",
    "                    mouse_list.append([mouse_name, cell_name, delta_days[el], average])\n",
    "                    \n",
    "                    random_diag = np.diagonal(random_distances, offset=el)\n",
    "                    random_average = np.mean(random_diag)\n",
    "                    random_list.append([mouse_name+'_random', 'random', delta_days[el], random_average])\n",
    "\n",
    "            else:\n",
    "                average = distances\n",
    "                random_average = random_distances\n",
    "\n",
    "                # store\n",
    "                mouse_list.append([mouse_name, cell_name, delta_days[1], average])\n",
    "                random_list.append([mouse_name+'_random', 'random', delta_days[1], random_average])\n",
    "        # if not matches were found, skip\n",
    "        if len(mouse_list) == 0:\n",
    "            continue\n",
    "        feature_df.append(pd.DataFrame(np.vstack(mouse_list), columns=['animal', 'match_id', 'interval', 'similarity']))\n",
    "        feature_df.append(pd.DataFrame(np.vstack(random_list), columns=['animal', 'match_id', 'interval', 'similarity']))\n",
    "\n",
    "    feature_df = pd.concat(feature_df)\n",
    "    feature_df['similarity'] = feature_df['similarity'].to_numpy().astype(float)\n",
    "    # store for the particular feature\n",
    "    feature_diffs[feature] = feature_df\n",
    "\n",
    "print(feature_diffs[feature].shape)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9192d436",
   "metadata": {},
   "source": [
    "# plot the per-animal and per-feature distances\n",
    "main_list = []\n",
    "\n",
    "# for all the features\n",
    "for feature in variable_list:\n",
    "    plot_list = []\n",
    "    # get the dataframe\n",
    "    current_df = feature_diffs[feature]\n",
    "    \n",
    "    # calculate the mean and sem\n",
    "    grouped_data = current_df.groupby(['animal'], as_index=False)[['interval', 'similarity']]\n",
    "\n",
    "    # for all the animals\n",
    "    for mouse_name, mouse_data in grouped_data:\n",
    "\n",
    "        mouse_mean = mouse_data.groupby(['interval'], as_index=False)['similarity'].mean()\n",
    "        mouse_mean.loc[:, 'interval'] = mouse_mean.loc[:, 'interval'].astype(float)\n",
    "        mouse_mean = mouse_mean.sort_values(['interval'])\n",
    "\n",
    "        mouse_sem = mouse_data.groupby(['interval'], as_index=False)['similarity'].sem()\n",
    "        mouse_sem.loc[:, 'interval'] = mouse_sem.loc[:, 'interval'].astype(float)\n",
    "        mouse_sem = mouse_sem.sort_values(['interval'])\n",
    "\n",
    "        mean_plot = hv.Curve((mouse_mean['interval'], mouse_mean['similarity']), kdims=['Interval'], vdims='Similarity')\n",
    "        sem_plot = hv.Spread((mouse_mean['interval'], mouse_mean['similarity'], mouse_sem['similarity']))\n",
    "        mean_plot.opts(height=400, width=400, xrotation=45, title=feature)\n",
    "\n",
    "        plot_list.append(mean_plot*sem_plot)\n",
    "    main_list.append(hv.Overlay(plot_list))\n",
    "\n",
    "hv.Layout(main_list).cols(4).opts(shared_axes=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d80cf5d",
   "metadata": {},
   "source": [
    "# plot across intervals\n",
    "\n",
    "# plot the per-animal and per-feature distances\n",
    "main_list = []\n",
    "# define the target interval\n",
    "target_interval = '1'\n",
    "# for all the features\n",
    "for feature in variable_list:\n",
    "    # get the dataframe\n",
    "#     current_df = feature_diffs[feature]\n",
    "#     current_df['feature'] = feature\n",
    "    # get the dataframe\n",
    "    current_df = feature_diffs[feature]\n",
    "    current_real = current_df[(current_df['match_id'] != 'random') & (current_df['interval'] == target_interval)]\n",
    "    current_random = current_df[(current_df['match_id'] == 'random') & (current_df['interval'] == target_interval)]\n",
    "    current_real['feature'] = feature\n",
    "    current_random['feature'] = feature+'_random'\n",
    "    \n",
    "    current_real['similarity'] = current_real['similarity'].fillna(0)\n",
    "    current_random['similarity'] = current_random['similarity'].fillna(0)\n",
    "#     print((current_df['interval'] ))\n",
    "    print(f\"{feature}: {stat.mannwhitneyu(current_real['similarity'], current_random['similarity'])[1]*len(variable_list):0.4f}\")\n",
    "\n",
    "    # store\n",
    "    main_list.append(current_real)\n",
    "    main_list.append(current_random)\n",
    "\n",
    "    # store\n",
    "    main_list.append(current_df)\n",
    "\n",
    "# turn into a dataframe\n",
    "main_list = pd.concat(main_list)\n",
    "\n",
    "box = hv.BoxWhisker(main_list, ['feature'], ['similarity'])\n",
    "box.opts(width=1200, height=600, xrotation=45, ylabel='distance')\n",
    "box\n",
    "#         mean_plot = hv.Curve((mouse_mean['interval'], mouse_mean['correlation']))\n",
    "#         sem_plot = hv.Spread((mouse_mean['interval'], mouse_mean['correlation'], mouse_sem['correlation']))\n",
    "#         mean_plot.opts(height=400, width=400, xrotation=45, title=feature)\n",
    "\n",
    "#         plot_list.append(mean_plot*sem_plot)\n",
    "#     main_list.append(hv.Overlay(plot_list))\n",
    "\n",
    "# hv.Layout(main_list).cols(4).opts(shared_axes=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed5af44",
   "metadata": {},
   "source": [
    "# get the tc similarities by correlating the actual TCs\n",
    "\n",
    "# print(tc_whole.columns[:50])\n",
    "# allocate the output\n",
    "tc_interval = {}\n",
    "# for all the features\n",
    "for feature in variable_list:\n",
    "    # get the relevant columns\n",
    "    current_columns = [el for el in tc_whole.columns if ('bin' in el) & (feature in el)]\n",
    "#     print(current_columns)\n",
    "#     raise ValueError\n",
    "    current_df = tc_whole[['day', 'animal', 'match_id'] + current_columns]\n",
    "    \n",
    "    feature_df = []\n",
    "    # run through all the mice\n",
    "    for mouse_name, mouse_data in current_df.groupby(['animal']):\n",
    "        mouse_list = []\n",
    "        random_list = []\n",
    "        index_list = []\n",
    "        for match_id, cell_data in mouse_data.groupby(['match_id']):\n",
    "            # skip the singles\n",
    "            if cell_data.shape[0] < 3:\n",
    "                continue\n",
    "            # calculate the correlation\n",
    "#             correlation = stat.spearmanr(cell_data[current_columns].T, nan_policy='omit')[0]\n",
    "            current_cells = cell_data[current_columns].to_numpy()\n",
    "            current_cells[np.isnan(current_cells)] = 0    \n",
    "            correlation = metrics.pairwise_distances(current_cells)\n",
    "\n",
    "            \n",
    "            # get a set of random cells of this size\n",
    "            random_idx = np.random.randint(0, mouse_data.shape[0]-1, cell_data.shape[0])\n",
    "            random_cells = mouse_data.iloc[random_idx, :]\n",
    "            random_cells = random_cells[current_columns].to_numpy()\n",
    "            random_cells[np.isnan(random_cells)] = 0    \n",
    "#             random_correlation = stat.spearmanr(random_cells[current_columns].T, nan_policy='omit')[0]\n",
    "            random_correlation = metrics.pairwise_distances(random_cells)\n",
    "            \n",
    "            if isinstance(random_correlation, float) & isinstance(correlation, np.ndarray):\n",
    "                random_correlation = np.zeros_like(correlation)*np.nan\n",
    "            elif isinstance(correlation, float) & isinstance(random_correlation, np.ndarray):\n",
    "                random_correlation = np.nan\n",
    "            \n",
    "            # get the delta days\n",
    "            day_data = cell_data.loc[:, 'day'].to_numpy()\n",
    "            delta_days = [(el-day_data[0]) for el in day_data]\n",
    "            delta_days = (delta_days/np.timedelta64(1, 'D')).astype(int)\n",
    "            # get the average of the off diagonal elements\n",
    "            if isinstance(correlation, np.ndarray):\n",
    "                # for all the off diagonals\n",
    "                for el in np.arange(1, correlation.shape[0]):\n",
    "                    diag = np.diagonal(correlation, offset=el)\n",
    "                    average = np.mean(diag)\n",
    "                    \n",
    "                    random_diag = np.diagonal(random_correlation, offset=el)\n",
    "                    random_average = np.mean(random_diag)\n",
    "                    # store\n",
    "                    mouse_list.append([mouse_name, match_id, delta_days[el], average])      \n",
    "                    random_list.append([mouse_name+'_random', 'random', delta_days[el], random_average])      \n",
    "            else:\n",
    "                average = correlation\n",
    "                random_average = random_correlation\n",
    "                # store\n",
    "                mouse_list.append([mouse_name, match_id, delta_days[1], average])\n",
    "                random_list.append([mouse_name+'_random', 'random', delta_days[1], random_average])\n",
    "\n",
    "        if len(mouse_list) == 0:\n",
    "            continue\n",
    "        feature_df.append(pd.DataFrame(np.vstack(mouse_list), columns=['animal', 'match_id', 'interval', 'correlation']))\n",
    "        feature_df.append(pd.DataFrame(np.vstack(random_list), columns=['animal', 'match_id', 'interval', 'correlation']))\n",
    "    \n",
    "    \n",
    "    feature_df = pd.concat(feature_df)\n",
    "    feature_df['correlation'] = feature_df['correlation'].to_numpy().astype(float)\n",
    "    \n",
    "    # store for the particular feature\n",
    "    tc_interval[feature] = feature_df\n",
    "\n",
    "print(tc_interval[feature].shape)\n",
    "    "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674de7ca",
   "metadata": {},
   "source": [
    "# plot the tc correlations as a function of feature\n",
    "main_list = []\n",
    "\n",
    "# for all the features\n",
    "for feature in variable_list:\n",
    "    plot_list = []\n",
    "    # get the dataframe\n",
    "    current_df = tc_interval[feature]\n",
    "    \n",
    "    # calculate the mean and sem\n",
    "    grouped_data = current_df.groupby(['animal'], as_index=False)[['interval', 'correlation']]\n",
    "\n",
    "    # for all the animals\n",
    "    for mouse_name, mouse_data in grouped_data:\n",
    "\n",
    "        mouse_mean = mouse_data.groupby(['interval'], as_index=False)['correlation'].mean()\n",
    "        mouse_mean.loc[:, 'interval'] = mouse_mean.loc[:, 'interval'].astype(float)\n",
    "        mouse_mean = mouse_mean.sort_values(['interval'])\n",
    "\n",
    "        mouse_sem = mouse_data.groupby(['interval'], as_index=False)['correlation'].sem()\n",
    "        mouse_sem.loc[:, 'interval'] = mouse_sem.loc[:, 'interval'].astype(float)\n",
    "        mouse_sem = mouse_sem.sort_values(['interval'])\n",
    "        \n",
    "        mean_plot = hv.Curve((mouse_mean['interval'], mouse_mean['correlation']), kdims=['Interval'], vdims='Correlation')\n",
    "        sem_plot = hv.Spread((mouse_mean['interval'], mouse_mean['correlation'], mouse_sem['correlation']))\n",
    "        if 'random' in mouse_name:\n",
    "            color = 'k'\n",
    "        else:\n",
    "            color = 'r'    \n",
    "        mean_plot.opts(height=400, width=400, xrotation=45, title=feature, color=color)\n",
    "        sem_plot.opts(height=400, width=400, xrotation=45, title=feature, color=color)\n",
    "\n",
    "        plot_list.append(mean_plot*sem_plot)\n",
    "    main_list.append(hv.Overlay(plot_list))\n",
    "\n",
    "hv.Layout(main_list).cols(4).opts(shared_axes=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbb8ff4",
   "metadata": {},
   "source": [
    "# plot the per-animal and per-feature distances\n",
    "main_list = []\n",
    "\n",
    "# define the target interval\n",
    "target_interval = '1'\n",
    "# for all the features\n",
    "for feature in variable_list:\n",
    "    # get the dataframe\n",
    "    current_df = tc_interval[feature]\n",
    "    current_real = current_df[(current_df['match_id'] != 'random') & (current_df['interval'] == target_interval)]\n",
    "    current_random = current_df[(current_df['match_id'] == 'random') & (current_df['interval'] == target_interval)]\n",
    "    current_real['feature'] = feature\n",
    "    current_random['feature'] = feature+'_random'\n",
    "    \n",
    "    current_real['correlation'] = current_real['correlation'].fillna(0)\n",
    "    current_random['correlation'] = current_random['correlation'].fillna(0)\n",
    "    \n",
    "    print(f\"{feature}: {stat.mannwhitneyu(current_real['correlation'], current_random['correlation'])[1]*len(variable_list):0.4f} {current_real.shape[0]}\")\n",
    "#     # calculate the mean and sem\n",
    "#     data_mean = current_df.loc[:, 'correlation'].mean().to_numpy()\n",
    "#     data_sem = current_df.loc[:, 'correlation'].sem().to_numpy()\n",
    "    # store\n",
    "    main_list.append(current_real)\n",
    "    main_list.append(current_random)\n",
    "\n",
    "# turn into a dataframe\n",
    "main_list = pd.concat(main_list)\n",
    "\n",
    "box = hv.BoxWhisker(main_list, ['feature'], ['correlation'])\n",
    "box.opts(width=1000, height=600, xrotation=45, ylabel='Distance')\n",
    "box"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "3b61d88a",
   "metadata": {},
   "source": [
    "# Tuning curve examples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df94811",
   "metadata": {},
   "source": [
    "# plot top tuning curves\n",
    "importlib.reload(fp)\n",
    "# define the target variable\n",
    "target_variables = ['mouse_x', 'mouse_speed', 'cricket_0_mouse_distance']\n",
    "\n",
    "# get the tc ranges\n",
    "tc_ranges = processing_parameters.tc_params\n",
    "# target_variable = 'mouse_speed'\n",
    "# target_variable = 'cricket_0_mouse_distance'\n",
    "# define the number of cells to grab per animal\n",
    "target_number = 1\n",
    "# define how to select the TCs\n",
    "selection_mode = 'top'\n",
    "\n",
    "# allocate a list for the plots\n",
    "plot_list = []\n",
    "# for all the target variables\n",
    "for target_variable in target_variables:\n",
    "    # for all the mice\n",
    "    for mouse_name, mouse_data in regression_df_ori.groupby(['animal']):\n",
    "        # get the top weighted cell indexes\n",
    "\n",
    "        # get their tuning curves\n",
    "        mouse_tcs = tc_whole_ori[(tc_whole_ori['animal'] == mouse_name)]\n",
    "\n",
    "    #     target_sorter = mouse_data[target_variable].to_numpy()\n",
    "        target_sorter = mouse_tcs[target_variable+'_Qual_index'].to_numpy()\n",
    "        top_weight_idx = np.argsort(target_sorter)\n",
    "        top_weight = np.sort(target_sorter)\n",
    "\n",
    "        # exclude nans\n",
    "        keep_vector = ~np.isnan(top_weight)\n",
    "        top_weight_idx = top_weight_idx[keep_vector]\n",
    "        top_weight = top_weight[keep_vector]\n",
    "        # randomize\n",
    "#         np.random.shuffle(top_weight_idx)\n",
    "        if selection_mode == 'top':\n",
    "            # take the top n\n",
    "            top_weight_idx = top_weight_idx[-target_number:]\n",
    "            top_weight = top_weight[-target_number:]\n",
    "        elif 'bottom':\n",
    "            # take the bottom n\n",
    "            top_weight_idx = top_weight_idx[:target_number]\n",
    "            top_weight = top_weight[:target_number]\n",
    "        else:\n",
    "            # randomize\n",
    "            continue\n",
    "\n",
    "\n",
    "        tc_columns = [el for el in mouse_tcs.columns if (target_variable in el) & ('bin' in el)]\n",
    "        tc_quals = mouse_tcs[target_variable+'_Qual_index'].to_numpy()\n",
    "        tc_quals = tc_quals[top_weight_idx]\n",
    "        mouse_tcs = mouse_tcs[tc_columns]\n",
    "        mouse_tcs = mouse_tcs.iloc[top_weight_idx, :]\n",
    "        \n",
    "        # define the x ticks\n",
    "        curr_range = tc_ranges[target_variable]\n",
    "        curr_range = np.linspace(curr_range[0], curr_range[1], 20)\n",
    "        xticks = [(int(el), int(curr_range[el])) for el in np.arange(5, 20, 5)]\n",
    "        \n",
    "#         print(xticks)\n",
    "#         raise ValueError\n",
    "        # for all the tcs\n",
    "        for idx1, (idx, tc) in enumerate(mouse_tcs.iterrows()):\n",
    "            # get the day\n",
    "    #         current_day = str(mouse_data.iloc[top_weight[idx1], 'day'])\n",
    "            # plot\n",
    "            plot = hv.Curve((np.arange(tc.shape[0]), tc.to_numpy()), kdims='Variable', vdims='Signal')\n",
    "#             plot.opts(width=400, height=600, yaxis=None, xlabel=' '.join((label_dict[target_variable], units_dict[target_variable])), xticks=xticks)\n",
    "            plot.opts(width=400, height=400, yaxis=None, xticks=xticks, xlabel='')\n",
    "\n",
    "            save_name = os.path.join(save_path, '_'.join((target_document, 'TC_ex', target_variable, mouse_name, str(idx))) + '.png')\n",
    "            # save the figure\n",
    "            fig = fp.save_figure(plot, save_name, fig_width=2, dpi=1200, fontsize='small', target='save')#, display_factor=0.1)\n",
    "            plot_list.append(plot)\n",
    "    \n",
    "#     print(mouse_tcs)\n",
    "#     raise ValueError\n",
    "# hv.Layout(plot_list).opts(shared_axes=True).cols(target_number)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70d34c4",
   "metadata": {},
   "source": [
    "# visualize single cell TCs based on matching\n",
    "\n",
    "# define the target feature\n",
    "target_feature = 'mouse_speed'\n",
    "\n",
    "# define the number of groups to take\n",
    "target_number = 20\n",
    "\n",
    "# get only the non-random data\n",
    "# print(tc_interval[target_feature]['match_id'] != 'random')\n",
    "tc_nonrandom = tc_mean[target_feature][tc_mean[target_feature]['match_id'] != 'random']\n",
    "# get the top N groups for the target feature\n",
    "top_idx = np.argsort(tc_nonrandom['distance'].to_numpy())[:target_number]\n",
    "# print(np.sort(tc_nonrandom['correlation'].to_numpy())[-target_number:])\n",
    "# raise ValueError\n",
    "top_groups = tc_nonrandom.iloc[top_idx, :]\n",
    "\n",
    "# initialize the plot list\n",
    "plot_list = []\n",
    "# plot the corresponding TCs\n",
    "for idx, group in top_groups.iterrows():\n",
    "    # get the match id and mouse\n",
    "    target_mouse = group['animal']\n",
    "    target_id = float(group['match_id'])\n",
    "    # get the individual tuning curves\n",
    "    target_cells = tc_whole[(tc_whole['animal']==target_mouse) & (tc_whole['match_id']==target_id)]\n",
    "    print(target_mouse, target_id, target_cells.shape, group['distance'])\n",
    "    # get the relevant columns\n",
    "    target_columns = [el for el in tc_whole.columns if ('bin' in el) & (target_feature in el)]\n",
    "    target_cells = target_cells[target_columns]\n",
    "    x = np.arange(target_cells.shape[1])\n",
    "    # plot\n",
    "    # accumulate the tuning curves\n",
    "    tc_list = []\n",
    "    # for all the cells\n",
    "    for cell_idx, cell in target_cells.iterrows():\n",
    "        plot = hv.Curve((x, cell.to_numpy()), kdims=['Bin'], vdims='Tuning')\n",
    "        plot.opts(height=300, width=300)\n",
    "        tc_list.append(plot)\n",
    "    # store\n",
    "    plot_list.append(hv.Overlay(tc_list))\n",
    "# plot the layout\n",
    "layout = hv.Layout(plot_list).opts(shared_axes=False)\n",
    "layout"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-prey_capture] *",
   "language": "python",
   "name": "conda-env-.conda-prey_capture-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
