{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d77ec6",
   "metadata": {},
   "source": [
    "# imports\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import importlib\n",
    "sys.path.insert(0, os.path.abspath(r'C:/Users/mmccann/repos/bonhoeffer/prey_capture/'))\n",
    "import scipy.signal as ss\n",
    "import scipy.stats as stat\n",
    "\n",
    "import panel as pn\n",
    "import holoviews as hv\n",
    "from holoviews import opts, dim\n",
    "from holoviews.operation import histogram\n",
    "hv.extension('bokeh')\n",
    "from bokeh.resources import INLINE\n",
    "\n",
    "import paths\n",
    "import processing_parameters\n",
    "import functions_misc as fm\n",
    "import functions_bondjango as bd\n",
    "import snakemake_scripts.tc_calculate as tc\n",
    "import snakemake_scripts.wf_tc_calculate as wf_tc\n",
    "import functions_plotting as fp\n",
    "import functions_data_handling as fdh\n",
    "from functions_tuning import calculate_dff"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609e02cc",
   "metadata": {},
   "source": [
    "def calculate_information(occupancy, tuning_curve, average):\n",
    "    \"\"\"Calculate the information on a tuning curve based on Stefanini et al. 2020\"\"\"\n",
    "    information = np.nansum(occupancy*(tuning_curve/average) * np.log2(tuning_curve/average))\n",
    "    return information\n",
    "\n",
    "\n",
    "def clipping_function(trace_in, threshold=8):\n",
    "    \"\"\"Clip traces to their threshold-th percentile\"\"\"\n",
    "    # skip if there are only zeros\n",
    "    if np.sum(trace_in) == 0:\n",
    "        return trace_in\n",
    "    # get the baseline\n",
    "    baseline = np.percentile(trace_in[trace_in > 0], threshold)\n",
    "\n",
    "    # clip the trace\n",
    "    trace_in[trace_in < baseline] = 0\n",
    "    return trace_in\n",
    "\n",
    "\n",
    "def clip_calcium(pre_data):\n",
    "    \"\"\" Clip the calcium traces based on baseline \"\"\"\n",
    "\n",
    "    # allocate memory for the cleaned up data\n",
    "    data = []\n",
    "    # define the clipping threshold in percentile of baseline\n",
    "    clip_threshold = 8\n",
    "    # for all the trials\n",
    "    for idx, el in enumerate(pre_data):\n",
    "\n",
    "        # get the current df\n",
    "        current_df = el[1]\n",
    "        labels = list(current_df.columns)\n",
    "        cells = [el for el in labels if 'cell' in el]\n",
    "        not_cells = [el for el in labels if 'cell' not in el]\n",
    "        # get the non-cell data\n",
    "        non_cell_data = current_df[not_cells]\n",
    "        # get the current calcium data\n",
    "        cell_data = current_df[cells].fillna(0)\n",
    "\n",
    "        # do the cell clipping\n",
    "        cell_data.apply(clipping_function, axis=1, raw=True, threshold=clip_threshold)\n",
    "\n",
    "        # assemble a new data frame with only the matched cells and the rest of the data\n",
    "        data.append(pd.concat((non_cell_data, cell_data), axis=1))\n",
    "    return data\n",
    "\n",
    "\n",
    "def parse_features(data, feature_list, bin_number=10):\n",
    "    \"\"\"set up the feature and calcium matrices\"\"\"\n",
    "\n",
    "    # allocate memory for a data frame without the encoding model features\n",
    "    feature_raw_trials = []\n",
    "    # allocate memory for the calcium\n",
    "    calcium_trials = []\n",
    "\n",
    "    # get the features\n",
    "    for idx, el in enumerate(data):\n",
    "        # get the intersection of the labels\n",
    "        label_intersect = [feat for feat in feature_list if feat in el.columns]\n",
    "\n",
    "        # # add the y coordinate of the variables with x\n",
    "        # coordinate_variables = [column.replace('_x', '_y') for column in label_intersect if '_x' in column]\n",
    "        # label_intersect += coordinate_variables\n",
    "\n",
    "        # get the features of interest\n",
    "        target_features = el.loc[:, label_intersect]\n",
    "        # get the original columns\n",
    "        original_columns = target_features.columns\n",
    "\n",
    "        # for all the columns\n",
    "        for label in original_columns:\n",
    "            # skip if latent or motif\n",
    "            if ('latent' in label) | (label == 'motifs'):\n",
    "                target_features[label] = target_features[label]\n",
    "                continue\n",
    "\n",
    "            # smooth the feature\n",
    "            target_features[label] = ss.medfilt(target_features[label], 21)\n",
    "\n",
    "        # # allocate a copy of the target features for changes\n",
    "        # temp_features = target_features.copy()\n",
    "        # # for the coordinate variables, turn into a 2D grid\n",
    "        # for variable in coordinate_variables:\n",
    "        #     x_variable = target_features[variable.replace('_y', '_x')].to_numpy()\n",
    "        #     y_variable = target_features[variable].to_numpy()\n",
    "        #     bin_ranges = processing_parameters.tc_params[variable.replace('_y', '_x')]\n",
    "        #     bins = np.linspace(bin_ranges[0], bin_ranges[1], num=bin_number + 1)\n",
    "        #     # bin the variables in 2D\n",
    "        #     current_tc = \\\n",
    "        #         stat.binned_statistic_2d(x_variable, y_variable, y_variable, statistic='count', bins=bins,\n",
    "        #                                  expand_binnumbers=True)\n",
    "        #\n",
    "        #     binnumbers = current_tc[3]\n",
    "        #     # current_tc = np.ravel_multi_index((current_tc[3][0, :], current_tc[3][1, :]), (bin_ranges[0], bin_ranges[1]), mode='clip')\n",
    "        #     current_tc = np.ravel_multi_index(binnumbers, (11, 11), mode='raise')\n",
    "            # replace the x column in the target features\n",
    "\n",
    "            # eliminate the\n",
    "\n",
    "        # store the features\n",
    "        feature_raw_trials.append(target_features)\n",
    "\n",
    "        # get the calcium data\n",
    "        cells = [cell for cell in el.columns if 'cell' in cell]\n",
    "        cells = el.loc[:, cells].to_numpy()\n",
    "\n",
    "        # store\n",
    "        calcium_trials.append(cells)\n",
    "\n",
    "    return feature_raw_trials, calcium_trials\n",
    "\n",
    "\n",
    "def extract_tc_parts(current_feature_0, cell_number, calcium_trials, feature_counts, bins, num_splits=2):\n",
    "    \"\"\"\n",
    "    Extract the split tuning curves for consistency calculation. Generalized version of extract_half_tc\n",
    "    :param current_feature_0:\n",
    "    :param cell_number:\n",
    "    :param calcium_trials:\n",
    "    :param feature_counts:\n",
    "    :param bins:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    tc_part_temp = []\n",
    "\n",
    "    # Split the trace into parts\n",
    "    split_bounds = np.array_split(np.arange(current_feature_0.shape[0]), num_splits)\n",
    "    \n",
    "    for split, split_vector in enumerate(split_bounds):\n",
    "        # get the split feature\n",
    "        split_feature_0 = current_feature_0[split_vector]\n",
    "\n",
    "        # exclude nan values\n",
    "        keep_vector = ~np.isnan(split_feature_0)\n",
    "        keep_feature_0 = split_feature_0[keep_vector]\n",
    "\n",
    "        # allocate a list for the cells\n",
    "        tc_cell = []\n",
    "\n",
    "        # for all the cells\n",
    "        for cell in np.arange(cell_number):\n",
    "            # get the current cell\n",
    "            split_cell = calcium_trials[split_vector, cell]\n",
    "            keep_cell = split_cell[keep_vector]\n",
    "\n",
    "            # get the tc\n",
    "            current_tc = \\\n",
    "                stat.binned_statistic(keep_feature_0, keep_cell, statistic='sum', bins=bins)[0]\n",
    "\n",
    "            # normalize the TC\n",
    "            norm_tc = current_tc / feature_counts\n",
    "\n",
    "            # remove nans and infs\n",
    "            norm_tc[np.isnan(norm_tc)] = 0\n",
    "            norm_tc[np.isinf(norm_tc)] = 0\n",
    "            # store\n",
    "            tc_cell.append(norm_tc)\n",
    "\n",
    "        # store the cells\n",
    "        tc_part_temp.append(tc_cell)\n",
    "\n",
    "    return tc_part_temp\n",
    "\n",
    "\n",
    "def extract_half_tc(current_feature_0, cell_number, calcium_trials, feature_counts, bins):\n",
    "    \"\"\"Get the the half tuning curves for consistency calculation\"\"\"\n",
    "    tc_half_temp = []\n",
    "\n",
    "    # for first and second half\n",
    "    for half in np.arange(2):\n",
    "        # get the half vector\n",
    "        half_bound = int(np.floor(current_feature_0.shape[0] / 2))\n",
    "        half_vector = np.arange(half_bound) + half_bound * half\n",
    "        half_feature_0 = current_feature_0[half_vector]\n",
    "        # exclude nan values\n",
    "        keep_vector = ~np.isnan(half_feature_0)\n",
    "        keep_feature_0 = half_feature_0[keep_vector]\n",
    "\n",
    "        # allocate a list for the cells\n",
    "        tc_cell = []\n",
    "\n",
    "        # for all the cells\n",
    "        for cell in np.arange(cell_number):\n",
    "            # get the current cell\n",
    "            half_cell = calcium_trials[half_vector, cell]\n",
    "            keep_cell = half_cell[keep_vector]\n",
    "\n",
    "            # get the tc\n",
    "            current_tc = \\\n",
    "                stat.binned_statistic(keep_feature_0, keep_cell, statistic='sum', bins=bins)[0]\n",
    "\n",
    "            # normalize the TC\n",
    "            norm_tc = current_tc / feature_counts\n",
    "            # remove nans and infs\n",
    "            norm_tc[np.isnan(norm_tc)] = 0\n",
    "            norm_tc[np.isinf(norm_tc)] = 0\n",
    "            # store\n",
    "            tc_cell.append(norm_tc)\n",
    "\n",
    "        # store the cells\n",
    "        tc_half_temp.append(tc_cell)\n",
    "\n",
    "    return tc_half_temp\n",
    "\n",
    "\n",
    "def shuffle_random(cell, counts_feature_0, feature_counts, bins, tc_idx, shuffle_number=100):\n",
    "    # allocate memory for the shuffles\n",
    "    # shuffle_array = np.zeros((shuffle_number, working_bin_number))\n",
    "    shuffle_array = np.zeros((shuffle_number, 1))\n",
    "    shuffle_prediction = np.zeros((shuffle_number, 1))\n",
    "\n",
    "    # TODO change randomization to wrapping with lag (or rather add it in addition to the current one)\n",
    "    # use np.take\n",
    "    # shuffle the calcium activity\n",
    "    for shuffle in np.arange(shuffle_number):\n",
    "\n",
    "        # randomize the calcium activity\n",
    "        random_cell = cell.copy()\n",
    "        random_cell = np.random.choice(random_cell, cell.shape[0])\n",
    "\n",
    "        # Get the sum of the bins\n",
    "        tc_random = \\\n",
    "            stat.binned_statistic(counts_feature_0, random_cell, statistic='sum', bins=bins)[0]\n",
    "\n",
    "        # get the information\n",
    "        shuffle_array[shuffle] = calculate_information(feature_counts, tc_random, np.mean(random_cell))\n",
    "\n",
    "        # process the TC\n",
    "        tc_random = tc_random / feature_counts\n",
    "        tc_random[np.isnan(tc_random)] = 0\n",
    "        tc_random[np.isinf(tc_random)] = 0\n",
    "        # shuffle_array[shuffle, :] = tc_random\n",
    "\n",
    "        # use the tc_idx to regenerate the activity\n",
    "        predicted_calcium = tc_random[tc_idx-2]\n",
    "\n",
    "        # get the correlation with the real calcium\n",
    "        random_quality = stat.spearmanr(random_cell, predicted_calcium, nan_policy='omit')[0]\n",
    "        shuffle_prediction[shuffle] = random_quality\n",
    "\n",
    "        return shuffle_array, shuffle_prediction\n",
    "    \n",
    "\n",
    "def shuffle_random_bin(cell, counts_feature_0, feature_counts, bins, tc_idx, time_bin_width=0.5, shuffle_number=100):\n",
    "\n",
    "    # allocate memory for the shuffles\n",
    "    shuffle_array = np.zeros((shuffle_number, 1))\n",
    "    shuffle_prediction = np.zeros((shuffle_number, 1))\n",
    "\n",
    "    # generate a time vector and bin it into 500 ms bins\n",
    "    time_vector = np.arange(cell.shape[0], dtype=float) / processing_parameters.wf_frame_rate\n",
    "    bin_edges = np.arange(time_vector[0], time_vector[-1], time_bin_width)\n",
    "    binned_time_idxs = np.digitize(time_vector, bin_edges)\n",
    "    unique_time_bins = np.unique(binned_time_idxs)\n",
    "\n",
    "    # shuffle the calcium activity\n",
    "    for shuffle in np.arange(shuffle_number):\n",
    "        \n",
    "        # Shuffle the time while maintaining the binning. Deliberately oversample to ensure we have enough\n",
    "        random_time_bins = np.random.choice(unique_time_bins.copy(), int(unique_time_bins.shape[0] * 1.2), replace=True)\n",
    "        random_time_idxs = np.squeeze(np.concatenate([np.argwhere(binned_time_idxs == el) for el in random_time_bins]))\n",
    "\n",
    "        # Trim the indexes to size of calcium activity and randomize the calcium activity\n",
    "        random_cell = cell.copy()\n",
    "        random_time_idxs = random_time_idxs[:random_cell.shape[0]]\n",
    "        random_cell = random_cell[random_time_idxs]\n",
    "\n",
    "        # randomize the calcium activity\n",
    "        random_cell = cell.copy()\n",
    "        random_cell = random_cell[random_time_idxs]\n",
    "        # print(random_cell.shape, counts_feature_0.shape)\n",
    "        \n",
    "        # Get the sum of the bins\n",
    "        tc_random = \\\n",
    "            stat.binned_statistic(counts_feature_0, random_cell, statistic='sum', bins=bins)[0]\n",
    "\n",
    "        # get the information\n",
    "        shuffle_array[shuffle] = calculate_information(feature_counts, tc_random, np.mean(random_cell))\n",
    "\n",
    "        # process the TC\n",
    "        tc_random = tc_random / feature_counts\n",
    "        tc_random[np.isnan(tc_random)] = 0\n",
    "        tc_random[np.isinf(tc_random)] = 0\n",
    "\n",
    "        # use the tc_idx to regenerate the activity\n",
    "        predicted_calcium = tc_random[tc_idx-2]\n",
    "\n",
    "        # get the correlation with the real calcium\n",
    "        random_quality = stat.spearmanr(random_cell, predicted_calcium, nan_policy='omit')[0]\n",
    "        shuffle_prediction[shuffle] = random_quality\n",
    "\n",
    "    return shuffle_array, shuffle_prediction\n",
    "\n",
    "\n",
    "def add_lag(cell, counts_feature_0, feature_counts, bins, tc_idx, lag=0.5):\n",
    "    \n",
    "    # calculate how many indices to lag on each iteration\n",
    "    lag_step =  int(lag * processing_parameters.wf_frame_rate)\n",
    "\n",
    "    # get the number of lags to calculate\n",
    "    num_lags = int(cell.shape[0] // lag_step)\n",
    "\n",
    "    # allocate memory for the shuffles\n",
    "    shuffle_array = np.zeros((num_lags, 1))\n",
    "    shuffle_prediction = np.zeros((num_lags, 1))\n",
    "\n",
    "    # lag the calcium activity\n",
    "    for shuffle in np.arange(num_lags, dtype=int):\n",
    "\n",
    "        # lag the calcium activity\n",
    "        lag_cell = np.roll(cell.copy(), shuffle * lag_step)\n",
    "\n",
    "        # Get the sum of the bins\n",
    "        tc_random = \\\n",
    "            stat.binned_statistic(counts_feature_0, lag_cell, statistic='sum', bins=bins)[0]\n",
    "\n",
    "        # get the information\n",
    "        shuffle_array[shuffle] = calculate_information(feature_counts, tc_random, np.mean(lag_cell))\n",
    "\n",
    "        # process the TC\n",
    "        tc_random = tc_random / feature_counts\n",
    "        tc_random[np.isnan(tc_random)] = 0\n",
    "        tc_random[np.isinf(tc_random)] = 0\n",
    "\n",
    "        # use the tc_idx to regenerate the activity\n",
    "        predicted_calcium = tc_random[tc_idx-2]\n",
    "\n",
    "        # get the correlation with the real calcium\n",
    "        random_quality = stat.spearmanr(lag_cell, predicted_calcium, nan_policy='omit')[0]\n",
    "        shuffle_prediction[shuffle] = random_quality\n",
    "\n",
    "    return shuffle_array, shuffle_prediction\n",
    "\n",
    "\n",
    "def extract_full_tc(counts_feature_0, feature_counts, cell_number, calcium_trials, \n",
    "                    bins, keep_vector_full, shuffle_number, percentile, shuffle_kind='random', lag_or_bin=1):\n",
    "    \"\"\"Get the full tc\"\"\"\n",
    "    # allocate memory for the full tc per cell\n",
    "    tc_cell_full = []\n",
    "    tc_cell_resp = np.zeros((cell_number, 4))\n",
    "\n",
    "    # calculate the full TC\n",
    "    for cell in np.arange(cell_number):\n",
    "        keep_cell = calcium_trials[keep_vector_full, cell]\n",
    "\n",
    "        # Get the sum of the bins\n",
    "        tc_cell, _, tc_idx = \\\n",
    "            stat.binned_statistic(counts_feature_0, keep_cell, statistic='sum', bins=bins)\n",
    "        \n",
    "        # get the information\n",
    "        information_content = calculate_information(feature_counts, tc_cell, np.mean(keep_cell))\n",
    "\n",
    "        # process the TC\n",
    "        tc_cell = tc_cell / feature_counts\n",
    "        tc_cell[np.isnan(tc_cell)] = 0\n",
    "        tc_cell[np.isinf(tc_cell)] = 0\n",
    "\n",
    "        # use the tc_idx to regenerate the activity\n",
    "        predicted_calcium = tc_cell[tc_idx-2]\n",
    "\n",
    "        # get the correlation with the real calcium\n",
    "        tc_quality = stat.spearmanr(keep_cell, predicted_calcium, nan_policy='omit')[0]\n",
    "\n",
    "        # shuffle the calcium activity\n",
    "        if shuffle_kind == 'random':\n",
    "            shuffle_array, shuffle_prediction = shuffle_random(keep_cell, counts_feature_0, feature_counts, bins, tc_idx, \n",
    "                                                               shuffle_number=shuffle_number)\n",
    "        elif shuffle_kind == 'random_bin':\n",
    "            shuffle_array, shuffle_prediction = shuffle_random_bin(keep_cell, counts_feature_0, feature_counts, bins, tc_idx, \n",
    "                                                                   time_bin_width=lag_or_bin, shuffle_number=shuffle_number)\n",
    "        elif shuffle_kind == 'lag_wrap':\n",
    "            shuffle_array, shuffle_prediction = add_lag(keep_cell, counts_feature_0, feature_counts, bins, tc_idx, lag=lag_or_bin)\n",
    "        else:\n",
    "            raise ValueError('Shuffle kind not recognized')\n",
    "\n",
    "        # get the threshold\n",
    "        resp_threshold = np.percentile(np.abs(shuffle_array.flatten()), percentile)\n",
    "        qual_threshold = np.percentile(np.abs(shuffle_prediction.flatten()), percentile)\n",
    "\n",
    "        # fill up the responsivity matrix\n",
    "        # tc_cell_resp[cell, 0] = np.mean(np.sort(np.abs(tc_cell), axis=None)[-3:]) / resp_threshold\n",
    "        # tc_cell_resp[cell, 1] = np.sum(np.abs(tc_cell) > resp_threshold) > 3\n",
    "        tc_cell_resp[cell, 0] = information_content\n",
    "        tc_cell_resp[cell, 1] = np.abs(information_content) > resp_threshold\n",
    "        tc_cell_resp[cell, 2] = tc_quality\n",
    "        tc_cell_resp[cell, 3] = np.abs(tc_quality) > qual_threshold\n",
    "\n",
    "        # store\n",
    "        tc_cell_full.append(tc_cell)\n",
    "\n",
    "    return tc_cell_full, tc_cell_resp\n",
    "\n",
    "\n",
    "def extract_tcs_responsivity(feature_raw_trials, calcium_trials, target_variables, cell_number,\n",
    "                             percentile=99, bin_number=10, shuffle_kind='random'):\n",
    "    '''\n",
    "    Extract the tuning curves (full and half) and their responsivity index\n",
    "    \n",
    "    feature_raw_trials: (pd.DataFrame) The kinematic features \n",
    "    calcium_trials: (np.array)  The calcium traces (cells x time)\n",
    "    target_variables: (list of str) names of the variables to extract\n",
    "    cell_number: (int) number of cells\n",
    "    percentile: (int) percentile for the responsivity index\n",
    "    bin_number: (int) number of bins for the tuning curves (bins tile the range of the TCs)\n",
    "    '''\n",
    "    \n",
    "    # get the number of pairs\n",
    "    var_number = len(target_variables)\n",
    "\n",
    "    # define the number of calcium shuffles\n",
    "    shuffle_number = 100\n",
    "\n",
    "    # allocate memory for the trial TCs\n",
    "    tc_half = {}\n",
    "    tc_full = {}\n",
    "    tc_resp = {}\n",
    "    tc_counts = {}\n",
    "    tc_edges = {}\n",
    "    # initialize the template_idx\n",
    "    template_idx = -1\n",
    "    # for all the features\n",
    "    for var_idx in np.arange(var_number):\n",
    "        # get the current feature\n",
    "        feature_name = target_variables[var_idx]\n",
    "        # feature_names = feature_name.split('__')\n",
    "        # skip the pair and save an empty if the feature is not present\n",
    "        try:\n",
    "            current_feature_0 = feature_raw_trials.loc[:, feature_name].to_numpy()\n",
    "            # save the index of the feature\n",
    "            template_idx = var_idx\n",
    "            # current_feature_1 = feature_raw_trials.loc[:, feature_names[1]].to_numpy()\n",
    "        except KeyError:\n",
    "            tc_half[feature_name] = []\n",
    "            tc_full[feature_name] = []\n",
    "            tc_resp[feature_name] = []\n",
    "            tc_counts[feature_name] = []\n",
    "            tc_edges[feature_name] = []\n",
    "            continue\n",
    "\n",
    "        # get the bins from the parameters file (bins based on range of the data)\n",
    "        try:\n",
    "            bin_ranges = processing_parameters.tc_params[feature_name]\n",
    "            # calculate the bin edges based on the ranges\n",
    "            if len(bin_ranges) == 1:\n",
    "                bins = np.arange(bin_ranges[0] + 1) - 0.5\n",
    "                # bins = bin_ranges[0]\n",
    "\n",
    "            else:\n",
    "                bins = np.linspace(bin_ranges[0], bin_ranges[1], num=bin_number + 1)\n",
    "\n",
    "        except KeyError:\n",
    "            # if not in the parameters, go for default and report\n",
    "            print(f'Feature {feature_name} not found, default to 10 bins ad hoc')\n",
    "            bins = 10\n",
    "\n",
    "        # exclude nan values\n",
    "        keep_vector_full = ~np.isnan(current_feature_0)\n",
    "        counts_feature_0 = current_feature_0[keep_vector_full]\n",
    "\n",
    "        # get the counts for each range bin\n",
    "        feature_counts_raw, tc_current_edges, _ = \\\n",
    "            stat.binned_statistic(counts_feature_0, counts_feature_0, statistic='count', bins=bins)\n",
    "        feature_counts = feature_counts_raw.copy()\n",
    "\n",
    "        # zero the positions with less than 3 counts\n",
    "        feature_counts[feature_counts < 3] = 0\n",
    "\n",
    "        # get the half tuning curves\n",
    "        tc_splits = processing_parameters.tc_consistency_splits\n",
    "        tc_half_temp = extract_tc_parts(current_feature_0, cell_number, calcium_trials, feature_counts, bins, num_splits=tc_splits)\n",
    "\n",
    "        # get the full tuning curves\n",
    "        tc_cell_full, tc_cell_resp = extract_full_tc(counts_feature_0, feature_counts, cell_number, calcium_trials, \n",
    "                                                     bins, keep_vector_full, shuffle_number, percentile, shuffle_kind, \n",
    "                                                     lag_or_bin=processing_parameters.tc_lags[feature_name])\n",
    "        # store the halves and fulls\n",
    "        tc_half[feature_name] = tc_half_temp\n",
    "        tc_full[feature_name] = tc_cell_full\n",
    "        tc_resp[feature_name] = tc_cell_resp\n",
    "        tc_counts[feature_name] = feature_counts_raw\n",
    "        tc_edges[feature_name] = tc_current_edges\n",
    "    \n",
    "    # run through the features and fill up the non-populated ones with nan\n",
    "    for feat in tc_half.keys():\n",
    "        if len(tc_half[feat]) == 0:\n",
    "            tc_half[feat] = tc_half[target_variables[template_idx]]\n",
    "\n",
    "            for i in np.arange(tc_splits):\n",
    "                tc_half[feat][i] = [el * np.nan for el in tc_half[feat][i]]\n",
    "\n",
    "            tc_full[feat] = tc_full[target_variables[template_idx]]\n",
    "            tc_full[feat] = [el * np.nan for el in tc_full[feat]]\n",
    "            tc_resp[feat] = tc_resp[target_variables[template_idx]] * np.nan\n",
    "            tc_counts[feat] = tc_counts[target_variables[template_idx]] * np.nan\n",
    "            tc_edges[feat] = tc_edges[target_variables[template_idx]] * np.nan\n",
    "\n",
    "    return tc_half, tc_full, tc_resp, tc_counts, tc_edges\n",
    "\n",
    "\n",
    "def extract_consistency(tc_half, target_variables, cell_number, shuffle_kind='random', comp_kind='halves', percentile=95):\n",
    "    \"\"\"Calculate TC consistency\"\"\"\n",
    "\n",
    "    # define the number of shuffles\n",
    "    shuffle_number = 100\n",
    "\n",
    "    # get the number of pairs\n",
    "    var_number = len(target_variables)\n",
    "\n",
    "    # get the number of splits\n",
    "    num_splits = processing_parameters.tc_consistency_splits\n",
    "\n",
    "    # allocate memory for the trial TCs\n",
    "    tc_cons = {}\n",
    "\n",
    "    # for all the features\n",
    "    for var_idx in np.arange(var_number):\n",
    "\n",
    "        # get the name\n",
    "        feature_name = target_variables[var_idx]\n",
    "\n",
    "        # get the two halves\n",
    "        halves = tc_half[feature_name]\n",
    "\n",
    "        # allocate an array for the correlations and tests\n",
    "        tc_half_temp = np.zeros([cell_number, num_splits])\n",
    "\n",
    "        # if empty, skip\n",
    "        if len(halves) == 0:\n",
    "            tc_cons[feature_name] = []\n",
    "            continue\n",
    "\n",
    "        # calculate the real and shuffle correlation\n",
    "        for cell in np.arange(cell_number):\n",
    "\n",
    "            # get the current cell first and second half\n",
    "            if comp_kind == 'halves':\n",
    "                half_idx = num_splits//2\n",
    "                # print(len(halves[:half_idx][cell]))\n",
    "                current_first = np.array([halves[i][cell] for i in np.arange(half_idx)]).flatten()\n",
    "                current_second = np.array([halves[i][cell] for i in np.arange(half_idx, num_splits)]).flatten()\n",
    "            \n",
    "            # get the odd and even splits\n",
    "            elif comp_kind == 'odd_even':\n",
    "                even_idx = np.arange(0, num_splits, 2)\n",
    "                odd_idx = np.arange(1, num_splits, 2)\n",
    "                current_first = np.array([halves[i][cell] for i in even_idx]).flatten()\n",
    "                current_second = np.array([halves[i][cell] for i in odd_idx]).flatten()\n",
    "\n",
    "            else:\n",
    "                raise ValueError('Comparison kind not recognized')\n",
    "            \n",
    "            # real correlation\n",
    "            real_correlation = np.corrcoef(current_first, current_second)[1][0]\n",
    "\n",
    "            # shuffle array\n",
    "            shuffle_array = np.zeros([shuffle_number, 1])\n",
    "\n",
    "            # Used by if shuffle kind is random_bin or lag_wrap\n",
    "            time_vector = np.arange(current_second.shape[0], dtype=float) / processing_parameters.wf_frame_rate\n",
    "            bin_edges = np.arange(time_vector[0], time_vector[-1], processing_parameters.tc_lags[feature_name])\n",
    "            binned_time_idxs = np.digitize(time_vector, bin_edges)\n",
    "            unique_time_bins = np.unique(binned_time_idxs)\n",
    "\n",
    "            # calculate the confidence interval\n",
    "            for shuffle in np.arange(shuffle_number):\n",
    "                random_second = current_second.copy().flatten()\n",
    "\n",
    "                # shuffle the second half calcium activity\n",
    "                if shuffle_kind == 'random':\n",
    "                    random_second = np.random.choice(random_second, random_second.shape[0])\n",
    "\n",
    "                elif shuffle_kind == 'random_bin':\n",
    "                    # Shuffle the time while maintaining the binning. Deliberately oversample to ensure we have enough\n",
    "                    random_time_bins = np.random.choice(unique_time_bins.copy(), int(unique_time_bins.shape[0] * 1.2),\n",
    "                                                        replace=True)\n",
    "                    random_time_idxs = np.squeeze(\n",
    "                        np.concatenate([np.argwhere(binned_time_idxs == el) for el in random_time_bins]))\n",
    "\n",
    "                    # Trim the indexes to size of calcium activity and randomize the calcium activity\n",
    "                    random_time_idxs = random_time_idxs[:random_second.shape[0]]\n",
    "                    random_second = random_second[random_time_idxs]\n",
    "\n",
    "                else:\n",
    "                    raise ValueError('Shuffle kind not recognized')\n",
    "\n",
    "                shuffle_array[shuffle] = np.corrcoef(current_first, random_second)[1][0]\n",
    "            # turn nans into 0\n",
    "            shuffle_array[np.isnan(shuffle_array)] = 0\n",
    "\n",
    "            # get the confidence interval\n",
    "            conf_interval = np.percentile(shuffle_array, percentile)\n",
    "            # store the correlation and whether it passes the criterion\n",
    "            tc_half_temp[cell, 0] = real_correlation\n",
    "            tc_half_temp[cell, 1] = (real_correlation > conf_interval) & (real_correlation > 0) & \\\n",
    "                                    (conf_interval > 0)\n",
    "\n",
    "        # store for the variable\n",
    "        tc_cons[feature_name] = tc_half_temp\n",
    "    return tc_cons\n",
    "\n",
    "\n",
    "def convert_to_dataframe(half_in, full_in, counts_in, resp_in, cons_in, edges_in, date, mouse, setup):\n",
    "    \"\"\"Convert the TCs and their metrics into dataframe format\"\"\"\n",
    "    # allocate an output dict\n",
    "    out_dict = {}\n",
    "    # also one for the counts and edges\n",
    "    count_dict = {}\n",
    "    edges_dict = {}\n",
    "    # cycle through features\n",
    "    for feat in half_in.keys():\n",
    "        # get all the components\n",
    "        c_half = half_in[feat]\n",
    "        c_full = full_in[feat]\n",
    "        c_count = counts_in[feat]\n",
    "        c_resp = resp_in[feat]\n",
    "        c_cons = cons_in[feat]\n",
    "        c_edges = edges_in[feat]\n",
    "\n",
    "        # if the feature is not present, skip\n",
    "        if len(c_half) == 0:\n",
    "            continue\n",
    "        # flatten the tcs and generate labels\n",
    "        flat_half = []\n",
    "        labels_half = []\n",
    "        for half in np.arange(2):\n",
    "            flat_half.append(np.array([el.flatten() for el in c_half[half]]))\n",
    "            labels_half.append(['half_'+str(half)+'_bin_'+str(el) for el in np.arange(flat_half[half].shape[1])])\n",
    "\n",
    "        flat_full = np.array([el.flatten() for el in c_full])\n",
    "        labels_full = ['bin_'+str(el) for el in np.arange(flat_full.shape[1])]\n",
    "\n",
    "        flat_count = np.array([el.flatten() for el in c_count]).T\n",
    "        labels_count = ['count_' + str(el) for el in np.arange(flat_count.shape[1])]\n",
    "\n",
    "        flat_edges = np.array([el.flatten() for el in c_edges]).T\n",
    "        labels_edges = ['edge_' + str(el) for el in np.arange(flat_edges.shape[1])]\n",
    "        # turn everything into dataframes\n",
    "        df_half = pd.DataFrame(np.hstack(flat_half), columns=np.hstack(labels_half), dtype=np.float32)\n",
    "        df_full = pd.DataFrame(flat_full, columns=labels_full, dtype=np.float32)\n",
    "        df_resp = pd.DataFrame(c_resp, columns=['Resp_index', 'Resp_test', 'Qual_index', 'Qual_test'], dtype=np.float32)\n",
    "        df_cons = pd.DataFrame(c_cons, columns=['Cons_index', 'Cons_test'], dtype=np.float32)\n",
    "        # concatenate\n",
    "        df_concat = pd.concat((df_half, df_full, df_resp, df_cons), axis=1)\n",
    "        # generate columns for date and animal\n",
    "        df_concat['day'] = date\n",
    "        df_concat['animal'] = mouse\n",
    "        df_concat['rig'] = setup\n",
    "        # store\n",
    "        out_dict[feat] = df_concat\n",
    "        # store the counts\n",
    "        df_count = pd.DataFrame(flat_count, columns=labels_count, dtype=np.float32)\n",
    "        df_count['day'] = date\n",
    "        df_count['animal'] = mouse\n",
    "        df_count['rig'] = setup\n",
    "        count_dict[feat] = df_count\n",
    "\n",
    "        # store the edges\n",
    "        df_edges = pd.DataFrame(flat_edges, columns=labels_edges, dtype=np.float32)\n",
    "        df_edges['day'] = date\n",
    "        df_edges['animal'] = mouse\n",
    "        df_edges['rig'] = setup\n",
    "        edges_dict[feat] = df_edges\n",
    "\n",
    "    return out_dict, count_dict, edges_dict\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cbb628",
   "metadata": {},
   "source": [
    "importlib.reload(fp)\n",
    "# set up the figure theme\n",
    "fp.set_theme()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be95a2c2",
   "metadata": {},
   "source": [
    "importlib.reload(processing_parameters)\n",
    "# get the search query\n",
    "search_string = processing_parameters.search_string + ', analysis_type:preprocessing'\n",
    "parsed_search_string = fdh.parse_search_string(search_string)\n",
    "\n",
    "# get the paths from the database\n",
    "all_path = bd.query_database('analyzed_data', search_string)\n",
    "input_path = [el['analysis_path'] for el in all_path if ('_preproc' in el['slug']) and (parsed_search_string['mouse'].lower() in el['slug'])]\n",
    "# get the day, animal and rig\n",
    "day = '_'.join(all_path[0]['slug'].split('_')[0:3])\n",
    "rig = all_path[0]['rig']\n",
    "animal = all_path[0]['slug'].split('_')[3:6]\n",
    "animal = '_'.join([animal[0].upper()] + animal[1:])\n",
    "\n",
    "# assemble the output path\n",
    "out_path = os.path.join(paths.analysis_path, '_'.join((day, animal, rig, 'tcday.hdf5')))\n",
    "\n",
    "# allocate memory for the data\n",
    "raw_data = []\n",
    "# allocate memory for excluded trials\n",
    "excluded_trials = []\n",
    "# for all the files\n",
    "for files in input_path:\n",
    "    # load the data\n",
    "    with pd.HDFStore(files, mode='r') as h:\n",
    "        if ('/matched_calcium' in h.keys()):\n",
    "\n",
    "            # concatenate the latents\n",
    "            dataframe = h['matched_calcium']\n",
    "            # store\n",
    "            raw_data.append(dataframe)\n",
    "            print(input_path)\n",
    "        else:\n",
    "            excluded_trials.append(files)\n",
    "print(f'Number of files loaded: {len(raw_data)}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b635a2a",
   "metadata": {},
   "source": [
    "importlib.reload(tc)\n",
    "importlib.reload(processing_parameters)\n",
    "\n",
    "variable_list = processing_parameters.variable_list_free\n",
    "\n",
    "ds = raw_data[0]\n",
    "\n",
    "# define ca activity type\n",
    "ca_type = 'spikes'    #'spikes' or 'fluor'\n",
    "\n",
    "# drop activity not of correct type\n",
    "cols_to_drop = [el for el in ds.columns if ('cell' in el) and (ca_type not in el)]\n",
    "ds.drop(cols_to_drop, axis='columns', inplace=True)\n",
    "\n",
    "# If using fluorescence data, calulate dF/F\n",
    "if ca_type == 'fluor':\n",
    "    ds = calculate_dff(ds, baseline_type='iti', inplace=True)\n",
    "\n",
    "# define the pairs to quantify\n",
    "if rig in ['VWheel', 'VWheelWF']:\n",
    "    variable_names = processing_parameters.variable_list_fixed\n",
    "    ds['wheel_speed_abs'] = np.abs(ds['wheel_speed'])\n",
    "else:\n",
    "    variable_names = processing_parameters.variable_list_free\n",
    "\n",
    "# Convert to cm\n",
    "for col in ['wheel_speed', 'wheel_speed_abs', 'wheel_acceleration', \n",
    "            'mouse_y_m', 'mouse_z_m', 'mouse_x_m',\n",
    "            'head_height', 'mouse_speed', 'mouse_acceleration']:\n",
    "    if col in ds.columns:\n",
    "        ds[col] = ds[col] * 100.\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "# clip the calcium traces\n",
    "clipped_data = tc.clip_calcium([('', ds)])\n",
    "\n",
    "# parse the features\n",
    "features, calcium = tc.parse_features(clipped_data, variable_list, bin_number=20)\n",
    "\n",
    "# concatenate all the trials\n",
    "features = pd.concat(features)\n",
    "calcium = np.concatenate(calcium)\n",
    "\n",
    "# calculate the delta heading angle (i.e. direction) for the cricket with respect to the mouse\n",
    "# get the heading\n",
    "# prey_heading = features.loc[:, 'cricket_0_delta_heading']\n",
    "# prey_direction = np.concatenate(([0], np.diff(prey_heading)), axis=0)\n",
    "# prey_distance = features.loc[:, 'cricket_0_mouse_distance']\n",
    "# prey_loom = np.concatenate(([0], np.diff(prey_distance)), axis=0)\n",
    "# prey_visual_angle = features.loc[:, 'cricket_0_visual_angle']\n",
    "# prey_delta_visual = np.concatenate(([0], np.diff(prey_visual_angle)), axis=0)\n",
    "# # add the variables to the features\n",
    "# features.loc[:, 'cricket_0_direction'] = prey_direction\n",
    "# features.loc[:, 'cricket_0_loom'] = prey_loom\n",
    "# features.loc[:, 'cricket_0_delta_visual'] = prey_delta_visual\n",
    "\n",
    "# define the variable to quantify\n",
    "# variable_pairs = ['mouse_angular_speed']\n",
    "# print(raw_data[0][1].keys()[:30])\n",
    "variable_pairs = processing_parameters.variable_list_free\n",
    "# variable_pairs = ['mouse_x']\n",
    "# variable_pairs = ['latent_0']\n",
    "\n",
    "# get the number of cells\n",
    "cell_num = calcium.shape[1]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6636c43",
   "metadata": {},
   "source": [
    "# plot the calcium\n",
    "print(calcium.shape)\n",
    "hv.Raster(calcium.T).opts(width=1000, height=600, tools=['hover'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9799156d",
   "metadata": {},
   "source": [
    "# %%time\n",
    "importlib.reload(tc)\n",
    "importlib.reload(processing_parameters)\n",
    "\n",
    "# define the bparameters from processing parameters\n",
    "bin_number = processing_parameters.bin_number\n",
    "shuffle_kind = processing_parameters.tc_shuffle_kind\n",
    "resp_qual_percentile = processing_parameters.tc_resp_qual_cutoff\n",
    "\n",
    "# get the TCs and their responsivity\n",
    "tcs_half, tcs_full, tcs_resp, tc_count, tc_bins = extract_tcs_responsivity(features, calcium, variable_names, cell_num, \n",
    "                                                                            percentile=resp_qual_percentile, bin_number=bin_number, shuffle_kind=shuffle_kind)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76044c38",
   "metadata": {},
   "source": [
    "# get the TC consistency\n",
    "consistency_percentile = processing_parameters.tc_consistency_cutoff\n",
    "consistency_comp = processing_parameters.tc_consistency_comp\n",
    "tcs_cons = extract_consistency(tcs_half, variable_names, cell_num, shuffle_kind=shuffle_kind, comp_kind=consistency_comp, percentile=consistency_percentile)\n",
    "\n",
    "# convert the outputs into a dataframe\n",
    "tcs_dict, tcs_counts_dict, tcs_bins_dict = tc.convert_to_dataframe(tcs_half, tcs_full, tc_count, tcs_resp, tcs_cons, tc_bins, day, animal, rig)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4807362",
   "metadata": {},
   "source": [
    "# Plot the criteria distributions\n",
    "plot_list = []\n",
    "map_dict = {}\n",
    "\n",
    "pair_number = len(variable_pairs)\n",
    "# print(tcs_cons)\n",
    "# for all features\n",
    "for pair_idx in np.arange(pair_number):\n",
    "     # get the name\n",
    "#     feature_name = feature_raw_trials[0].columns[feature]\n",
    "    feature_name = list(tcs_cons.keys())[pair_idx]\n",
    "#     feature_name = \n",
    "    # collect data across trials\n",
    "    across_cons = tcs_cons[feature_name]\n",
    "    across_resp = tcs_resp[feature_name]\n",
    "    # remove the inf\n",
    "    across_cons[np.isinf(across_cons)] = np.nan\n",
    "    across_resp[np.isinf(across_resp)] = np.nan\n",
    "    \n",
    "    # remove outliers (NEED TO FIX THIS IN PREPROCESSING)\n",
    "#     across_cons[across_cons>1] = np.nan\n",
    "#     across_resp[across_resp>6] = np.nan\n",
    "    \n",
    "#     # also generate maps to identify the trial and cell\n",
    "#     trial_map = np.hstack([np.ones([el[feature_name].shape[0]])*idx \n",
    "#                            for idx, el in enumerate(tcs_cons)]).T.astype(int)\n",
    "#     cell_map = np.hstack([np.arange(el[feature_name].shape[0]) \n",
    "#                           for el in tcs_cons]).T.astype(int)\n",
    "    \n",
    "    # plot\n",
    "    cons_pass = (across_cons[:, 1]==1) & (across_resp[:, 1]==0)\n",
    "    resp_pass = (across_resp[:, 1]==1) & (across_cons[:, 1]==0)\n",
    "    both_pass = (across_cons[:, 1]==1) & (across_resp[:, 1]==1)\n",
    "    none_pass = (across_cons[:, 1]==0) & (across_resp[:, 1]==0)\n",
    "    both_plot = hv.Scatter((across_cons[both_pass, 0], across_resp[both_pass, 0]), \n",
    "                      kdims=['Consistency'], vdims=['Responsivity'], label='Both')\n",
    "    both_plot.opts(title=feature_name, width=600, height=600)\n",
    "    none_plot = hv.Scatter((across_cons[none_pass, 0], across_resp[none_pass, 0]), \n",
    "                      kdims=['Consistency'], vdims=['Responsivity'], label='None')\n",
    "    cons_plot = hv.Scatter((across_cons[cons_pass, 0], across_resp[cons_pass, 0]), \n",
    "                      kdims=['Consistency'], vdims=['Responsivity'], label='Cons')\n",
    "    resp_plot = hv.Scatter((across_cons[resp_pass, 0], across_resp[resp_pass, 0]), \n",
    "                      kdims=['Consistency'], vdims=['Responsivity'], label='Resp')\n",
    "    plot_list.append(both_plot*none_plot*cons_plot*resp_plot)\n",
    "#     plot_list.append(both_plot)\n",
    "    \n",
    "    # store the maps and vector for plotting later\n",
    "#     map_dict[feature_name] = [trial_map, cell_map, both_pass]\n",
    "    print(f'Number of cells passing the thresholds for {feature_name}: {np.sum(both_pass)}')\n",
    "print(f'Total number of cells: {both_pass.shape[0]}')\n",
    "hv.Layout(plot_list).opts(opts.Scatter(size=10))\n",
    "# print(across_cons)\n",
    "# print(across_resp)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f66667",
   "metadata": {},
   "source": [
    "# plot cells passing the criterion\n",
    "\n",
    "# define the target feature\n",
    "target_feature = variable_pairs[1]\n",
    "# get the indexes of the target cells\n",
    "cell_idx = np.argwhere(both_pass).flatten()\n",
    "# allocate memory for the plots\n",
    "cell_plots = []\n",
    "\n",
    "# for all the cells\n",
    "for target_cell in cell_idx:\n",
    "    # get the bins\n",
    "    current_bins = processing_parameters.tc_params[target_feature]\n",
    "    bins0 = np.linspace(current_bins[0], current_bins[1], bin_number)\n",
    "    # get the tuning curve\n",
    "    current_tc = tcs_full[target_feature][target_cell]\n",
    "    current_half0 = tcs_half[target_feature][0][target_cell]\n",
    "    current_half1 = tcs_half[target_feature][1][target_cell]\n",
    "#     print(tc)\n",
    "    \n",
    "    # plot\n",
    "    plot = hv.Curve((bins0, current_tc), kdims=target_feature, vdims='Activity (a.u.)', label='Full')\n",
    "    halfplot0 = hv.Curve((bins0, current_half0), kdims=target_feature, vdims='Activity (a.u.)', label='half 0')\n",
    "    halfplot1 = hv.Curve((bins0, current_half1), kdims=target_feature, vdims='Activity (a.u.)', label='half 1')\n",
    "    # store\n",
    "    cell_plots.append(plot*halfplot0*halfplot1)\n",
    "# plot the layout\n",
    "hv.Layout(cell_plots).opts(shared_axes=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9249f59",
   "metadata": {},
   "source": [
    "# plot the occupancy TCs\n",
    "\n",
    "plot_list = []\n",
    "\n",
    "cmap='Spectral'\n",
    "\n",
    "pair_number = len(variable_pairs)\n",
    "# print(tcs_cons)\n",
    "# for all features\n",
    "for pair_idx in np.arange(pair_number):\n",
    "    current_bins = processing_parameters.tc_params[variable_pairs[pair_idx]]\n",
    "\n",
    "\n",
    "    bins0 = np.linspace(current_bins[0], current_bins[1], bin_number)\n",
    "#     bins1 = np.linspace(current_bins[1][0], current_bins[1][1], 10)\n",
    "    \n",
    "#     var_names = variable_pairs[pair_idx].split('__')[::-1]\n",
    "    var_names = variable_pairs[pair_idx]\n",
    "\n",
    "#     full_map = hv.Image((bins1, bins0, 1-tcs_counts[variable_pairs[pair_idx]]), kdims=var_names)\n",
    "    full_map = hv.Curve((bins0, tc_count[var_names]), kdims=[var_names], vdims='Activity (a.u.)')\n",
    "#     full_map.opts(cmap=cmap, colorbar=True)\n",
    "    #     full_map.opts(shared_axes=False, xrotation=45, cmap=cmap)\n",
    "#     full_map = fp.format_figure(full_map, frame_width=400, frame_height=400)\n",
    "#     full_map.opts(tools=['hover'], fontsize=14, colorbar_opts={'major_label_text_font_size':'40pt'})\n",
    "#     full_map.opts()\n",
    "    plot_list.append(full_map)\n",
    "\n",
    "hv.Layout(plot_list).cols(5)\n",
    "    "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa1e20d",
   "metadata": {},
   "source": [
    "target_cell = 21\n",
    "pair_number = len(variable_pairs)\n",
    "cmap = 'Purples'\n",
    "plot_list = []\n",
    "# print(tcs_cons)\n",
    "# for all features\n",
    "for pair_idx in np.arange(pair_number):\n",
    "    current_bins = processing_parameters.tc_params[variable_pairs[pair_idx]]\n",
    "\n",
    "\n",
    "    bins0 = np.linspace(current_bins[0], current_bins[1], bin_number)\n",
    "#     bins1 = np.linspace(current_bins[1][0], current_bins[1][1], 10)\n",
    "    \n",
    "#     var_names = variable_pairs[pair_idx].split('__')[::-1]\n",
    "    var_names = variable_pairs[pair_idx]\n",
    "    full_map = hv.Curve((bins0, tcs_full[var_names][target_cell]), kdims=[var_names], vdims='Activity (a.u.)')\n",
    "#     full_map = hv.Image((bins1, bins0, tcs_full[variable_pairs[pair_idx]][target_cell]), kdims=var_names)\n",
    "#     full_map.opts(cmap=cmap, colorbar=True)\n",
    "    #     full_map.opts(shared_axes=False, xrotation=45, cmap=cmap)\n",
    "#     full_map = fp.format_figure(full_map, frame_width=400, frame_height=400)\n",
    "#     full_map.opts(tools=['hover'], fontsize=14, colorbar_opts={'major_label_text_font_size':'40pt'})\n",
    "#     full_map.opts()\n",
    "    plot_list.append(full_map)\n",
    "\n",
    "hv.Layout(plot_list).cols(5)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201c75df",
   "metadata": {},
   "source": [
    "# Also plot it's calcium activity\n",
    "calcium_plot = hv.Curve((np.arange(calcium.shape[0]), calcium[:, target_cell])).opts(width=800, xrotation=45)\n",
    "calcium_plot"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df663e7",
   "metadata": {},
   "source": [
    "plot_list = []\n",
    "for feature in features.columns:\n",
    "    trace = features[feature].to_numpy()\n",
    "    traces = np.array_split(trace, 2)\n",
    "    plot = hv.Overlay([hv.Curve(traces[0]), hv.Curve(traces[1])]).opts(title=feature, xrotation=45)\n",
    "    plot_list.append(plot)\n",
    "hv.Layout(plot_list).cols(5).opts(shared_axes=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94815734",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prey_capture_old",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
