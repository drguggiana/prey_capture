{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster and visualize the aggregated results in high D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# imports\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath(r'D:\\Code Repos\\prey_capture'))\n",
    "\n",
    "import panel as pn\n",
    "import holoviews as hv\n",
    "from holoviews import opts, dim\n",
    "hv.extension('bokeh')\n",
    "from bokeh.resources import INLINE\n",
    "\n",
    "import paths\n",
    "import functions_bondjango as bd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.mixture as mix\n",
    "import sklearn.decomposition as decomp\n",
    "import functions_plotting as fp\n",
    "import functions_data_handling as fd\n",
    "import functions_plotting as fp\n",
    "import umap\n",
    "# define the name to be used for the saved figures\n",
    "save_name = 'clusters_mouse'\n",
    "line_width = 5"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# load the data\n",
    "# get the data paths\n",
    "try:\n",
    "    data_path = snakemake.input[0]\n",
    "except NameError:\n",
    "    # define the search string\n",
    "    search_string = 'result:succ,lighting:normal,rig:miniscope,analysis_type:aggEnc'\n",
    "    # query the database for data to plot\n",
    "    data_all = bd.query_database('analyzed_data', search_string)\n",
    "    data_path = data_all[0]['analysis_path']\n",
    "print(data_path)\n",
    "\n",
    "# load the data\n",
    "data = fd.aggregate_loader(data_path)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# define the target parameter and PCA\n",
    "# target_parameter = 'mouse_cricket_distance'\n",
    "target_parameter = 'cricket_0_mouse_distance'\n",
    "\n",
    "# assemble the array with the parameters of choice\n",
    "if 'aggEnc' in search_string:\n",
    "    target_data = data.loc[:, [target_parameter] + ['event_id', 'trial_id']].groupby(\n",
    "        ['trial_id', 'event_id']).agg(list).to_numpy()\n",
    "else:\n",
    "    target_data = data.loc[:, [target_parameter] + ['trial_id']].groupby(\n",
    "        ['trial_id']).agg(list).to_numpy()\n",
    "# # HACK REMOVE\n",
    "# target_data = np.array([el for sublist in target_data for el in sublist if len(el) == 594])\n",
    "target_data = np.array([el for sublist in target_data for el in sublist])\n",
    "\n",
    "# print(len(target_data))\n",
    "# [print(len(el[0])) for el in target_data]\n",
    "# PCA the data before clustering\n",
    "pca = decomp.PCA()\n",
    "transformed_data = pca.fit_transform(target_data)\n",
    "print(transformed_data.shape)\n",
    "# fp.plot_2d([[pca.explained_variance_ratio_]])\n",
    "\n",
    "hv.Curve(np.cumsum(pca.explained_variance_ratio_)).opts(tools=['hover'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Cluster the data\n",
    "\n",
    "# define the vector of components\n",
    "component_vector = [2, 3, 4, 5, 10, 20, 30]\n",
    "# allocate memory for the results\n",
    "gmms = []\n",
    "# for all the component numbers\n",
    "for comp in component_vector:\n",
    "    # # define the number of components\n",
    "    # n_components = 10\n",
    "    gmm = mix.GaussianMixture(n_components=comp, covariance_type='diag', n_init=50)\n",
    "    gmm.fit(transformed_data[:, :7])\n",
    "    gmms.append(gmm.bic(transformed_data[:, :7]))\n",
    "\n",
    "# select the minimum bic number of components\n",
    "n_components = np.array(component_vector)[np.argmin(gmms)]\n",
    "# predict the cluster indexes\n",
    "gmm = mix.GaussianMixture(n_components=n_components, covariance_type='diag', n_init=50)\n",
    "cluster_idx = gmm.fit_predict(transformed_data[:, :7])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# discard singletons\n",
    "# turn cluster_idx in a float\n",
    "cluster_idx = cluster_idx.astype(float)\n",
    "# get the IDs\n",
    "clu_unique = np.unique(cluster_idx)\n",
    "for clu in clu_unique:\n",
    "    # get the number of traces in the cluster\n",
    "    number_traces = sum(cluster_idx==clu)\n",
    "    # if it's less than 5, eliminate the cluster\n",
    "    if number_traces < 5:\n",
    "        cluster_idx[cluster_idx==clu] = np.nan\n",
    "    \n",
    "# plot the BIC\n",
    "hv.Curve((component_vector, gmms))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# plot the clusters\n",
    "# add the cluster indexes to the dataframe\n",
    "cluster_data = np.array([np.mean(target_data[cluster_idx == el, :], axis=0) for el in np.arange(n_components)])\n",
    "cluster_std = np.array([np.std(target_data[cluster_idx == el, :], axis=0)/np.sqrt(np.sum(cluster_idx == el))\n",
    "                        for el in np.arange(n_components)])\n",
    "# plot the results\n",
    "# fp.plot_2d([cluster_data])\n",
    "cluster_plot = hv.Overlay([hv.Curve(el, label=str(idx), kdims=['Time (s)'], \n",
    "                                    vdims=[target_parameter.replace('_', ' ')+' (px)'])\n",
    "                           for idx, el in enumerate(cluster_data)] + \n",
    "                            [hv.Spread((np.arange(el.shape[0]),el,cluster_std[idx, :])) \n",
    "                             for idx, el in enumerate(cluster_data)])\n",
    "\n",
    "cluster_plot.relabel('Clusters').opts({'Curve': dict(color=hv.Palette('Category20')), \n",
    "                                         'Spread': dict(color=hv.Palette('Category20'))})\n",
    "\n",
    "cluster_plot.opts(opts.Curve(width=fp.pix(10.7), height=fp.pix(5), toolbar=None, \n",
    "                        hooks=[fp.margin], fontsize=fp.font_sizes['small'], line_width=12, xticks=3, yticks=3),\n",
    "            opts.Overlay(legend_position='right', text_font='Arial'))\n",
    "\n",
    "# assemble the save path\n",
    "save_path = os.path.join(paths.figures_path,save_name+'_cluster.png')\n",
    "hv.save(cluster_plot,save_path)\n",
    "\n",
    "cluster_plot\n",
    "# print(cluster_plot)\n",
    "\n",
    "# hv.Curve(cluster_data)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "hv.Image(target_data[cluster_idx == 6, :], kdims=['Time','Encounters'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# plot the clusters as an image\n",
    "\n",
    "# define the target parameter\n",
    "# target = 'mouse_cricket_distance'\n",
    "target = 'vrcricket_0_mouse_distance'\n",
    "\n",
    "# # load the parameter\n",
    "# parameter = data[[target,'trial_id']].copy()\n",
    "# group the single traces\n",
    "grouped_parameter = data.loc[:, [target] + ['event_id', 'trial_id']].groupby(\n",
    "    ['trial_id', 'event_id']).agg(list).to_numpy()\n",
    "\n",
    "# print(grouped_parameter)\n",
    "# grouped_parameter = np.array([el for el in grouped_parameter[target]])\n",
    "grouped_parameter = np.array([el for sublist in grouped_parameter for el in sublist if len(el) == 594])\n",
    "\n",
    "\n",
    "# plot all traces\n",
    "\n",
    "[sorted_traces,_,_] = fp.sort_traces(grouped_parameter)\n",
    "\n",
    "image = hv.Image(sorted_traces, ['Time','Trial #'], \n",
    "                 [target.replace('_', ' ')], bounds=[0, 0, grouped_parameter.shape[0], grouped_parameter.shape[1]])\n",
    "image.opts(width=fp.pix(5.8), height=fp.pix(5.8), toolbar=None, \n",
    "                    hooks=[fp.margin], fontsize=fp.font_sizes['small'], \n",
    "                    xticks=3, yticks=3,\n",
    "                    colorbar=True, cmap='viridis', \n",
    "           colorbar_opts={'major_label_text_align': 'left'})\n",
    "\n",
    "\n",
    "# assemble the save path\n",
    "save_path = os.path.join(paths.figures_path,save_name+'_'+target+'.png')\n",
    "hv.save(image,save_path)\n",
    "\n",
    "\n",
    "image"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# UMAP\n",
    "\n",
    "# embed the data via UMAP\n",
    "reducer = umap.UMAP(min_dist=0.5, n_neighbors=10)\n",
    "embedded_data = reducer.fit_transform(transformed_data)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Plot the embedding\n",
    "\n",
    "# use the cluster indexes\n",
    "umap_data = np.concatenate((embedded_data,np.expand_dims(cluster_idx, axis=1)),axis=1)\n",
    "\n",
    "# # use the trial ID\n",
    "# # group the single traces\n",
    "# grouped_parameter = data.loc[:, ['event_id', 'trial_id']].groupby(\n",
    "#     ['trial_id']).agg(list)\n",
    "# temp_parameter = []\n",
    "# counter = 0\n",
    "# for idx, el in enumerate(grouped_parameter['event_id']):\n",
    "#     # get the event ids\n",
    "#     event_ids = np.unique(el)\n",
    "#     temp_parameter.append(idx*np.ones(event_ids.shape[0]))\n",
    "\n",
    "# grouped_parameter = np.concatenate(temp_parameter, axis=0)\n",
    "# umap_data = np.concatenate((embedded_data,np.expand_dims(grouped_parameter, axis=1)),axis=1)\n",
    "\n",
    "# highlight the last encounter of every group\n",
    "# allocate a list for that \n",
    "winner_list = []\n",
    "grouped_parameter = data.loc[:, ['event_id', 'trial_id']].groupby(\n",
    "    ['trial_id']).agg(list)\n",
    "\n",
    "# for all the trials\n",
    "for idx, el in enumerate(grouped_parameter['event_id']):\n",
    "    # get the event ids\n",
    "    encounter_list = np.zeros(np.unique(el).shape[0])\n",
    "    encounter_list[-1] = 1\n",
    "    winner_list.append(encounter_list)\n",
    "\n",
    "grouped_parameter = np.concatenate(winner_list, axis=0)\n",
    "\n",
    "\n",
    "umap_plot = hv.Scatter(umap_data, vdims=['Dim 2','cluster'], kdims=['Dim 1'])\n",
    "umap_plot.opts(color='cluster', colorbar=True, cmap='Category10', size=20)\n",
    "\n",
    "umap_plot.opts(opts.Scatter(width=fp.pix(5.7), height=fp.pix(7.8), toolbar=None, \n",
    "                        hooks=[fp.margin], fontsize=fp.font_sizes['small'], xticks=3, yticks=3))\n",
    "#             opts.Overlay(legend_position='right', text_font='Arial'))\n",
    "\n",
    "# winner_data = embedded_data[grouped_parameter==1]\n",
    "\n",
    "# winner_plot = hv.Scatter(winner_data, vdims=['Dim 2'], kdims=['Dim 1'])\n",
    "# winner_plot.opts(width=fp.pix(5.7), height=fp.pix(7.8), toolbar=None, \n",
    "#                         hooks=[fp.margin], fontsize=fp.font_sizes['small'], xticks=3, yticks=3, color='black', size=20)\n",
    "# umap_overlay = umap_plot*winner_plot\n",
    "\n",
    "# assemble the save path\n",
    "save_path = os.path.join(paths.figures_path,save_name+'_umap.png')\n",
    "hv.save(umap_plot,save_path)\n",
    "\n",
    "umap_plot\n"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
