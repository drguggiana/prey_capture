{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import importlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import holoviews as hv\n",
    "hv.extension('bokeh')\n",
    "\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(r'C:/Users/mmccann/repos/bonhoeffer/prey_capture/'))\n",
    "import paths\n",
    "import processing_parameters\n",
    "import functions_bondjango as bd\n",
    "import functions_misc as fmisc\n",
    "import functions_matching as fm\n",
    "import functions_data_handling as fdh\n",
    "import functions_tuning as tuning\n",
    "import functions_plotting as fp\n",
    "import functions_kinematic as fk\n",
    "import functions_loaders as fl\n",
    "from wirefree_experiment import WirefreeExperiment, DataContainer\n",
    "from functions_wirefree_trigger_fix import get_trial_duration_stats, drop_partial_or_long_trials\n",
    "\n",
    "# fig_path = paths.wf_figures_path\n",
    "fig_path = r\"D:\\thesis\\figures\""
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Kick out ones that are oin the exps_to_drop.yaml file\n",
    "with open(paths.file_exclusion_path, 'r') as f:\n",
    "    # Load the contents of the file into a dictionary\n",
    "    files_to_exclude = yaml.unsafe_load(f)\n",
    "    exclude_from_this = files_to_exclude.get('all', [])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make histograms of running speeds across all experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "frame_rate = processing_parameters.wf_frame_rate\n",
    "\n",
    "all_paths, all_queries = fl.query_search_list()\n",
    "mice = ['_'.join(os.path.basename(path).split('_')[7:10]) for path in all_paths[0]]\n",
    "print(all_paths)\n",
    "\n",
    "# load the data\n",
    "data_list = []\n",
    "path_list = []\n",
    "for path, queries in zip(all_paths, all_queries):\n",
    "    \n",
    "        data, _, metadata  = fl.load_preprocessing(path, queries, latents_flag=False)\n",
    "        data_list.append(data)\n",
    "        path_list.append(path)\n",
    "\n",
    "# data_list = [ds for el in data_list for ds in el]"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all preprocessing files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "lower_lim = 0.1\n",
    "bin = 0.2  # seconds"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# freely moving\n",
    "running_data = []\n",
    "# free_data = data_list[0] + data_list[1]\n",
    "for data in data_list[0]:\n",
    "    binned_run, _, _  = sp.stats.binned_statistic(data['time_vector'], data['mouse_speed'], 'mean', bins=np.arange(0, len(data['time_vector']), bin))\n",
    "    running_data.append(binned_run * 100)\n",
    "\n",
    "running_data = np.concatenate(running_data, axis=0)\n",
    "\n",
    "# create a histogram with log spaced bins\n",
    "free_zeros = np.sum(running_data < lower_lim)\n",
    "print(free_zeros)\n",
    "counts, edges = np.histogram(running_data[running_data >= lower_lim], bins=np.logspace(np.log10(lower_lim),np.log10(100), 50))\n",
    "free_zero = hv.Bars(('0', free_zeros)).opts(width=100, height=500, xlabel=None, ylabel=None, color=fp.hv_blue_hex)\n",
    "# counts = np.concatenate(([free_zeros], counts))\n",
    "# edges = np.concatenate(([0.001, 0.0015], edges))\n",
    "free_hist = hv.Histogram((edges, counts)).opts(xlabel=None, ylabel=None, logx=True, logy=False).opts(width=500, height=500, color=fp.hv_blue_hex)\n",
    "\n",
    "save_path = os.path.join(fig_path, 'running_speed', 'running_speed_hist_free_zero.png')\n",
    "free_zero = fp.save_figure(free_zero, save_path=save_path, fig_width=1, dpi=800, fontsize='paper', target='screen', display_factor=0.2)\n",
    "\n",
    "save_path = os.path.join(fig_path, 'running_speed', 'running_speed_hist_free.png')\n",
    "free_hist = fp.save_figure(free_hist, save_path=save_path, fig_width=5, dpi=800, fontsize='paper', target='save', display_factor=0.2)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# head fixed\n",
    "wheel_data = []\n",
    "# fixed_data = data_list[2] + data_list[3]\n",
    "for data in data_list[0]:\n",
    "    data['wheel_speed_abs'] = np.abs(data['wheel_speed'])\n",
    "    binned_wheel, _, _  = sp.stats.binned_statistic(data['time_vector'], data['wheel_speed_abs'], 'mean', bins=np.arange(0, len(data['time_vector']), bin))\n",
    "    wheel_data.append(binned_wheel)\n",
    "\n",
    "wheel_data = np.concatenate(wheel_data, axis=0)\n",
    "\n",
    "# create a histogram with log spaced bins\n",
    "wheel_zeros = np.sum(wheel_data < lower_lim)\n",
    "print(wheel_zeros)\n",
    "counts, edges = np.histogram(wheel_data[wheel_data >= lower_lim], bins=np.logspace(np.log10(lower_lim),np.log10(100), 50))\n",
    "# counts = np.concatenate(([wheel_zeros], counts))\n",
    "# edges = np.concatenate(([0.001, 0.0015], edges))\n",
    "fixed_zero = hv.Bars(('0', wheel_zeros)).opts(width=100, height=500, xlabel=None, ylabel=None, color='blue')\n",
    "fixed_hist = hv.Histogram((edges, counts)).opts(xlabel=None, ylabel=None, logx=True, logy=False).opts(width=500, height=500, color='blue')\n",
    "\n",
    "save_path = os.path.join(fig_path, '20_pupil_diam_wheel_speed', 'session2_running_speed_hist_fixed_zero.png')\n",
    "fixed_zero = fp.save_figure(fixed_zero, save_path=save_path, fig_width=1, dpi=800, fontsize='paper', target='screen', display_factor=0.2)\n",
    "\n",
    "save_path = os.path.join(fig_path, '20_pupil_diam_wheel_speed', 'session2_running_speed_hist_fixed.png')\n",
    "fixed_hist = fp.save_figure(fixed_hist, save_path=save_path, fig_width=5, dpi=800, fontsize='paper', target='save', display_factor=0.2)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# head fixed\n",
    "wheel_data_dists = []\n",
    "wheel_data = []\n",
    "hist_plot_list = []\n",
    "\n",
    "for data in data_list[0]:\n",
    "    data['pupil_diameter'] = np.abs(data['pupil_diameter'])\n",
    "    binned_wheel, _, _  = sp.stats.binned_statistic(data['time_vector'], data['pupil_diameter'], 'mean', bins=np.arange(0, len(data['time_vector']), bin))\n",
    "    wheel_data.append(binned_wheel)\n",
    "\n",
    "wheel_data = np.concatenate(wheel_data, axis=0)\n",
    "wheel_data_dists.append(np.nan_to_num(wheel_data))\n",
    "print(np.nanmean(wheel_data))\n",
    "\n",
    "# create a histogram with log spaced bins\n",
    "wheel_zeros = np.sum(wheel_data < lower_lim)\n",
    "print(wheel_zeros)\n",
    "counts, edges = np.histogram(wheel_data[wheel_data >= lower_lim], bins=np.linspace(10, 300, 50))\n",
    "fixed_zero = hv.Bars(('0', wheel_zeros)).opts(width=100, height=500, xlabel=None, ylabel=None, color='red')\n",
    "fixed_hist = hv.Histogram((edges, counts)).opts(xlabel=None, ylabel=None, logx=False, logy=False).opts(width=500, height=500, color='red')\n",
    "\n",
    "# # save_path = os.path.join(fig_path, '20_pupil_diam_wheel_speed', 'session2_running_speed_hist_fixed_zero.png')\n",
    "# fixed_zero = fp.save_figure(fixed_zero, save_path=save_path, fig_width=1, dpi=800, fontsize='paper', target='screen', display_factor=0.2)\n",
    "\n",
    "# # save_path = os.path.join(fig_path, '20_pupil_diam_wheel_speed', 'session2_running_speed_hist_fixed.png')\n",
    "fixed_hist_0 = fp.save_figure(fixed_hist, save_path=save_path, fig_width=5, dpi=800, fontsize='paper', target='screen', display_factor=0.2)\n",
    "hist_plot_list.append(fixed_hist_0)\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# head fixed\n",
    "wheel_data = []\n",
    "\n",
    "for data in data_list[1]:\n",
    "    data['pupil_diameter'] = np.abs(data['pupil_diameter'])\n",
    "    binned_wheel, _, _  = sp.stats.binned_statistic(data['time_vector'], data['pupil_diameter'], 'mean', bins=np.arange(0, len(data['time_vector']), bin))\n",
    "    wheel_data.append(binned_wheel)\n",
    "\n",
    "wheel_data = np.concatenate(wheel_data, axis=0)\n",
    "wheel_data_dists.append(np.nan_to_num(wheel_data))\n",
    "print(np.nanmean(wheel_data))\n",
    "\n",
    "# create a histogram with log spaced bins\n",
    "wheel_zeros = np.sum(wheel_data < lower_lim)\n",
    "print(wheel_zeros)\n",
    "counts, edges = np.histogram(wheel_data[wheel_data >= lower_lim], bins=np.linspace(10, 300, 50))\n",
    "fixed_zero = hv.Bars(('0', wheel_zeros)).opts(width=100, height=500, xlabel=None, ylabel=None, color='blue')\n",
    "fixed_hist = hv.Histogram((edges, counts)).opts(xlabel=None, ylabel=None, logx=False, logy=False).opts(width=500, height=500, color='blue', alpha=0.5)\n",
    "\n",
    "# # save_path = os.path.join(fig_path, '20_pupil_diam_wheel_speed', 'session2_running_speed_hist_fixed_zero.png')\n",
    "# fixed_zero = fp.save_figure(fixed_zero, save_path=save_path, fig_width=1, dpi=800, fontsize='paper', target='screen', display_factor=0.2)\n",
    "\n",
    "# # save_path = os.path.join(fig_path, '20_pupil_diam_wheel_speed', 'session2_running_speed_hist_fixed.png')\n",
    "fixed_hist_0 = fp.save_figure(fixed_hist, save_path=save_path, fig_width=5, dpi=800, fontsize='paper', target='screen', display_factor=0.2)\n",
    "hist_plot_list.append(fixed_hist_0)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "hist_overlay = hv.Overlay(hist_plot_list).opts(width=500, height=500, xlabel='', ylabel='')\n",
    "save_path = os.path.join(fig_path, '20_pupil_diam_wheel_speed', 'overlay_pupil_diameter_hist_fixed.png')\n",
    "hist_overlay = fp.save_figure(hist_overlay, save_path=save_path, fig_width=5, dpi=800, fontsize='paper', target='both', display_factor=0.2)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from scipy.stats import kruskal"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "kruskal(*wheel_data_dists)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "both_hists = hv.Overlay([free_hist, fixed_hist]).opts(hv.opts.Histogram(alpha=0.7))\n",
    "save_path = os.path.join(fig_path, 'running_speed','overlay_histograms.png')\n",
    "both_hists = fp.save_figure(both_hists, save_path=save_path, fig_width=5, dpi=800, fontsize='paper', target='both', display_factor=0.2)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "both_hists_wo_labels = hv.Overlay([free_hist, fixed_hist]).opts(hv.opts.Histogram(alpha=0.7, xaxis=None, yaxis=None))\n",
    "save_path = os.path.join(fig_path, 'running_speed','overlay_histograms_wo_labels.png')\n",
    "both_hists_wo_labels = fp.save_figure(both_hists, save_path=save_path, fig_width=5, dpi=800, fontsize='paper', target='both', display_factor=0.2)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check one particular experiment to prototype running onset tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "importlib.reload(processing_parameters)\n",
    "\n",
    "# get the search string\n",
    "search_string = 'mouse:MM_221109_a, slug:01_11_2023,'\n",
    "parsed_search = fdh.parse_search_string(search_string)\n",
    "\n",
    "# get the paths from the database\n",
    "\n",
    "# get the raw experiment\n",
    "exp_query = bd.query_database('vr_experiment', search_string)\n",
    "exp_query.sort(key=lambda x: x['rig'])\n",
    "\n",
    "# Get the preprocessing file\n",
    "preproc_query = bd.query_database('analyzed_data', search_string + r', analysis_type:preprocessing')\n",
    "preproc_query = [q for q in preproc_query if parsed_search['mouse'].lower() in q['slug']]\n",
    "preproc_query.sort(key=lambda x: x['rig'])\n",
    "preproc_paths = np.sort(np.array([el['analysis_path'] for el in preproc_query if (el['analysis_type'] == 'preprocessing') and\n",
    "                        (parsed_search['mouse'].lower() in el['slug'])]))\n",
    "\n",
    "# Get the tuning curve file\n",
    "tc_query = bd.query_database('analyzed_data', search_string + r', analysis_type:tc_analysis')\n",
    "tc_query = [q for q in tc_query if parsed_search['mouse'].lower() in q['slug']]\n",
    "tc_query.sort(key=lambda x: x['rig'])\n",
    "tc_paths = np.sort(np.array([el['analysis_path'] for el in tc_query if (el['analysis_type'] == 'tc_analysis') and\n",
    "                    (parsed_search['mouse'].lower() in el['slug'])]))\n",
    "\n",
    "# Get the cell matching file\n",
    "cell_match_query = bd.query_database('analyzed_data', search_string + r', analysis_type:cellmatching')\n",
    "cell_match_query = [q for q in cell_match_query if parsed_search['mouse'].lower() in q['slug']]\n",
    "cell_match_query.sort(key=lambda x: x['rig'])\n",
    "cell_matching_path = np.array([el['analysis_path'] for el in cell_match_query if (el['analysis_type'] == 'cellmatching') and\n",
    "                                (parsed_search['mouse'].lower() in el['slug']) and ('daycellmatch' in el['slug'])])\n",
    "\n",
    "# Get the calcium file names\n",
    "calcium_paths = np.array([p.replace('preproc', 'calciumraw') for p in preproc_paths])\n",
    "\n",
    "# Parse the rigs\n",
    "rigs = np.array([os.path.basename(file).split('_')[6] for file in calcium_paths])\n",
    "\n",
    "print(cell_matching_path)\n",
    "print(calcium_paths)\n",
    "print(preproc_paths)\n",
    "print(tc_paths)\n",
    "print(rigs)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load the matching assignments and find the column that corresponds to each file\n",
    "assignments =  fm.match_cells(cell_matching_path[0])\n",
    "new_cols = [col.split('_')[-2] for col in assignments.columns]\n",
    "assignments.columns = new_cols\n",
    "match_cols = [i for i, (rig, col) in enumerate(zip(rigs, assignments.columns)) if str(col) in rig]\n",
    "\n",
    "# Use number of non-NaNs in each row to filter out components that were not registered in enough sessions\n",
    "assignments_filtered = assignments.dropna().astype(int).to_numpy()\n",
    "unassigned = np.array(assignments[np.sum(~np.isnan(assignments), axis=1) < 2])\n",
    "unassigned = [unassigned[~np.isnan(unassigned[:, 0]), 0].astype(int), unassigned[~np.isnan(unassigned[:, 1]), 1].astype(int)]\n",
    "unassigned = [np.sort(np.unique(unassigned[0])), np.sort(np.unique(unassigned[1]))]\n",
    "\n",
    "# Specify the path to the curated cell matches file\n",
    "curated_cell_matches_path = os.path.join(r\"C:\\Users\\mmccann\\Desktop\\curated_cell_matches_multi_normal_ALL.xlsx\")\n",
    "\n",
    "try:\n",
    "    # Read all sheets into a list of dataframes\n",
    "    curated_matches_dict = pd.read_excel(curated_cell_matches_path, sheet_name=None)\n",
    "\n",
    "    # Concatenate the dataframes into a single dataframe\n",
    "    curated_matches = pd.concat(curated_matches_dict.values(), ignore_index=True)\n",
    "\n",
    "    # Get the hand-picked matches for the current experiment\n",
    "    day_mouse_curated_idxs = curated_matches[(curated_matches['mouse'] == parsed_search['mouse']) & \n",
    "                                            (curated_matches['day'] == parsed_search['slug'])]['index'].values\n",
    "    day_mouse_curated_matches = assignments_filtered[day_mouse_curated_idxs, :]\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Could not find the file {curated_cell_matches_path}. Continuing with CaImAn matches...\")\n",
    "    day_mouse_curated_matches = assignments_filtered"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load freely moving data\n",
    "exp_free = WirefreeExperiment(exp_info=exp_query[0], preproc_info=preproc_query[0], tc_info=tc_query[0])\n",
    "exp_free._load_preprocessing()\n",
    "exp_free._load_tc()\n",
    "\n",
    "exp_free.dff = tuning.calculate_dff(exp_free.raw_fluor.copy(), baseline_type='quantile', quantile=0.25)\n",
    "exp_free.norm_dff = tuning.normalize_responses(exp_free.dff.copy())\n",
    "exp_free.norm_dff = drop_partial_or_long_trials(exp_free.norm_dff)\n",
    "exp_free.norm_spikes = tuning.normalize_responses(exp_free.inferred_spikes.copy())\n",
    "exp_free.norm_spikes = drop_partial_or_long_trials(exp_free.norm_spikes)\n",
    "\n",
    "# load head fixed data\n",
    "exp_fixed = WirefreeExperiment(exp_info=exp_query[1], preproc_info=preproc_query[1], tc_info=tc_query[1])\n",
    "exp_fixed._load_preprocessing()\n",
    "exp_fixed._load_tc()\n",
    "\n",
    "exp_fixed.dff = tuning.calculate_dff(exp_fixed.raw_fluor.copy(), baseline_type='quantile', quantile=0.25)\n",
    "exp_fixed.norm_dff = tuning.normalize_responses(exp_fixed.dff.copy())\n",
    "exp_fixed.norm_dff = drop_partial_or_long_trials(exp_fixed.norm_dff)\n",
    "exp_fixed.norm_spikes = tuning.normalize_responses(exp_fixed.inferred_spikes.copy())\n",
    "exp_fixed.norm_spikes = drop_partial_or_long_trials(exp_fixed.norm_spikes)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def running_triggered_averages(activity, starts, pre_trial=0, post_trial=0):\n",
    "\n",
    "    activity.dropna(inplace=True)\n",
    "\n",
    "    bouts = np.array([[idx, idx + int(post_trial*processing_parameters.wf_frame_rate)] for idx in starts])\n",
    "    bouts[:, 0] -= int(pre_trial * processing_parameters.wf_frame_rate)\n",
    "\n",
    "    if bouts[-1, 1] > activity.index[-1]:\n",
    "        bouts[-1, 1] = activity.index[-1]\n",
    "    if bouts[0, 0] < 0:\n",
    "        bouts[0, 0] = 0\n",
    "\n",
    "    # Get the shifts from the zero point (important for plotting)\n",
    "    max_zero_idx_shift = np.max(bouts[:, 1] - bouts[:, 0])\n",
    "\n",
    "    traces = []\n",
    "    for i, frame in enumerate(bouts):\n",
    "        ds_slice = activity.iloc[frame[0]:frame[1], :].copy()\n",
    "        ds_slice['trial_num'] = i\n",
    "\n",
    "        zero_idx_shift = np.abs((frame[1] - frame[0]) - max_zero_idx_shift)\n",
    "        ds_slice['zero_idx_shift'] = zero_idx_shift\n",
    "\n",
    "        traces.append(ds_slice)\n",
    "\n",
    "\n",
    "    traces = pd.concat(traces, axis=0).reset_index(drop=True)\n",
    "    # traces.drop(['trial_num', 'direction', 'direction_wrapped', 'orientation'], axis=1, inplace=True)\n",
    "    # traces = traces.groupby('trial_num').agg(list)\n",
    "    return traces, bouts\n",
    "\n",
    "def smooth(y, box_pts):\n",
    "    box = np.ones(box_pts)/box_pts\n",
    "    y_smooth = np.convolve(y, box, mode='same')\n",
    "    return y_smooth"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "min_run_duration = 0.5 * processing_parameters.wf_frame_rate\n",
    "max_run_sep = 1.0 * processing_parameters.wf_frame_rate\n",
    "speed_plot = hv.Curve(exp_fixed.kinematics[['time_vector', 'wheel_speed_abs']]).opts(color='k') \n",
    "\n",
    "time = exp_fixed.kinematics['time_vector'].to_numpy()\n",
    "smoothed_wheel_speed = smooth(exp_fixed.kinematics['wheel_speed_abs'], 10)\n",
    "smoothed_speed_splot = hv.Curve((time, smoothed_wheel_speed)).opts(color='r')\n",
    "\n",
    "fixed_speed_cutoff = np.percentile(smoothed_wheel_speed, 65)\n",
    "threshold_plot = hv.HLine(fixed_speed_cutoff).opts(color='blue')\n",
    "\n",
    "is_running = np.argwhere(smoothed_wheel_speed > fixed_speed_cutoff).squeeze()\n",
    "running_bouts = [bout for bout in fmisc.consecutive(is_running, stepsize=max_run_sep) if len(bout) >= min_run_duration]\n",
    "runs = [np.vstack((exp_fixed.kinematics['time_vector'].iloc[run].values, -5*np.ones_like(run))).T for run in running_bouts]\n",
    "running_start_idxs = np.array([a[0] for a in running_bouts])\n",
    "running_start_times = exp_fixed.kinematics['time_vector'].iloc[running_start_idxs].values\n",
    "runs_plot = hv.Path(runs).opts(color='r', line_width=2)\n",
    "\n",
    "fixed_speed_plot = hv.Overlay([speed_plot, smoothed_speed_splot, runs_plot, threshold_plot]).opts(height=280, width=1400, yaxis=None, xlabel=None, ylabel=None)\n",
    "\n",
    "\n",
    "# save_path = os.path.join(fig_path, '5_running_evoked_responses', f'fixed_speed_annotated.png')\n",
    "# fixed_speed_plot = fp.save_figure(fixed_speed_plot, save_path=save_path, fig_width=14, dpi=800, fontsize='paper', target='none', display_factor=0.3)\n",
    "\n",
    "print(exp_fixed.kinematics['wheel_speed_abs'].min(), exp_fixed.kinematics['wheel_speed_abs'].max())\n",
    "fixed_speed_plot"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "from hmmlearn import hmm\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "scores = list()\n",
    "models = list()\n",
    "for n_components in range(1, 3):\n",
    "    for idx in range(10):  # ten different random starting states\n",
    "        # define our hidden Markov model\n",
    "        model = hmm.GMMHMM(n_components=n_components, random_state=idx,\n",
    "                               n_iter=100)\n",
    "        model.fit(exp_fixed.kinematics['wheel_speed_abs'].to_numpy()[:, None])\n",
    "        models.append(model)\n",
    "        scores.append(model.score(exp_fixed.kinematics['wheel_speed_abs'].to_numpy()[:, None]))\n",
    "      #   print(f'Converged: {model.monitor_.converged}\\t\\t'\n",
    "      #         f'Score: {scores[-1]}')\n",
    "\n",
    "# get the best model\n",
    "model = models[np.argmax(scores)]\n",
    "print(f'The best model had a score of {max(scores)} and '\n",
    "      f'{model.n_components} components')\n",
    "\n",
    "# use the Viterbi algorithm to predict the most likely sequence of states\n",
    "# given the model\n",
    "states = model.predict(exp_fixed.kinematics['wheel_speed_abs'].to_numpy()[:, None])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "predict_speed_plot = hv.Curve((time, states)).opts(color='r')\n",
    "\n",
    "fixed_speed_plot = hv.Overlay([speed_plot, predict_speed_plot, runs_plot, threshold_plot]).opts(height=280, width=1400, yaxis=None, xlabel=None, ylabel=None)\n",
    "\n",
    "\n",
    "# save_path = os.path.join(fig_path, '5_running_evoked_responses', f'fixed_speed_annotated.png')\n",
    "# fixed_speed_plot = fp.save_figure(fixed_speed_plot, save_path=save_path, fig_width=14, dpi=800, fontsize='paper', target='none', display_factor=0.3)\n",
    "\n",
    "# print(exp_fixed.kinematics['wheel_speed_abs'].min(), exp_fixed.kinematics['wheel_speed_abs'].max())\n",
    "fixed_speed_plot"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "fixed_cells = [col for col in exp_fixed.norm_dff.columns if 'cell' in col]\n",
    "exp_fixed.cells_to_match = np.random.choice(fixed_cells, 10, replace=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def get_running_modulated_cells(activity_df, cells, running_bouts_idxs, ci_interval=0.95):\n",
    "\n",
    "    # Find cells that are significantly modulated by running in general\n",
    "    still_bouts_idxs = np.setdiff1d(activity_df.index, running_bouts_idxs)\n",
    "    cell_running_activity = activity_df.loc[running_bouts_idxs, cells].apply(np.nanmean, axis=0)\n",
    "    cell_still_activity = activity_df.loc[still_bouts_idxs, cells].apply(np.nanmean, axis=0)\n",
    "    running_diff = cell_running_activity - cell_still_activity\n",
    "    running_cis = sp.stats.t.interval(ci_interval, len(running_diff)-1, loc=np.mean(running_diff), scale=sp.stats.sem(running_diff))\n",
    "    sig_running_modulated = (running_diff < running_cis[0]) | (running_diff > running_cis[1])\n",
    "\n",
    "    # Find cells that are significantly modulated by running during visual stimulus\n",
    "    vis_stim_idxs = activity_df[activity_df.trial_num >= 1].index\n",
    "    vis_running_idxs = np.intersect1d(running_bouts_idxs, vis_stim_idxs)\n",
    "    vis_still_idxs = np.intersect1d(still_bouts_idxs, vis_stim_idxs)\n",
    "    vis_cell_running_activity = activity_df.loc[vis_running_idxs, cells].apply(np.nanmean, axis=0)\n",
    "    vis_cell_still_activity = activity_df.loc[vis_still_idxs, cells].apply(np.nanmean, axis=0)\n",
    "    vis_running_diff = vis_cell_running_activity - vis_cell_still_activity\n",
    "    vis_running_cis = sp.stats.t.interval(ci_interval, len(vis_running_diff)-1, loc=np.mean(vis_running_diff), scale=sp.stats.sem(vis_running_diff))\n",
    "    sig_vis_running_modulated = (vis_running_diff < vis_running_cis[0]) | (vis_running_diff > vis_running_cis[1])\n",
    "\n",
    "    df = pd.DataFrame({'run_activity': cell_running_activity, 'still_activity': cell_still_activity, \n",
    "                       'run_diff': running_diff, 'sig_run_modulated': sig_running_modulated,\n",
    "                       'vis_run_activity': cell_running_activity, 'vis_still_activity': cell_still_activity,\n",
    "                       'vis_run_diff': vis_running_diff, 'sig_vis_run_modulated': sig_vis_running_modulated})\n",
    "    \n",
    "    return df\n",
    "    "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "pred_state_idxs = np.argwhere(states > 0).squeeze()\n",
    "\n",
    "fixed_run_mod = get_running_modulated_cells(exp_fixed.norm_spikes, fixed_cells, pred_state_idxs)\n",
    "run_mod_idx = fixed_run_mod.sig_run_modulated + fixed_run_mod.sig_vis_run_modulated\n",
    "col = np.where(run_mod_idx == 2, 'r', np.where(run_mod_idx == 1, 'g', 'k'))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "max_range = np.max([fixed_run_mod.run_activity.max(), fixed_run_mod.still_activity.max()])\n",
    "x = np.linspace(0, max_range, 100)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(fixed_run_mod.vis_still_activity, fixed_run_mod.vis_run_activity, c=col, alpha=0.8)\n",
    "plt.plot(np.linspace(0, max_range, 100), np.linspace(0, max_range, 100), 'k--')\n",
    "plt.plot(np.linspace(0, max_range, 100), x+0.005, 'k--')\n",
    "plt.plot(np.linspace(0.005, max_range, 100), np.linspace(0.005, max_range, 100)-0.005, 'k--')\n",
    "plt.xlim(0, max_range)\n",
    "plt.ylim(0, max_range)\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "pre_run = 1\n",
    "post_run = 4\n",
    "pre_trial = int(pre_run * processing_parameters.wf_frame_rate)\n",
    "\n",
    "run_traces, run_bouts = running_triggered_averages(exp_fixed.norm_dff.copy(), running_starts, pre_trial=pre_run, post_trial=post_run)\n",
    "idxs_shifts = run_traces.groupby(['trial_num']).apply(lambda x: np.unique(x.zero_idx_shift)[0]).reset_index().rename({0: 'zero_idx_shift'}, axis=1)\t\n",
    "run_traces = run_traces[fixed_cells + ['trial_num', 'zero_idx_shift']].groupby('trial_num').agg(list)\n",
    "\n",
    "plot_list = []\n",
    "diff_list = []\n",
    "\n",
    "for i, cell in enumerate(fixed_cells):\n",
    "    save_path = os.path.join(fig_path, '5_running_evoked_responses', f'fixed_cell_{i}.png')\n",
    "\n",
    "    resps = fmisc.list_lists_to_array(run_traces[cell].tolist(), alignment='left')\n",
    "    mean = np.nanmean(resps, axis=0)\n",
    "\n",
    "    pre_trial_resp = np.nanmean(resps[:, :pre_trial], axis=1)\n",
    "    post_run_resp = np.nanmean(resps[:, pre_trial:], axis=1)\n",
    "    diff = post_run_resp - pre_trial_resp\n",
    "    diff_list.append(np.nanmean(diff, axis=0))\n",
    "    \n",
    "    trials_list = [hv.Curve(resps[r, :]).opts(color='gray', alpha=0.25) for r in np.arange(resps.shape[0])]\n",
    "    mean_list = [hv.Curve((np.arange(0, mean.shape[0]), mean)).opts(color='r', xlabel='', ylabel='', title=cell, fontsize={'xticks': 10, 'yticks': 10,})]\n",
    "    vlines = [hv.VLine(int(pre_run * processing_parameters.wf_frame_rate)).opts(color='k', alpha=1, line_width=0.5)]\n",
    "    run_evoked_plot = hv.Overlay(trials_list + mean_list + vlines).opts(width=150, height=150, yaxis=None, xaxis=None, xlabel=None, ylabel=None, title='')\n",
    "    # run_evoked_plot = fp.save_figure(run_evoked_plot, save_path=save_path, fig_width=3, dpi=800, fontsize='paper', target='none', display_factor=0.2)\n",
    "    plot_list.append(run_evoked_plot)\n",
    "\n",
    "running_trig_avg = hv.Layout(plot_list).cols(10)\n",
    "running_trig_avg"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freely Moving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "min_run_duration = 0.5 * processing_parameters.wf_frame_rate\n",
    "max_run_sep = 1.0 * processing_parameters.wf_frame_rate\n",
    "speed_plot = hv.Curve(exp_free.kinematics[['time_vector', 'mouse_speed']]).opts(color='k') \n",
    "\n",
    "time = exp_free.kinematics['time_vector'].to_numpy()\n",
    "smoothed_wheel_speed = smooth(exp_free.kinematics['mouse_speed'], 10)\n",
    "smoothed_speed_splot = hv.Curve((time, smoothed_wheel_speed)).opts(color='r')\n",
    "\n",
    "fixed_speed_cutoff = np.percentile(smoothed_wheel_speed, 65)\n",
    "threshold_plot = hv.HLine(fixed_speed_cutoff).opts(color='blue')\n",
    "\n",
    "is_running = np.argwhere(smoothed_wheel_speed > fixed_speed_cutoff).squeeze()\n",
    "running_bouts = [bout for bout in fmisc.consecutive(is_running, stepsize=max_run_sep) if len(bout) >= min_run_duration]\n",
    "runs = [np.vstack((exp_free.kinematics['time_vector'].iloc[run].values, -5*np.ones_like(run))).T for run in running_bouts]\n",
    "running_start_idxs = np.array([a[0] for a in running_bouts])\n",
    "running_start_times = exp_free.kinematics['time_vector'].iloc[running_start_idxs].values\n",
    "runs_plot = hv.Path(runs).opts(color='r', line_width=2)\n",
    "\n",
    "free_speed_plot = hv.Overlay([speed_plot, smoothed_speed_splot, runs_plot, threshold_plot]).opts(height=280, width=1400, yaxis=None, xlabel=None, ylabel=None)\n",
    "\n",
    "\n",
    "# save_path = os.path.join(fig_path, '5_running_evoked_responses', f'fixed_speed_annotated.png')\n",
    "# fixed_speed_plot = fp.save_figure(fixed_speed_plot, save_path=save_path, fig_width=14, dpi=800, fontsize='paper', target='none', display_factor=0.3)\n",
    "\n",
    "print(exp_free.kinematics['mouse_speed'].min(), exp_free.kinematics['mouse_speed'].max())\n",
    "free_speed_plot"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "scores = list()\n",
    "models = list()\n",
    "for n_components in range(1, 3):\n",
    "    for idx in range(10):  # ten different random starting states\n",
    "        # define our hidden Markov model\n",
    "        model = hmm.GMMHMM(n_components=n_components, random_state=idx,\n",
    "                               n_iter=100)\n",
    "        model.fit(exp_free.kinematics['mouse_speed'].to_numpy()[:, None])\n",
    "        models.append(model)\n",
    "        scores.append(model.score(exp_free.kinematics['mouse_speed'].to_numpy()[:, None]))\n",
    "      #   print(f'Converged: {model.monitor_.converged}\\t\\t'\n",
    "      #         f'Score: {scores[-1]}')\n",
    "\n",
    "# get the best model\n",
    "model = models[np.argmax(scores)]\n",
    "print(f'The best model had a score of {max(scores)} and '\n",
    "      f'{model.n_components} components')\n",
    "\n",
    "# use the Viterbi algorithm to predict the most likely sequence of states\n",
    "# given the model\n",
    "states = model.predict(exp_free.kinematics['mouse_speed'].to_numpy()[:, None])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "predict_speed_plot = hv.Curve((time, states)).opts(color='r')\n",
    "is_running = np.argwhere(states > 0).squeeze()\n",
    "\n",
    "running_bouts = [bout for bout in fmisc.consecutive(is_running, stepsize=max_run_sep) if len(bout) >= 1]\n",
    "runs = [np.vstack((exp_free.kinematics['time_vector'].iloc[run].values, -5*np.ones_like(run))).T for run in running_bouts]\n",
    "runs_plot = hv.Path(runs).opts(color='r', line_width=2)\n",
    "\n",
    "free_speed_plot = hv.Overlay([speed_plot, predict_speed_plot, runs_plot]).opts(height=280, width=1400, yaxis=None, xlabel=None, ylabel=None)\n",
    "free_speed_plot"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "free_cells = [col for col in exp_free.norm_dff.columns if 'cell' in col]\n",
    "exp_free.cells_to_match = np.random.choice(free_cells, 10, replace=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "pre_run = 1\n",
    "post_run = 4\n",
    "pre_trial = int(pre_run * processing_parameters.wf_frame_rate)\n",
    "\n",
    "run_traces, run_bouts = running_triggered_averages(exp_free.norm_dff.copy(), running_starts, pre_trial=pre_run, post_trial=post_run)\n",
    "idxs_shifts = run_traces.groupby(['trial_num']).apply(lambda x: np.unique(x.zero_idx_shift)[0]).reset_index().rename({0: 'zero_idx_shift'}, axis=1)\t\n",
    "run_traces = run_traces[free_cells + ['trial_num', 'zero_idx_shift']].groupby('trial_num').agg(list)\n",
    "\n",
    "plot_list = []\n",
    "diff_list = []\n",
    "\n",
    "for i, cell in enumerate(free_cells):\n",
    "    save_path = os.path.join(fig_path, '5_running_evoked_responses', f'free_cell_{i}.png')\n",
    "\n",
    "    resps = fmisc.list_lists_to_array(run_traces[cell].tolist(), alignment='left')\n",
    "    mean = np.nanmean(resps, axis=0)\n",
    "\n",
    "    pre_trial_resp = np.nanmean(resps[:, :pre_trial], axis=1)\n",
    "    post_run_resp = np.nanmean(resps[:, pre_trial:], axis=1)\n",
    "    diff = post_run_resp - pre_trial_resp\n",
    "    diff_list.append(np.nanmean(diff, axis=0))\n",
    "    \n",
    "    trials_list = [hv.Curve(resps[r, :]).opts(color='gray', alpha=0.25) for r in np.arange(resps.shape[0])]\n",
    "    mean_list = [hv.Curve((np.arange(0, mean.shape[0]), mean)).opts(color='r', xlabel='', ylabel='', title=cell, fontsize={'xticks': 10, 'yticks': 10,})]\n",
    "    vlines = [hv.VLine(int(pre_run * processing_parameters.wf_frame_rate)).opts(color='k', alpha=1, line_width=0.5)]\n",
    "    run_evoked_plot = hv.Overlay(trials_list + mean_list + vlines).opts(width=150, height=150, yaxis=None, xaxis=None, xlabel=None, ylabel=None, title='')\n",
    "    # run_evoked_plot = fp.save_figure(run_evoked_plot, save_path=save_path, fig_width=3, dpi=800, fontsize='paper', target='none', display_factor=0.2)\n",
    "    plot_list.append(run_evoked_plot)\n",
    "\n",
    "running_trig_avg = hv.Layout(plot_list).cols(10)\n",
    "running_trig_avg"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prey_capture",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
