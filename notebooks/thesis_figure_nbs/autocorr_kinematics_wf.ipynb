{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.signal as sig\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(r'C:/Users/mmccann/repos/bonhoeffer/prey_capture/'))\n",
    "\n",
    "import paths\n",
    "import processing_parameters\n",
    "import functions_loaders as fl\n",
    "import functions_data_handling as fdh\n",
    "import functions_bondjango as bd\n",
    "from functions_misc import list_lists_to_array, find_nearest\n",
    "\n",
    "import panel as pn\n",
    "import holoviews as hv\n",
    "from holoviews import opts, dim\n",
    "from holoviews.operation import histogram\n",
    "hv.extension('bokeh')\n",
    "from bokeh.resources import INLINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_paths, all_queries = fl.query_search_list()\n",
    "mice = ['_'.join(os.path.basename(path).split('_')[7:10]) for path in all_paths[0]]\n",
    "print(all_paths)\n",
    "\n",
    "data_list = []\n",
    "# load the data\n",
    "for path, queries in zip(all_paths, all_queries):\n",
    "    \n",
    "    data, _, metadata  = fl.load_preprocessing(path, queries, latents_flag=False)\n",
    "    data_list.append(data)\n",
    "\n",
    "data_list = [ds for el in data_list for ds in el]\n",
    "frame_rate = processing_parameters.wf_frame_rate\n",
    "kinem_vars =  processing_parameters.variable_list_free #processing_parameters.variable_list_free + processing_parameters.variable_list_fixed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test autocorrelation of kinematic variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 10    # seconds\n",
    "\n",
    "autocorr_dict = {}\n",
    "mouse_speed_list = []\n",
    "mouse_angular_speed_list = []\n",
    "pupil_diam_list = []\n",
    "for ds in data_list:\n",
    "    ds.dropna(inplace=True)\n",
    "    \n",
    "    if 'mouse_angular_speed' in ds.columns:\n",
    "        mouse_angular_speed_list.append(ds['mouse_angular_speed'].to_numpy())\n",
    "    if 'mouse_speed' in ds.columns:\n",
    "        mouse_speed_list.append(ds['mouse_speed'].to_numpy())\n",
    "\n",
    "    for kvar in kinem_vars:\n",
    "        if kvar in ds.columns:\n",
    "\n",
    "            # Handle exception for x and y position in head fixed data\n",
    "            if (kvar == 'mouse_x_m') and ('wheel_speed' in ds.columns):\n",
    "                continue\n",
    "            elif (kvar == 'mouse_y_m') and ('wheel_speed' in ds.columns):\n",
    "                continue\n",
    "            else:\n",
    "                autocorr_list = []\n",
    "                x = ds[kvar].to_numpy()\n",
    "\n",
    "                if window_size == 'all':\t\n",
    "                    xp = x - x.mean()\n",
    "                    result = sig.correlate(xp, xp, mode='full')\n",
    "                    result = result[result.size//2:] / np.var(x) / len(xp)\n",
    "\n",
    "                else:\n",
    "                    # Parse the signal into chunks\n",
    "                    chunk_size = x.size//(window_size*frame_rate)\n",
    "                    if chunk_size < 1:\n",
    "                        pass\n",
    "                    else:\n",
    "                        x = np.array_split(x, chunk_size)\n",
    "                        for y in x:\n",
    "                            yp = y - y.mean()\n",
    "                            result = sig.correlate(yp, yp, mode='full')\n",
    "                            result = result[result.size//2:] / np.var(y) / len(yp)\n",
    "                            autocorr_list.append(result)\n",
    "                        result = list_lists_to_array(autocorr_list)\n",
    "\n",
    "                autocorr_dict[kvar] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mouse_speed = np.concatenate(mouse_speed_list)\n",
    "# mouse_speed *= 360/(2*np.pi)\n",
    "mouse_speed = mouse_speed[~np.isnan(mouse_speed)]\n",
    "lower_thresh = np.percentile(mouse_speed, 2)\n",
    "upper_thresh = np.percentile(mouse_speed, 99.5)\n",
    "# mouse_speed = mouse_speed[mouse_speed <= upper_thresh]\n",
    "# mouse_speed = mouse_speed[mouse_speed >= lower_thresh]\n",
    "fig=plt.figure()\n",
    "plt.hist(mouse_speed, bins=200)\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mouse_angular_speed = np.concatenate(mouse_angular_speed_list)\n",
    "# mouse_angular_speed *= 360/(2*np.pi)\n",
    "mouse_angular_speed = mouse_angular_speed[~np.isnan(mouse_angular_speed)]\n",
    "lower_thresh = np.percentile(mouse_angular_speed, 2)\n",
    "upper_thresh = np.percentile(mouse_angular_speed, 99.5)\n",
    "# mouse_angular_speed = mouse_angular_speed[mouse_angular_speed <= upper_thresh]\n",
    "# mouse_angular_speed = mouse_angular_speed[mouse_angular_speed >= lower_thresh]\n",
    "fig=plt.figure()\n",
    "plt.hist(mouse_angular_speed, bins=200)\n",
    "# plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=6, figsize=(10, 5))\n",
    "mean_autocorr_dict = {}\n",
    "for ax, key in zip(axes.flatten(), autocorr_dict.keys()):\n",
    "    autocorr = autocorr_dict[key]\n",
    "    mean_autocorr = np.mean(autocorr, axis=0)\n",
    "    mean_autocorr_dict[key] = mean_autocorr\n",
    "    if window_size != 'all':\n",
    "        ax.plot(np.arange(0, autocorr.shape[-1])/frame_rate, autocorr.T, alpha=0.2)\n",
    "        ax.plot(np.arange(0, autocorr.shape[-1])/frame_rate, mean_autocorr, color='r')\n",
    "        ax.set_xlim(-1, window_size+1)\n",
    "        ax.hlines(0, -1, window_size+1, color='k', linestyle='--')\n",
    "\n",
    "    else:\n",
    "        ax.plot(np.arange(0, autocorr.shape[-1])/frame_rate, autocorr.T)\n",
    "\n",
    "    ax.set_title(key)\n",
    "    ax.set_xlabel('Time (s)')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in mean_autocorr_dict.keys():\n",
    "    autocorr = mean_autocorr_dict[key]\n",
    "    zero = np.argwhere(np.diff(np.sign(autocorr)))[0][0]\n",
    "    print(f'Autocorrelation zero crossing for {key} is {zero/frame_rate} seconds')\n",
    "    print(f'Autocorrelation minimum is {np.nanmin(autocorr)} at {np.nanargmin(autocorr)/frame_rate} seconds\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at how consistent traces are across session halves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace1_dict = {}\n",
    "trace2_dict = {}\n",
    "for kvar in kinem_vars:\n",
    "    kvar1_list = []\n",
    "    kvar2_list = []\n",
    "\n",
    "    for ds in data_list:\n",
    "\n",
    "        ds.dropna(inplace=True)\n",
    "        \n",
    "        if 'wheel_speed' in ds.columns:\n",
    "            ds['wheel_speed_abs'] = ds['wheel_speed'].abs().copy()\n",
    "\n",
    "        if kvar in ds.columns:\n",
    "            # Handle exception for x and y position in head fixed data\n",
    "            if (kvar == 'mouse_x_m') and ('wheel_speed' in ds.columns):\n",
    "                continue\n",
    "            elif (kvar == 'mouse_y_m') and ('wheel_speed' in ds.columns):\n",
    "                continue\n",
    "            else:\n",
    "                x = ds[kvar].to_numpy()\n",
    "                x = np.array_split(x, 2)\n",
    "                kvar1_list.append(x[0])\n",
    "                kvar2_list.append(x[1])\n",
    "            \n",
    "            # trace1_dict[kvar] = np.concatenate(kvar1_list)\n",
    "            # trace2_dict[kvar] = np.concatenate(kvar2_list)\n",
    "    trace1_dict[kvar] = list_lists_to_array(kvar1_list)\n",
    "    trace2_dict[kvar] = list_lists_to_array(kvar2_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_list = []\n",
    "for feature in trace1_dict.keys():\n",
    "    traces1 = trace1_dict[feature]\n",
    "    traces2 = trace2_dict[feature]\n",
    "    plt1 = hv.Path((np.arange(traces1.shape[0]), traces1))\n",
    "    plt2 = hv.Path((np.arange(traces2.shape[0]), traces2))\n",
    "    overlay = plt1 * plt2\n",
    "    overlay.opts(title=feature, xrotation=45)\n",
    "    plot_list.append(overlay)\n",
    "hv.Layout(plot_list).cols(5).opts(shared_axes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prey_capture_old",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
