{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1ad7f9",
   "metadata": {},
   "source": [
    "# imports\n",
    "# suppress holoviews warning. Using warnings module didn't work\n",
    "import logging\n",
    "# logging.getLogger(\"param.Dimension\").setLevel(logging.CRITICAL)\n",
    "# logging.getLogger(\"param.ParameterizedMetaclass\").setLevel(logging.CRITICAL)\n",
    "# logging.getLogger(\"param.HistogramPlot\").setLevel(logging.CRITICAL)\n",
    "# logging.getLogger(\"param.AdjointLayout\").setLevel(logging.CRITICAL)\n",
    "# logging.getLogger(\"param.OverlayPlot\").setLevel(logging.CRITICAL)\n",
    "# logging.getLogger(\"param.HoloMap\").setLevel(logging.CRITICAL)\n",
    "# logging.getLogger(\"param.CurvePlot\").setLevel(logging.CRITICAL)\n",
    "# logging.getLogger(\"param.Layout\").setLevel(logging.CRITICAL)\n",
    "# logging.getLogger(\"param.LayerPlot\").setLevel(logging.CRITICAL)\n",
    "# logging.getLogger(\"param.RasterPlot\").setLevel(logging.CRITICAL)\n",
    "# logging.getLogger(\"param.Scatter3DPlot\").setLevel(logging.CRITICAL)\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath(r'D:\\Code Repos\\prey_capture'))\n",
    "\n",
    "\n",
    "import panel as pn\n",
    "import holoviews as hv\n",
    "from holoviews import opts, dim\n",
    "hv.extension('bokeh')\n",
    "from bokeh.resources import INLINE\n",
    "\n",
    "import paths\n",
    "import functions_bondjango as bd\n",
    "import functions_misc as fm\n",
    "import functions_plotting as fp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.mixture as mix\n",
    "import sklearn.decomposition as decomp\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_predict, train_test_split\n",
    "from sklearn import svm, datasets\n",
    "from sklearn import preprocessing\n",
    "import sklearn.linear_model as lin\n",
    "import sklearn.metrics as smet\n",
    "import scipy.signal as ss\n",
    "import scipy.stats as stat\n",
    "import scipy.spatial.distance as space\n",
    "\n",
    "import random\n",
    "import functions_data_handling as fd\n",
    "import functions_vame as fv\n",
    "import importlib\n",
    "import processing_parameters\n",
    "import PSID\n",
    "from PSID.evaluation import evalPrediction\n",
    "import sklearn.cross_decomposition as cros\n",
    "import umap\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e585e34",
   "metadata": {},
   "source": [
    "# Load the desired files\n",
    "importlib.reload(processing_parameters)\n",
    "\n",
    "# define the threshold for matched cells\n",
    "match_threshold = 10\n",
    "# get the data paths\n",
    "try: \n",
    "    data_path = snakemake.input[0]\n",
    "except NameError:\n",
    "    # get the search list\n",
    "    search_list = processing_parameters.search_list\n",
    "    # allocate memory for the data\n",
    "    pre_normal_data = []\n",
    "    pre_mod_data = []\n",
    "    \n",
    "    # allocate a list for all paths (need to preload to get the dates)\n",
    "    all_paths = []\n",
    "    # for all the search strings\n",
    "    for search_string in search_list:\n",
    "\n",
    "        # query the database for data to plot\n",
    "        data_all = bd.query_database('analyzed_data', search_string)\n",
    "        data_all = [el for el in data_all if 'preproc' in el['slug']]\n",
    "        data_path = [el['analysis_path'] for el in data_all if '_preproc' in el['slug']]\n",
    "        all_paths.append(data_path)\n",
    "    print(all_paths[0])\n",
    "    # get the dates present\n",
    "    data_dates = np.unique([os.path.basename(el)[:10] for el in np.concatenate(all_paths)])\n",
    "    # now load the files\n",
    "    for data_path in all_paths:\n",
    "        # load the calcium data\n",
    "        beh_data = []\n",
    "        # for all the files\n",
    "        for files in data_path:\n",
    "            # load the data\n",
    "            with pd.HDFStore(files) as h:\n",
    "                beh_data.append(h['full_traces'])\n",
    "                if '/matched_calcium' in h.keys():\n",
    "                    # get the cell matches\n",
    "                    cell_matches = h['cell_matches']\n",
    "                    \n",
    "                    # perform only if there are more files\n",
    "                    if len(data_dates) > 1:\n",
    "                        match_dates = [el for el in data_dates if el in cell_matches.columns]\n",
    "                        # get only the days present in the search\n",
    "                        cell_matches = cell_matches[match_dates]\n",
    "                        # generate a list with the number of days and the number of cells kept\n",
    "\n",
    "                        # get the unique cell combinations\n",
    "                        # unique contains the unique patterns followed by cells across days\n",
    "                        # inverse indicates which pattern is followed by each cell\n",
    "                        # count contains the number of times each pattern is found\n",
    "                        unique, inverse, counts = np.unique(~np.isnan(cell_matches.to_numpy()), axis=0, \n",
    "                                        return_counts=True, return_inverse=True)\n",
    "                        \n",
    "                        # remove the single day and no day cases\n",
    "                        counts[np.sum(unique, axis=1)==0] = 0\n",
    "                        counts[np.sum(unique, axis=1)==1] = 0\n",
    "\n",
    "                        # get an index vector with only the most popular pattern\n",
    "                        # (regardless of how many cells share it)\n",
    "                        cell_idx = np.array(inverse==np.argmax(counts))\n",
    "                        # leave only those cells in the matches\n",
    "                        cell_matches = cell_matches.iloc[cell_idx, :]\n",
    "                    else:\n",
    "                        counts = 1\n",
    "                        cell_idx = cell_matches[data_dates].to_numpy()\n",
    "                        cell_idx = ~np.isnan(cell_idx)\n",
    "                        cell_matches = cell_matches.iloc[cell_idx, :]\n",
    "                    # separate based on normal vs mod\n",
    "                    if 'dark' in files:\n",
    "                        pre_mod_data.append((files, h['matched_calcium'], cell_matches))\n",
    "                    else:\n",
    "                        pre_normal_data.append((files, h['matched_calcium'], cell_matches))\n",
    "                    \n",
    "print(f'Number of matched cells: {np.sum(cell_idx)}')\n",
    "print(f'Number of matched days: {unique[np.argmax(counts)].sum()}')\n",
    "\n",
    "# hv.Image(cell_matches.to_numpy()).opts(width=800, tools=['hover'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e291781",
   "metadata": {},
   "source": [
    "# Leave only common cells across all datasets\n",
    "\n",
    "# allocate memory for the cleaned up data\n",
    "normal_data = []\n",
    "mod_data = []\n",
    "plot_data = []\n",
    "\n",
    "# print(pre_normal_data[0][2])\n",
    "# for all the normal trials\n",
    "for idx, el in enumerate(pre_normal_data):\n",
    "    # get the date\n",
    "    current_date = os.path.basename(el[0])[:10]\n",
    "    # get the corresponding indexes\n",
    "    current_idx = el[2][current_date].to_numpy()\n",
    "    # if they're all nans, skip the day\n",
    "    if np.isnan(np.sum(current_idx)):\n",
    "        continue\n",
    "    # get the current df\n",
    "    current_df = el[1]\n",
    "    labels = list(current_df.columns)\n",
    "    cells = [el for el in labels if 'cell' in el]\n",
    "    not_cells = [el for el in labels if 'cell' not in el]\n",
    "    # get the non-cell data\n",
    "    non_cell_data = current_df[not_cells]\n",
    "    # get the current calcium data\n",
    "    cell_data = current_df[cells]\n",
    "    # remove the non matched cells\n",
    "#     cell_data = cell_data.iloc[:, current_idx]\n",
    "    # rename the cell fields\n",
    "    cell_names = ['cell_' + str(el) for el in np.arange(cell_data.shape[1])]\n",
    "    cell_data.columns = cell_names\n",
    "    # normalize the single trial activity\n",
    "#     cell_data = (cell_data-cell_data.mean())/cell_data.std()\n",
    "    # get rid of the nan values created by silent neurons in a given trial\n",
    "    cell_data[np.isnan(cell_data)] = 0\n",
    "    # assemble a new data frame with only the matched cells and the rest of the data\n",
    "    normal_data.append(pd.concat((non_cell_data, cell_data), axis=1))\n",
    "    \n",
    "print(normal_data[0].shape)\n",
    "print(normal_data[0].columns)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c00338",
   "metadata": {},
   "source": [
    "# align coordinates egocentrically\n",
    "importlib.reload(fv)\n",
    "# define the relevant variables\n",
    "crop_size = (200, 200)\n",
    "\n",
    "# allocate memory for the new coordinates\n",
    "new_coordinates = []\n",
    "new_columns = []\n",
    "# for all trials\n",
    "for trial in normal_data:\n",
    "    # run the alignment\n",
    "    coordinates, _, column_list = fv.align_demo(trial, [], [], [],\n",
    "                   crop_size, use_video=False, check_video=False, vid_path=None)\n",
    "    # save\n",
    "    new_coordinates.append(coordinates)\n",
    "    new_columns.append(column_list)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3e79a1",
   "metadata": {},
   "source": [
    "# Replace the raw coordinates by the egocentric ones\n",
    "print(new_columns[0])\n",
    "\n",
    "# for all the trials\n",
    "for idx, trial in enumerate(normal_data):\n",
    "    # replace the columns\n",
    "    for idx2, column in enumerate(new_columns[idx]):\n",
    "        trial[column] = new_coordinates[idx][idx2, :]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b08531e",
   "metadata": {},
   "source": [
    "# Multivariate regression\n",
    "\n",
    "\n",
    "# define the target variable\n",
    "# target_behavior = ['cricket_0_x', 'cricket_0_y', 'cricket_0_mouse_distance']\n",
    "# target_behavior = ['cricket_0_x']\n",
    "# target_behavior = ['mouse_head_x', 'mouse_head_y', 'mouse_speed']\n",
    "# target_behavior = ['mouse_x']\n",
    "# target_behavior = ['cricket_0_visual_angle']\n",
    "# target_behavior = ['cricket_0_mouse_distance', 'cricket_0_delta_heading', 'cricket_0_visual_angle']\n",
    "# target_behavior = ['mouse_speed']\n",
    "# target_behavior = ['cricket_0_delta_heading']\n",
    "target_behavior = ['cricket_0_mouse_distance']\n",
    "\n",
    "# define the downsampling factor\n",
    "downsamp = 1\n",
    "# get the number of points for each trial\n",
    "trial_vector = np.vstack([np.ones((el.shape[0], 1))*idx for idx, el in enumerate(normal_data)]).flatten()\n",
    "\n",
    "# get the table\n",
    "sub_data = pd.concat(normal_data)\n",
    "print(f'Shape of the input data (frames*columns): {sub_data.shape}')\n",
    "\n",
    "# get the available columns\n",
    "labels = list(sub_data.columns)\n",
    "cells = [el for el in labels if 'cell' in el]\n",
    "not_cells = [el for el in labels if 'cell' not in el]\n",
    "# get the cell data\n",
    "calcium_data = np.array(sub_data[cells].copy())\n",
    "\n",
    "# # use the PSID calculated latent\n",
    "# calcium_data = np.concatenate(final_latent, axis=0)\n",
    "\n",
    "# generate design matrix with past time points too\n",
    "# define the number of time points to accumulate\n",
    "time_points = 1\n",
    "\n",
    "if time_points > 1:\n",
    "    # allocate memory for the matrix\n",
    "    design_matrix = np.zeros((calcium_data.shape[0], calcium_data.shape[1]*time_points))\n",
    "    # create the source matrix\n",
    "#     source_matrix = np.vstack((np.zeros((time_points, calcium_data.shape[1])), calcium_data))\n",
    "    source_matrix = np.vstack((np.zeros((time_points-1, calcium_data.shape[1])), calcium_data))\n",
    "\n",
    "    # for all the timepoints\n",
    "    for times in np.arange(calcium_data.shape[0]):\n",
    "        design_matrix[times, :] = source_matrix[times:times+time_points, :].flatten()\n",
    "\n",
    "    # replace the calcium data\n",
    "    calcium_data = design_matrix\n",
    "\n",
    "# subselect cells based on responsiveness\n",
    "# define the fraction of cells from the top responsive to take\n",
    "cell_fraction = 1\n",
    "# if not all cells are required\n",
    "if cell_fraction < 1:\n",
    "    # get the responsiveness (as average zscored activity)\n",
    "    responsiveness = np.mean(stat.zscore(calcium_data, axis=0), axis=0)\n",
    "    # get the desired percentile\n",
    "    target_percentile = np.percentile(responsiveness, (1-cell_fraction)*100)\n",
    "    # take only the selected cells\n",
    "    calcium_data = calcium_data[:, responsiveness>target_percentile]\n",
    "\n",
    "# get the parameter\n",
    "parameter = sub_data.loc[:, target_behavior].to_numpy()\n",
    "# smooth the parameter\n",
    "parameter[:, 0] = ss.medfilt(parameter[:, 0], 21)\n",
    "\n",
    "# define the time shift (in frames, positive is to the future)\n",
    "time_shift = 0\n",
    "# trim the parameter and calcium traces accordingly\n",
    "if time_shift > 0:\n",
    "    parameter = parameter[time_shift:]\n",
    "    calcium_data = calcium_data[:-time_shift, :]\n",
    "    trial_vector = trial_vector[:-time_shift]\n",
    "elif time_shift < 0:\n",
    "    parameter = parameter[:time_shift]\n",
    "    calcium_data = calcium_data[-time_shift:, :]\n",
    "    trial_vector = trial_vector[-time_shift:]\n",
    "\n",
    "\n",
    "# exclude points without the parameter\n",
    "nan_vector = np.sum(np.isnan(parameter), axis=1) == 0\n",
    "parameter = parameter[nan_vector, :]\n",
    "calcium_data = calcium_data[nan_vector, :]\n",
    "trial_vector = trial_vector[nan_vector]\n",
    "\n",
    "# bin the data\n",
    "if downsamp > 1:\n",
    "    parameter = ss.decimate(parameter, downsamp, axis=0)\n",
    "    calcium_data = ss.decimate(calcium_data, downsamp, axis=0)\n",
    "    # warning, ghetto implementation\n",
    "    trial_vector = trial_vector[::downsamp]\n",
    "\n",
    "# # turn the parameter into labels\n",
    "# parameter = np.abs(sub_data.loc[:, 'cricket_0_mouse_distance'].to_numpy()) > 25\n",
    "# parameter = np.expand_dims(parameter[nan_vector], axis=1)\n",
    "# print(np.unique(parameter, return_counts=True))\n",
    "# print('Warning, using delta heading as classification labels')\n",
    "    \n",
    "# # remove the repeated values in a row (from tracking)\n",
    "# keep_vector = np.pad(np.diff(parameter[:, 0])!=0, (1, 0), mode='constant', constant_values=0)\n",
    "\n",
    "# # limit the data to a field of view\n",
    "# sec_parameter = sub_data.loc[:, 'cricket_0_delta_heading'].to_numpy()\n",
    "# sec_parameter = sec_parameter[nan_vector]\n",
    "# parameter_range = (-100 < sec_parameter) & (sec_parameter < 100)\n",
    "# keep_vector = parameter_range\n",
    "# keep_vector = (keep_vector) & (parameter_range)\n",
    "\n",
    "# # limit the data to a given distance to prey\n",
    "# distance_to_prey = sub_data.loc[:, 'cricket_0_mouse_distance'].to_numpy()\n",
    "# distance_to_prey = distance_to_prey[nan_vector]\n",
    "# distance_vector = distance_to_prey > 3\n",
    "# keep_vector = (keep_vector) & (distance_vector)\n",
    "\n",
    "# # select the data based on the above criteria\n",
    "# parameter = parameter[keep_vector, :]\n",
    "# calcium_data = calcium_data[keep_vector, :]\n",
    "# trial_vector = trial_vector[keep_vector]\n",
    "\n",
    "# # bin the parameter vector\n",
    "# parameter = np.digitize(parameter, np.linspace(np.min(parameter), np.max(parameter), 10)) - 1\n",
    "\n",
    "# # shufle the label vector\n",
    "# random.shuffle(parameter)\n",
    "\n",
    "# split the data\n",
    "calcium_train, calcium_test, parameter_train, parameter_test = \\\n",
    "    train_test_split(calcium_data, parameter, test_size=0.2, shuffle=False)\n",
    "\n",
    "# scale the features\n",
    "calcium_scaler = preprocessing.StandardScaler().fit(calcium_train)\n",
    "calcium_train = calcium_scaler.transform(calcium_train)\n",
    "calcium_test = calcium_scaler.transform(calcium_test)\n",
    "\n",
    "# parameter_scaler = preprocessing.StandardScaler().fit(parameter_train)\n",
    "# parameter_train = parameter_scaler.transform(parameter_train)\n",
    "# parameter_test = parameter_scaler.transform(parameter_test)\n",
    "\n",
    "# initialize the classifier\n",
    "# linear = lin.MultiTaskElasticNetCV(max_iter=5000, l1_ratio=[.1, .5, .7, .9, .95, .99, 1], \n",
    "#                                    n_jobs=7, alphas=[.0001, .01, .1, 1, 10, 100], fit_intercept=True)\n",
    "#                                    n_jobs=7, alphas=[.001])\n",
    "linear = lin.TweedieRegressor(alpha=0.01, max_iter=5000, fit_intercept=False, power=0)\n",
    "# linear = lin.HuberRegressor(alpha=.1, max_iter=5000, fit_intercept=True, epsilon=10)\n",
    "# linear = svm.SVR(max_iter=10000, kernel='rbf', C=100)\n",
    "# linear = svm.LinearSVR(max_iter=5000, C=100)\n",
    "# linear = svm.LinearSVC(max_iter=100000, C=1000, class_weight='balanced')\n",
    "# linear = svm.SVC(max_iter=10000, C=.1, class_weight='balanced', kernel='rbf')\n",
    "\n",
    "# train the classifier\n",
    "linear.fit(calcium_train, parameter_train)\n",
    "\n",
    "# predict train and test\n",
    "linear_pred = linear.predict(calcium_train)\n",
    "linear_pred_last = linear.predict(calcium_test)\n",
    "\n",
    "# # smooth the predictions\n",
    "linear_pred = ss.medfilt(linear_pred.flatten(), 21)\n",
    "linear_pred_last = ss.medfilt(linear_pred_last.flatten(), 21)\n",
    "\n",
    "try:\n",
    "#     print('Train Exp variance:'+str(smet.r2_score(parameter_train, linear_pred)))\n",
    "#     print('Test Exp variance:'+str(smet.r2_score(parameter_test, linear_pred_last)))\n",
    "    print('Train Correlation:'+str(stat.spearmanr(parameter_train.flatten(), linear_pred)[0]))\n",
    "    print('Test Correlation:'+str(stat.spearmanr(parameter_test.flatten(), linear_pred_last)[0]))\n",
    "#     print('Exp variance:'+str(smet.r2_score(parameter, linear_pred)))\n",
    "#     print('Exp variance:'+str(smet.r2_score(parameter_last, linear_pred_last)))\n",
    "except TypeError:\n",
    "    print('Train Accuracy:'+str(smet.accuracy_score(parameter_train, linear_pred)))\n",
    "    print('Test Accuracy:'+str(smet.accuracy_score(parameter_test, linear_pred_last)))\n",
    "#     print('Accuracy:'+str(smet.accuracy_score(parameter, linear_pred)))\n",
    "#     print('Accuracy:'+str(smet.accuracy_score(parameter_last, linear_pred_last)))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea554203",
   "metadata": {},
   "source": [
    "hv.extension('bokeh')\n",
    "# plot the weights\n",
    "# weights = linear.coef_\n",
    "# print(weights)\n",
    "# freq, edges = np.histogram(weights)\n",
    "# barplot = hv.Bars((edges, freq)).opts(width=800)\n",
    "\n",
    "train_scatter = hv.Scatter((parameter_train[:, 0], linear_pred), \n",
    "                           kdims=['Data', 'Prediction']).opts(width=400, height=400)\n",
    "test_scatter = hv.Scatter((parameter_test[:, 0], linear_pred_last),\n",
    "                          kdims=['Data', 'Prediction']).opts(width=400, height=400)\n",
    "\n",
    "\n",
    "# plot the predictions\n",
    "\n",
    "# x_real = np.array(range(parameter.shape[0]))\n",
    "# real = hv.Curve((x_real, parameter[:, 0])).opts(width=800)\n",
    "x_real = np.array(range(parameter_train.shape[0]))\n",
    "real = hv.Curve((x_real, parameter_train[:, 0])).opts(width=800)\n",
    "\n",
    "# x_last = np.array(range(parameter_last.shape[0]))\n",
    "# last = hv.Curve((x_last, parameter_last[:, 0])).opts(width=800)\n",
    "x_last = np.array(range(parameter_test.shape[0]))\n",
    "last = hv.Curve((x_last, parameter_test[:, 0])).opts(width=800)\n",
    "\n",
    "try:\n",
    "    pred_last = hv.Curve((x_last, linear_pred_last[:, 0]))\n",
    "except IndexError:\n",
    "    pred_last = hv.Curve((x_last, linear_pred_last))\n",
    "try:\n",
    "    pred_real = hv.Curve((x_real, linear_pred[:, 0]))\n",
    "except IndexError:\n",
    "    pred_real = hv.Curve((x_real, linear_pred))\n",
    "\n",
    "# combo = (train_scatter+test_scatter)\n",
    "# combo\n",
    "\n",
    "(real*pred_real+last*pred_last+train_scatter+test_scatter).opts(shared_axes=False).cols(1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cdde70",
   "metadata": {},
   "source": [
    "# custom aggregator for hextiles\n",
    "def aggregator(data_in):\n",
    "    if data_in.shape[0] < 3:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.nanmean(data_in)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edcba87",
   "metadata": {},
   "source": [
    "# calculate a rolling CC and compare to target variables\n",
    "\n",
    "# define the window width (in frames)\n",
    "window = 10\n",
    "half_window = int(window/2)\n",
    "# define the target variable\n",
    "target_variable0 = 'cricket_0_delta_heading'\n",
    "target_variable1 = 'cricket_0_mouse_distance'\n",
    "\n",
    "# target_variable0 = 'mouse_x'\n",
    "# target_variable1 = 'mouse_y'\n",
    "\n",
    "# target_variable0 = 'cricket_0_x'\n",
    "# target_variable1 = 'cricket_0_y'\n",
    "\n",
    "# allocate memory for the rolling correlation\n",
    "rolling_correlation = np.zeros_like(parameter)\n",
    "# generate the prediction\n",
    "prediction = ss.medfilt(linear.predict(calcium_scaler.transform(calcium_data)).flatten(), 21)\n",
    "# for all the elements within the window\n",
    "for idx, _ in enumerate(prediction[half_window:-half_window]):\n",
    "    # get the correlation coefficient for the prediction window and store\n",
    "    correlation = stat.spearmanr(parameter[idx-half_window:idx+half_window], prediction[idx-half_window:idx+half_window])[0]\n",
    "    rolling_correlation[idx+half_window] = correlation\n",
    "\n",
    "# produce the average calcium activity\n",
    "average_calcium = np.nanmean(calcium_data, axis=1)\n",
    "\n",
    "# define the plotting variables\n",
    "x1 = sub_data.loc[:, target_variable0]\n",
    "# y0 = rolling_correlation.flatten()\n",
    "y1 = sub_data.loc[:, target_variable1]\n",
    "\n",
    "# angle_vector = x1 < 0\n",
    "\n",
    "# x1 = x1[angle_vector]\n",
    "# y1 = y1[angle_vector]\n",
    "# rolling_correlation = rolling_correlation[angle_vector]\n",
    "# convert to polar coordinates\n",
    "# x1_pol = y1*np.cos(np.deg2rad(x1))\n",
    "# y1_pol = y1*np.sin(np.deg2rad(x1))\n",
    "# y1 = x1_pol\n",
    "# x1 = y1_pol\n",
    "# hextile parameters\n",
    "gridsize = 20\n",
    "min_count = 3\n",
    "# plot the comparison\n",
    "plot0 = hv.HexTiles((x1, y1), kdims=[target_variable0, 'Correlation'])\n",
    "plot0.opts(width=700, height=700, tools=['hover'], gridsize=gridsize, min_count=min_count)\n",
    "\n",
    "plot1 = hv.HexTiles((x1, y1, rolling_correlation.flatten()), \n",
    "                   kdims=[target_variable0, target_variable1], vdims=['Correlation'])\n",
    "plot1.opts(width=700, height=700, aggregator=aggregator, tools=['hover'], gridsize=gridsize, cmap='RdBu', clim=(-1, 1))\n",
    "\n",
    "# plot1 = hv.Scatter((x1, y1, rolling_correlation.flatten()), \n",
    "#                    kdims=[target_variable0, target_variable1], vdims=['Correlation'])\n",
    "# plot1.opts(width=700, height=700, tools=['hover'], cmap='RdBu')\n",
    "\n",
    "plot2 = hv.HexTiles((x1, y1, average_calcium.flatten()), \n",
    "                   kdims=[target_variable0, target_variable1], vdims=['Activity'])\n",
    "plot2.opts(width=700, height=700, aggregator=aggregator, tools=['hover'], gridsize=gridsize)\n",
    "\n",
    "\n",
    "(plot0+plot1+plot2).opts(shared_axes=False).cols(2)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eae500d",
   "metadata": {},
   "source": [
    "# plot the data going into the regression\n",
    "hv.extension('bokeh')\n",
    "\n",
    "x_train = np.array(range(parameter_train.shape[0]))\n",
    "x_test = np.array(range(parameter_test.shape[0]))\n",
    "\n",
    "calcium_train_plot = hv.Image(calcium_train.T)\n",
    "calcium_train_plot.opts(width=1200, tools=['hover'])\n",
    "parameter_train_plot = hv.Curve((x_train, parameter_train[:, 0]))\n",
    "parameter_train_plot.opts(width=1200, tools=['hover'])\n",
    "calcium_test_plot = hv.Image(calcium_test.T)\n",
    "calcium_test_plot.opts(width=1200, tools=['hover'])\n",
    "parameter_test_plot = hv.Curve((x_test, parameter_test[:, 0]))\n",
    "parameter_test_plot.opts(width=1200, tools=['hover'])\n",
    "\n",
    "\n",
    "\n",
    "(calcium_train_plot+parameter_train_plot+calcium_test_plot+parameter_test_plot).opts(shared_axes=False).cols(1)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e10c90",
   "metadata": {},
   "source": [
    "hv.extension('bokeh')\n",
    "# plot the PCA trajectories of the data\n",
    "pca_model = decomp.PCA(n_components=10)\n",
    "\n",
    "calcium_pca = pca_model.fit_transform(calcium_data)\n",
    "# calcium_pca = pca_model.fit_transform(np.concatenate(normal_data))\n",
    "color_list = ['red', 'green', 'blue', 'magenta']\n",
    "\n",
    "# define the set of PCs to plot\n",
    "pc = [0, 1, 2]\n",
    "\n",
    "plot_list1 = []\n",
    "plot_list2 = []\n",
    "plot_list3 = []\n",
    "for idx, el in enumerate(normal_data):\n",
    "\n",
    "    if idx<8:\n",
    "        c_idx = 0\n",
    "    elif 8 < idx < 12:\n",
    "        c_idx = 1\n",
    "    elif 12 < idx < 20:\n",
    "        c_idx = 2\n",
    "    else:\n",
    "        c_idx = 3\n",
    "    \n",
    "    plot_list1.append(hv.Curve((calcium_pca[trial_vector==idx, pc[0]], calcium_pca[trial_vector==idx, pc[1]])))\n",
    "    plot_list2.append(hv.Curve((calcium_pca[trial_vector==idx, pc[0]], calcium_pca[trial_vector==idx, pc[2]])))\n",
    "    plot_list3.append(hv.Curve((calcium_pca[trial_vector==idx, pc[1]], calcium_pca[trial_vector==idx, pc[2]])))\n",
    "    \n",
    "    plot_list1[idx].opts(width=600, height=600, color=hv.Cycle('Spectral'))\n",
    "    plot_list2[idx].opts(width=600, height=600, color=hv.Cycle('Spectral'))\n",
    "    plot_list3[idx].opts(width=600, height=600, color=hv.Cycle('Spectral'))\n",
    "#     plot_list1[idx].opts(width=600, height=600, color=color_list[c_idx])\n",
    "#     plot_list2[idx].opts(width=600, height=600, color=color_list[c_idx])\n",
    "#     plot_list3[idx].opts(width=600, height=600, color=color_list[c_idx])\n",
    "# proj2 = hv.Curve((calcium_pca[:, 0], calcium_pca[:, 2]))\n",
    "# proj3 = hv.Curve((calcium_pca[:, 1], calcium_pca[:, 2]))\n",
    "\n",
    "# proj1+proj2+proj3\n",
    "\n",
    "ov1 = hv.Overlay(plot_list1)\n",
    "ov2 = hv.Overlay(plot_list2)\n",
    "ov3 = hv.Overlay(plot_list3)\n",
    "\n",
    "(ov1+ov2+ov3).cols(1)\n",
    "# plot the trajectory\n",
    "# curve3d = hv.Path3D(calcium_pca)*hv.Scatter3D(np.reshape(calcium_pca[0,:],(1,-1)))\n",
    "# curve3d"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a2b4af",
   "metadata": {},
   "source": [
    "hv.extension('bokeh')\n",
    "# plot the PCA trajectories of the data on a per trial basis\n",
    "\n",
    "# calcium_pca = pca_model.fit_transform(np.concatenate(normal_data))\n",
    "\n",
    "plot_list1 = []\n",
    "plot_list2 = []\n",
    "plot_list3 = []\n",
    "for idx, data in enumerate(normal_data):\n",
    "    \n",
    "    labels = list(data.columns)\n",
    "    cells = [el for el in labels if 'cell' in el]\n",
    "    # get the cell data\n",
    "    calcium_data = np.array(data[cells].copy())\n",
    "    # get rid of the super small values\n",
    "    calcium_data[np.isnan(calcium_data)] = 0\n",
    "    pca_model = decomp.PCA(n_components=3)\n",
    "\n",
    "    calcium_pca = pca_model.fit_transform(calcium_data)\n",
    "\n",
    "    plot_list1.append(hv.Curve((calcium_pca[:, 0], calcium_pca[:, 1])))\n",
    "    plot_list2.append(hv.Curve((calcium_pca[:, 0], calcium_pca[:, 2])))\n",
    "    plot_list3.append(hv.Curve((calcium_pca[:, 1], calcium_pca[:, 2])))\n",
    "    \n",
    "    plot_list1[idx].opts(width=600, height=600, color=hv.Cycle('Spectral'))\n",
    "    plot_list2[idx].opts(width=600, height=600, color=hv.Cycle('Spectral'))\n",
    "    plot_list3[idx].opts(width=600, height=600, color=hv.Cycle('Spectral'))\n",
    "# proj2 = hv.Curve((calcium_pca[:, 0], calcium_pca[:, 2]))\n",
    "# proj3 = hv.Curve((calcium_pca[:, 1], calcium_pca[:, 2]))\n",
    "\n",
    "# proj1+proj2+proj3\n",
    "\n",
    "ov1 = hv.Overlay(plot_list1)\n",
    "ov2 = hv.Overlay(plot_list2)\n",
    "ov3 = hv.Overlay(plot_list3)\n",
    "\n",
    "(ov1+ov2+ov3).cols(1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7631a2e",
   "metadata": {},
   "source": [
    "# perform PSID\n",
    "\n",
    "# define the target behavior(s)\n",
    "# target_behavior = ['mouse_x', 'mouse_y', 'mouse_head_x', 'mouse_head_y', \n",
    "#                    'mouse_body2_x', 'mouse_body2_y', 'mouse_body3_x', 'mouse_body3_y', \n",
    "#                    'mouse_base_x', 'mouse_base_y', 'mouse_speed'\n",
    "#                   , 'cricket_0_x', 'cricket_0_y','cricket_0_mouse_distance',\n",
    "#                   'cricket_0_delta_heading', 'cricket_0_visual_angle']\n",
    "# target_behavior = ['cricket_0_mouse_distance', 'cricket_0_x', 'cricket_0_y', 'cricket_0_visual_angle']\n",
    "\n",
    "target_behavior = ['mouse_x', 'mouse_y', 'mouse_head_x', 'mouse_head_y', \n",
    "                   'mouse_body2_x', 'mouse_body2_y', 'mouse_body3_x', 'mouse_body3_y', \n",
    "                   'mouse_base_x', 'mouse_base_y', 'mouse_speed',\n",
    "                   'cricket_0_mouse_distance', 'cricket_0_x', 'cricket_0_y', \n",
    "                   'cricket_0_delta_heading', 'cricket_0_speed']\n",
    "# target_behavior = ['cricket_0_mouse_distance']\n",
    "# target_behavior = ['mouse_x', 'mouse_y']\n",
    "# 'cricket_0_x', 'cricket_0_y','cricket_0_mouse_distance'\n",
    "# define the percentage of each trial to use as test\n",
    "test_perc = 0.2\n",
    "\n",
    "# allocate memory for the train and test ca and behavior data\n",
    "beh_train = []\n",
    "beh_test = []\n",
    "ca_train = []\n",
    "ca_test = []\n",
    "\n",
    "# for all the trials\n",
    "for idx, trial in enumerate(normal_data + mod_data):\n",
    "#     # get the table\n",
    "#     sub_data = pd.concat(normal_data)\n",
    "\n",
    "    # get the available columns\n",
    "    labels = list(trial.columns)\n",
    "    cells = [el for el in labels if 'cell' in el]\n",
    "    # get the cell data\n",
    "    calcium_data = np.array(trial[cells].copy())\n",
    "    # get rid of the super small values\n",
    "    calcium_data[np.isnan(calcium_data)] = 0\n",
    "\n",
    "    try:\n",
    "        # get the parameter\n",
    "        beh_data = trial[target_behavior].to_numpy()\n",
    "\n",
    "        # smooth the parameter\n",
    "        beh_data = ss.medfilt(beh_data, (21, 1))\n",
    "    except KeyError:\n",
    "        continue\n",
    "    # filter the data\n",
    "#     # remove the repeated parameter bins (due to redundancy)\n",
    "#     keep_vector = np.pad(np.diff(beh_data[:, 0])!=0, (1, 0), mode='constant', constant_values=0)\n",
    "    \n",
    "#     # limit the parameter based on field of view\n",
    "#     sec_parameter = trial.loc[:, 'cricket_0_delta_heading'].to_numpy()\n",
    "#     parameter_range = (20 > sec_parameter)  #& (sec_parameter > -90)\n",
    "#     keep_vector = (keep_vector) & (parameter_range)\n",
    "    \n",
    "    # limit the parameter based on distance to prey\n",
    "#     distance_to_prey = trial.loc[:, 'cricket_0_mouse_distance'].to_numpy()\n",
    "#     distance_vector = distance_to_prey > 3\n",
    "#     keep_vector = (keep_vector) & (distance_vector)\n",
    "    \n",
    "#     # perform the removal\n",
    "#     beh_data = beh_data[keep_vector, :]\n",
    "#     calcium_data = calcium_data[keep_vector, :]\n",
    "    # skip if empty\n",
    "    if calcium_data.shape[0] == 0:\n",
    "        continue\n",
    "    \n",
    "    downsamp = 1\n",
    "    # bin the data\n",
    "    if downsamp > 1:\n",
    "        beh_data = ss.decimate(beh_data, downsamp, axis=0)\n",
    "        calcium_data = ss.decimate(calcium_data, downsamp, axis=0)\n",
    "\n",
    "    # get the threshold index\n",
    "    threshold_idx = int(calcium_data.shape[0]*(test_perc))\n",
    "    # split the data\n",
    "    ca_trial_train = calcium_data[threshold_idx:, :] \n",
    "    ca_trial_test = calcium_data[:threshold_idx, :] \n",
    "    beh_trial_train = beh_data[threshold_idx:, :]\n",
    "    beh_trial_test = beh_data[:threshold_idx, :] \n",
    "    \n",
    "    # store the data\n",
    "    ca_train.append(ca_trial_train)\n",
    "    ca_test.append(ca_trial_test)\n",
    "    beh_train.append(beh_trial_train)\n",
    "    beh_test.append(beh_trial_test)\n",
    "\n",
    "# scale the data\n",
    "ca_scaler = preprocessing.StandardScaler().fit(np.concatenate(ca_train))\n",
    "beh_scaler = preprocessing.StandardScaler().fit(np.concatenate(beh_train))\n",
    "\n",
    "ca_train = [ca_scaler.transform(el) for el in ca_train]\n",
    "ca_test = [ca_scaler.transform(el) for el in ca_test]\n",
    "beh_train = [beh_scaler.transform(el) for el in beh_train]\n",
    "beh_test = [beh_scaler.transform(el) for el in beh_test]\n",
    "\n",
    "# train the PSID model\n",
    "idSys = PSID.PSID(ca_train, beh_train, nx=8, n1=8, i=20)\n",
    "# idSys = PSID.PSID(ca_train, beh_train, nx=1, n1=1, i=20) # for cricket distance\n",
    "# idSys = PSID.PSID(ca_train, beh_train, nx=20, n1=10, i=35)\n",
    "\n",
    "# allocate memory for the predictions\n",
    "beh_pred = []\n",
    "ca_pred = []\n",
    "latent_pred = []\n",
    "# predict each trial\n",
    "for trial in ca_test:\n",
    "    beh_p, ca_p, latent_p = idSys.predict(trial)\n",
    "    beh_pred.append(beh_p)\n",
    "    ca_pred.append(ca_p)\n",
    "    latent_pred.append(latent_p)\n",
    "\n",
    "combo_beh_test = np.vstack(beh_test)\n",
    "combo_beh_pred = np.vstack(beh_pred)\n",
    "\n",
    "combo_ca_test = np.vstack(ca_test)\n",
    "combo_ca_pred = np.vstack(ca_pred)\n",
    "    \n",
    "R2TrialBased_beh = evalPrediction(combo_beh_test, combo_beh_pred, 'CC')\n",
    "R2TrialBased_ca = evalPrediction(combo_ca_test, combo_ca_pred, 'CC')\n",
    "\n",
    "print(np.sum(R2TrialBased_ca>0))\n",
    "print(np.nanmean(R2TrialBased_ca))\n",
    "print(R2TrialBased_beh)\n",
    "# print(R2TrialBased_ca)\n",
    "\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e6d21d",
   "metadata": {},
   "source": [
    "# plot the PSID result compared to the variable of interest\n",
    "hv.extension('bokeh')\n",
    "combo_latent = np.vstack(latent_pred)\n",
    "\n",
    "# get the x axis\n",
    "x = np.array(np.arange(combo_beh_test.shape[0]))\n",
    "\n",
    "c1 = hv.Curve((x, combo_beh_test[:, 0])).opts(width=800, title='Test vs predicted behavior')\n",
    "c2 = hv.Curve((x, combo_beh_pred[:, 0]))\n",
    "\n",
    "if combo_latent.shape[1] == 1:\n",
    "    c3 = hv.Curve((x, combo_latent[:, 0])).opts(shared_axes=False, width=800, title='Latents')\n",
    "else:\n",
    "    c3 = hv.Image(combo_latent.T).opts(shared_axes=False, width=800, \n",
    "                                       cmap='viridis', tools=['hover'], title='Latents')\n",
    "if combo_beh_test.shape[1] == 1:\n",
    "    c4 = hv.Curve((x, combo_beh_test[:, 0])).opts(axiswise=True, \n",
    "                                                  width=800, tools=['hover'], title='Test behavior')\n",
    "    c5 = hv.Curve((x, combo_beh_pred[:, 0])).opts(shared_axes=False, \n",
    "                                                  width=800, tools=['hover'], title='Predicted behavior')\n",
    "else:\n",
    "    c4 = hv.Image(combo_beh_test.T).opts(axiswise=True, \n",
    "                                         width=800, cmap='viridis', tools=['hover'], \n",
    "                                         title='Test behavior')\n",
    "    c5 = hv.Image(combo_beh_pred.T).opts(shared_axes=False, \n",
    "                                         width=800, cmap='viridis', \n",
    "                                         tools=['hover'], title='Predicted behavior')\n",
    "c6 = hv.Image(combo_ca_test.T).opts(shared_axes=False, \n",
    "                                    width=800, cmap='viridis', tools=['hover'], title='Test calcium')\n",
    "c7 = hv.Image(combo_ca_pred.T).opts(shared_axes=False, \n",
    "                                    width=800, cmap='viridis', tools=['hover'], title='Predicted calcium')\n",
    "\n",
    "(c1*c2+c3+c4+c5+c6+c7).cols(1)\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d187a642",
   "metadata": {},
   "source": [
    "# run the model on all experiments\n",
    "\n",
    "# allocate memory for the predictions\n",
    "final_beh = []\n",
    "final_ca = []\n",
    "final_latent = []\n",
    "\n",
    "# predict each trial\n",
    "for trial in normal_data + mod_data:\n",
    "    \n",
    "    # get the available columns\n",
    "    labels = list(trial.columns)\n",
    "    cells = [el for el in labels if 'cell' in el]\n",
    "    # get the cell data\n",
    "    calcium_data = np.array(trial[cells].copy())\n",
    "    # get rid of the super small values\n",
    "    calcium_data[np.isnan(calcium_data)] = 0\n",
    "    \n",
    "    beh_p, ca_p, latent_p = idSys.predict(calcium_data)\n",
    "    final_beh.append(beh_p)\n",
    "    final_ca.append(ca_p)\n",
    "    final_latent.append(latent_p)\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d2a26e",
   "metadata": {},
   "source": [
    "# plot the PSID dynamics reconstruction\n",
    "hv.extension('plotly')\n",
    "    \n",
    "# opts.defaults(opts.Scatter3D(cmap='viridis'))\n",
    "# define the target parameter\n",
    "target_parameter = 'mouse_speed'\n",
    "print(normal_data[0].columns[:40])\n",
    "\n",
    "# define the target dimensions\n",
    "tar_dim = np.array([0, 1, 2]) + 0\n",
    "\n",
    "# allocate memory for the output list\n",
    "plot_list = []\n",
    "\n",
    "target_trial = [0, 8]\n",
    "for idx, trial in enumerate(final_latent[target_trial[0]:target_trial[1]+1]):\n",
    "# for idx, trial in enumerate(final_latent):\n",
    "    \n",
    "    # get a parameter\n",
    "    try:\n",
    "        parameter = normal_data[idx][target_parameter]\n",
    "        parameter = (parameter-parameter.min())/(parameter.max()-parameter.min())\n",
    "#         parameter = np.log(parameter)\n",
    "    \n",
    "        plot_list.append(hv.Scatter3D((trial[:, tar_dim[0]], trial[:, tar_dim[1]], trial[:, tar_dim[2]])))\n",
    "        plot_list[idx].opts(size=2, height=800, width=800, color=parameter, colorbar=True)\n",
    "#     plot_list[idx].opts(hv.opts.Scatter3D(cmap='viridis'))\n",
    "    except KeyError:\n",
    "#             parameter = np.zeros(trial.shape[0])\n",
    "        continue\n",
    "\n",
    "ov = hv.Overlay(plot_list)\n",
    "ov\n",
    "# hv.help(hv.Scatter3D)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d773e9",
   "metadata": {},
   "source": [
    "# plot individual latents\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e411ca9",
   "metadata": {},
   "source": [
    "#\n",
    "# Embed with UMAP\n",
    "transformed_data = np.concatenate(final_latent)\n",
    "# embed the data via UMAP\n",
    "reducer = umap.UMAP(min_dist=0.1, n_neighbors=30)\n",
    "embedded_data = reducer.fit_transform(transformed_data)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db86fcca",
   "metadata": {},
   "source": [
    "# Create the UMAP labels\n",
    "# allocate memory for the distances\n",
    "distance_list = []\n",
    "target_key = 'cricket_0_mouse_distance'\n",
    "\n",
    "# define the interval to take from the edges\n",
    "# edges = [7, -14] # for bins = 14\n",
    "# edges = [15, -15] # for bins = 30\n",
    "\n",
    "# for all the files\n",
    "for idx, files in enumerate(normal_data + mod_data):\n",
    "#     temp_values = np.log(files.index[edges[0]:edges[1]])\n",
    "#     temp_values = (files['time_vector'][edges[0]:edges[1]]/np.max(files['time_vector'][edges[0]:edges[1]]))\n",
    "#     temp_values = files['mouse_speed'][edges[0]:edges[1]]\n",
    "    if target_key in files.keys():\n",
    "        temp_values = files[target_key]\n",
    "#         temp_values = np.log(ss.medfilt(files[target_key], 41))\n",
    "#         print(temp_values.shape)\n",
    "#         temp_values = np.ones(files.shape[0])*idx\n",
    "#         temp_values = np.log(files[target_key])\n",
    "#         temp_values[temp_values>10] = 10\n",
    "\n",
    "        temp_values[np.isinf(temp_values)] = 0\n",
    "#         snout = files[['mouse_snout_x', 'mouse_snout_y']].to_numpy()[edges[0]:edges[1], :]\n",
    "#         tail = files[['mouse_base_x', 'mouse_base_y']].to_numpy()[edges[0]:edges[1], :]\n",
    "#         temp_values = fk.distance_calculation(snout, tail)\n",
    "#         temp_values[temp_values>10] = 10\n",
    "\n",
    "    else:\n",
    "#         temp_values = np.zeros_like(files['mouse_x'][edges[0]:edges[1]])\n",
    "        temp_values = np.zeros_like(files['mouse_x'])\n",
    "#     temp_values[temp_values==0] = np.nan\n",
    "    distance_list.append(temp_values)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b892ade",
   "metadata": {},
   "source": [
    "# plot the UMAP results\n",
    "hv.extension('bokeh')\n",
    "\n",
    "# define the interval between points\n",
    "interv = 2\n",
    "\n",
    "compiled_labels = np.expand_dims(np.hstack(distance_list), axis=1)\n",
    "\n",
    "umap_data = np.concatenate((embedded_data,compiled_labels),axis=1)\n",
    "\n",
    "compiled_labels = compiled_labels[::interv]\n",
    "umap_data = umap_data[::interv, :]\n",
    "\n",
    "\n",
    "umap_plot = hv.Scatter(umap_data, vdims=['Dim 2',target_key], kdims=['Dim 1'])\n",
    "umap_plot.opts(color=target_key, colorbar=True, cmap='Spectral', size=3)\n",
    "\n",
    "umap_plot.opts(height=600, width=800)\n",
    "\n",
    "umap_plot\n",
    "\n",
    "# umap_plot.opts(opts.Scatter(width=fp.pix(5.7), height=fp.pix(7.8), toolbar=None, \n",
    "#                         hooks=[fp.margin], fontsize=fp.font_sizes['small'], xticks=3, yticks=3))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1f661b",
   "metadata": {},
   "source": [
    "# Calculate pairwise distances\n",
    "\n",
    "# define the behavioral variables\n",
    "# target_behavior = ['cricket_0_mouse_distance', 'cricket_0_speed', 'cricket_0_visual_angle',\n",
    "#                   'cricket_0_delta_heading']\n",
    "\n",
    "target_behavior = ['mouse_x', 'mouse_y', 'mouse_head_x', 'mouse_head_y', \n",
    "                   'mouse_body2_x', 'mouse_body2_y', 'mouse_body3_x', 'mouse_body3_y', \n",
    "                   'mouse_base_x', 'mouse_base_y', 'mouse_speed']\n",
    "# define the distance metric\n",
    "distance_metric = 'manhattan'\n",
    "\n",
    "# allocate memory for the distances\n",
    "distance_list = []\n",
    "# for all the trials\n",
    "for trial in normal_data:\n",
    "    # get the behavioral variables\n",
    "    behavior = trial[target_behavior].to_numpy()\n",
    "    # get the calcium data\n",
    "    labels = list(sub_data.columns)\n",
    "    cells = [el for el in labels if 'cell' in el]\n",
    "    calcium = trial[cells].to_numpy()\n",
    "    \n",
    "    # downsample\n",
    "    downsamp = 2\n",
    "    # bin the data\n",
    "    if downsamp > 1:\n",
    "        behavior = ss.decimate(behavior, downsamp, axis=0)\n",
    "        calcium = ss.decimate(calcium, downsamp, axis=0)\n",
    "    \n",
    "    # calculate the respective distances\n",
    "    behavior_distance = smet.pairwise_distances(behavior, metric=distance_metric)\n",
    "    behavior_distance = behavior_distance[np.triu_indices(behavior_distance.shape[0], 1)]\n",
    "    calcium_distance = smet.pairwise_distances(calcium, metric=distance_metric)\n",
    "    calcium_distance = calcium_distance[np.triu_indices(calcium_distance.shape[0], 1)]\n",
    "    # store\n",
    "    distance_list.append((behavior_distance, calcium_distance))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab2ceca",
   "metadata": {},
   "source": [
    "# plot the distances\n",
    "\n",
    "# allocate the plot list\n",
    "plot_list = []\n",
    "# define the density of sampling\n",
    "sampling = 10\n",
    "# for every trial\n",
    "for trial in distance_list:\n",
    "    \n",
    "#     curr_plot = hv.Scatter((trial[0][::sampling], trial[1][::sampling]))\n",
    "#     curr_plot.opts(width=800, height=800, logx=True)\n",
    "#     x = trial[0][::sampling]\n",
    "#     y = trial[1][::sampling]\n",
    "    \n",
    "    x = np.log(trial[0][::sampling])\n",
    "    y = np.log(trial[1][::sampling])\n",
    "    x[np.isinf(x)] = 0\n",
    "    y[np.isinf(y)] = 0\n",
    "    \n",
    "#     np.random.shuffle(y)\n",
    "\n",
    "    H, xedges, yedges = np.histogram2d(x, y, 100)\n",
    "    curr_plot = hv.Image(np.flipud(H))\n",
    "    curr_plot.opts(height=400, width=400, cmap='Viridis')\n",
    "    plot_list.append(curr_plot)\n",
    "\n",
    "# plot\n",
    "hv.Layout(plot_list).cols(2)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0124c04",
   "metadata": {},
   "source": [
    "hv.extension('bokeh')\n",
    "# im_real = hv.Image(calcium_data.T).opts(width=800, cmap='viridis', tools=['hover'])\n",
    "# im_real\n",
    "target_behavior = ['mouse_x', 'mouse_y', 'mouse_head_x', 'mouse_head_y', \n",
    "                   'mouse_body2_x', 'mouse_body2_y', 'mouse_body3_x', 'mouse_body3_y', \n",
    "                   'mouse_base_x', 'mouse_base_y', 'mouse_speed']\n",
    "# hv.Curve(calcium_data[:, 0])\n",
    "all_behavior = np.concatenate([el[target_behavior].to_numpy() for el in normal_data], axis=0)\n",
    "print(all_behavior.shape)\n",
    "all_behavior = np.array([(el-np.nanmin(el))/(np.nanmax(el)-np.nanmin(el)) for el in all_behavior]).T\n",
    "\n",
    "hv.Image(all_behavior).opts(width=800, cmap='viridis').redim.range(z=(0, 0.9))\n",
    "\n",
    "# print(np.sum(np.isnan(calcium_data)))\n",
    "# im_last = hv.Image(calcium_last.T).opts(width=800, cmap='viridis')\n",
    "\n",
    "# im_combo = (im_real + im_last).cols(1)\n",
    "# im_combo"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588abb74",
   "metadata": {},
   "source": [
    "# Plot single cell \n",
    "\n",
    "all_calcium\n",
    "\n",
    "hv.Image(all_behavior).opts(width=800, cmap='viridis').redim.range(z=(0, 0.9))\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d28c9f",
   "metadata": {},
   "source": [
    "# Calculate the per cell rev correlation\n",
    "\n",
    "# define the interval for calculation in frames\n",
    "target_interval = 10\n",
    "# define the target behavioral variable\n",
    "# target_behavior = 'cricket_0_mouse_distance'\n",
    "target_behavior = ['mouse_x', 'mouse_y', 'mouse_head_x', 'mouse_head_y', \n",
    "                   'mouse_body2_x', 'mouse_body2_y', 'mouse_body3_x', 'mouse_body3_y', \n",
    "                   'mouse_base_x', 'mouse_base_y', 'mouse_speed',\n",
    "                   'cricket_0_x', 'cricket_0_y','cricket_0_mouse_distance',\n",
    "                   'cricket_0_delta_heading', 'cricket_0_visual_angle']\n",
    "\n",
    "# get the behavioral data\n",
    "# current_behavior = np.expand_dims(data.loc[:, target_behavior].to_numpy(), axis=1)\n",
    "current_behavior = data.loc[:, target_behavior].to_numpy()\n",
    "beh_columns = len(target_behavior)\n",
    "\n",
    "# concatenate all the data\n",
    "data = pd.concat(normal_data, axis=0)\n",
    "\n",
    "# get the available columns\n",
    "labels = list(data.columns)\n",
    "cells = [el for el in labels if 'cell' in el]\n",
    "# get the cell data\n",
    "calcium_data = np.array(data[cells].copy()).T\n",
    "# scale for min and max\n",
    "calcium_data = (calcium_data - np.nanmin(calcium_data))/\\\n",
    "               (np.nanmax(calcium_data) - np.nanmin(calcium_data))\n",
    "# get rid of the nan values\n",
    "calcium_data[np.isnan(calcium_data)] = 0\n",
    "# get the number of time points\n",
    "time_number = calcium_data.shape[1]\n",
    "# get the number of cells\n",
    "roi_number = calcium_data.shape[0]\n",
    "\n",
    "# pad the behavior data\n",
    "# padded_behavior = np.vstack((np.zeros((target_interval, 1)), current_behavior)).T\n",
    "\n",
    "padded_behavior = np.vstack((np.zeros((target_interval, beh_columns)), current_behavior, \n",
    "                             np.zeros((target_interval, beh_columns)))).T\n",
    "# remove nans (heads up, there are a lot)\n",
    "padded_behavior[np.isnan(padded_behavior)] = 0\n",
    "\n",
    "# allocate memory to save the ols\n",
    "ols_list = []\n",
    "prediction_list = []\n",
    "r2_list = []\n",
    "# for all the cells\n",
    "for cells in np.arange(roi_number-36):\n",
    "    print(f'current cell: {cells}')\n",
    "    # allocate memory for the padded matrix\n",
    "#     time_matrix = np.zeros((time_number, target_interval))\n",
    "    time_matrix = np.zeros((time_number, 2*target_interval*beh_columns))\n",
    "    # get the calcium data\n",
    "    current_calcium = calcium_data[cells:cells+1, :].T\n",
    "        \n",
    "    # fill the padded matrix\n",
    "    for frame, el in enumerate(current_calcium):\n",
    "#         time_matrix[frame, :] = padded_behavior[:, frame:frame+target_interval]\n",
    "        time_matrix[frame, :] = padded_behavior[:, frame:frame+2*target_interval].flatten()\n",
    "#     # calculate the covariance matrix\n",
    "#     cov_matrix = time_matrix.T@time_matrix\n",
    "#     # calculate the STA\n",
    "#     sta = time_matrix.T@current_calcium\n",
    "#     try:\n",
    "#         # calculate the ols estimate\n",
    "#         ols = np.linalg.inv(cov_matrix) @ sta   \n",
    "#     except np.linalg.LinAlgError:\n",
    "#         ols = []\n",
    "        \n",
    "# ols_list.append(ols)\n",
    "#     ols_estimator = lin.PoissonRegressor(alpha=10, max_iter=5000, fit_intercept=False)\n",
    "    ols_estimator = lin.TweedieRegressor(alpha=0, max_iter=10000, fit_intercept=True, power=1)\n",
    "    ols_estimator.fit(time_matrix, current_calcium.ravel())\n",
    "    # save the ols\n",
    "    ols_list.append(ols_estimator.coef_)\n",
    "    current_prediction = ols_estimator.predict(time_matrix)\n",
    "    print(current_prediction)\n",
    "    prediction_list.append(current_prediction)\n",
    "    r2_list.append(smet.r2_score(current_calcium, current_prediction))\n",
    "    "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d96877",
   "metadata": {},
   "source": [
    "# plot the input data\n",
    "print(current_behavior.shape)\n",
    "hv.extension('bokeh')\n",
    "im = hv.Image(current_behavior[:10000, :].T).opts(width=1000, height=400)\n",
    "\n",
    "\n",
    "im"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e7eba5",
   "metadata": {},
   "source": [
    "# plot the fit quality\n",
    "\n",
    "freq, edges = np.histogram(r2_list)\n",
    "hv.Bars((edges, freq)).opts(width=800, tools=['hover'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7e7c3b",
   "metadata": {},
   "source": [
    "# Plot the OLS\n",
    "\n",
    "# allocate memory for the cells\n",
    "cell_dict = {}\n",
    "prediction_dict = {}\n",
    "\n",
    "# generate the x axis vector\n",
    "# x = range(target_interval)\n",
    "x_pred = range(current_behavior.shape[0])\n",
    "x = range(2*target_interval)\n",
    "# for all the cells\n",
    "for cells in np.arange(roi_number-36):\n",
    "    \n",
    "    # generate the plot and save\n",
    "#     cell_plot = hv.Curve((x, ols_list[cells].flatten()), kdims=['Time'], vdims=target_behavior)\n",
    "#     cell_plot.opts(shared_axes=False, width=600)\n",
    "    \n",
    "    current_calcium = calcium_data[cells:cells+1, :].ravel()\n",
    "\n",
    "    prediction_plot = hv.Curve((x_pred, current_calcium)) * \\\n",
    "        hv.Curve((x_pred, prediction_list[cells]))\n",
    "    prediction_plot.opts(shared_axes=False, width=600)\n",
    "    # save in a dict\n",
    "#     cell_dict[cells] = cell_plot\n",
    "    prediction_dict[cells] = prediction_plot\n",
    "\n",
    "cell_holomap = hv.HoloMap(prediction_dict)\n",
    "cell_holomap"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7cdf59",
   "metadata": {},
   "source": [
    "# train on the mod data\n",
    "\n",
    "# define the target variable\n",
    "# target_behavior = ['cricket_0_x', 'cricket_0_y']\n",
    "# target_behavior = ['mouse_x', 'mouse_y']\n",
    "target_behavior = ['cricket_0_mouse_distance']\n",
    "direct_label = 0\n",
    "# define the number of bins\n",
    "bin_number = 5\n",
    "# print(str(mouse)+str(day))\n",
    "# get the table\n",
    "# sub_data = data[mouse][day]\n",
    "sub_data = mod_data\n",
    "# get the available columns\n",
    "labels = list(sub_data.columns)\n",
    "cells = [el for el in labels if 'cell' in el]\n",
    "not_cells = [el for el in labels if 'cell' not in el]\n",
    "# get the cell data\n",
    "calcium_data_mod = np.array(sub_data[cells].copy())\n",
    "# get rid of the super small values\n",
    "calcium_data_mod[np.isnan(calcium_data_mod)] = 0\n",
    "\n",
    "# if calcium_data.shape[0] == 0:\n",
    "#     continue\n",
    "# scale (convert to float to avoid warning, potentially from using too small a dtype)\n",
    "calcium_data_mod = preprocessing.StandardScaler().fit_transform(calcium_data_mod)\n",
    "\n",
    "# get the distance to cricket\n",
    "# distance = ss.medfilt(sub_data.loc[:, target_behavior].to_numpy(), 21)\n",
    "parameter = sub_data.loc[:, target_behavior].to_numpy()\n",
    "keep_vector = np.pad(np.diff(parameter[:, 0])!=0, (1, 0), mode='constant', constant_values=0)\n",
    "\n",
    "# distance_to_prey = sub_data.loc[:, 'cricket_0_mouse_distance'].to_numpy()\n",
    "# distance_vector = distance_to_prey>3\n",
    "# keep_vector = (keep_vector) & (distance_vector)\n",
    "\n",
    "parameter = parameter[keep_vector, :]\n",
    "calcium_data_mod = calcium_data_mod[keep_vector, :]\n",
    "\n",
    "# random.shuffle(parameter)\n",
    "\n",
    "\n",
    "# # shufle the label vector\n",
    "# random.shuffle(label_vector)\n",
    "# random.shuffle(distance)\n",
    "\n",
    "# train the classifier\n",
    "\n",
    "calcium_train, calcium_test, parameter_train, parameter_test = \\\n",
    "    train_test_split(calcium_data_mod, parameter, test_size=0.2)\n",
    "\n",
    "linear_mod = lin.MultiTaskElasticNetCV(max_iter=5000, l1_ratio=[.1, .5, .7, .9, .95, .99, 1], n_jobs=7)\n",
    "linear_mod.fit(calcium_train, parameter_train)\n",
    "linear_pred_mod = linear_mod.predict(calcium_test)\n",
    "\n",
    "\n",
    "print(calcium_data_mod.shape)\n",
    "\n",
    "\n",
    "print('Exp variance:'+str(smet.r2_score(parameter_test, linear_pred_mod)))\n",
    "print(linear_mod.alpha_)\n",
    "print(linear_mod.l1_ratio_)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ae9923",
   "metadata": {},
   "source": [
    "# predict dark position with light decoder\n",
    "\n",
    "linear_pred_mod = linear_mod.predict(calcium_data_mod)\n",
    "linear_pred_combo = linear.predict(calcium_data_mod)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125fc540",
   "metadata": {},
   "source": [
    "# Plot the prediction over time\n",
    "\n",
    "# get the number of time points\n",
    "time_number = linear_pred.shape[0]\n",
    "# define the x axis\n",
    "x_axis = np.array(range(time_number))\n",
    "# print(time_number)\n",
    "\n",
    "# normalized_param = parameter_test\n",
    "parameter = mod_data.loc[:, target_behavior].to_numpy()\n",
    "\n",
    "# normalized_param = normalized_param/np.max(normalized_param)\n",
    "\n",
    "# normalized_prediction = ss.medfilt(bins[linear_pred], 21)\n",
    "# normalized_prediction = ss.medfilt(bins[linear_pred], 21)\n",
    "# normalized_prediction = ss.medfilt(linear_pred, 21)\n",
    "# normalized_prediction = linear_pred.astype(float)\n",
    "normalized_prediction = ss.medfilt(linear_pred.astype(float), (1, 1))\n",
    "\n",
    "# normalized_prediction = normalized_prediction/np.max(normalized_prediction)\n",
    "# normalized_prediction = ss.medfilt(linear_pred, 21)\n",
    "\n",
    "# normalized_labels = ss.medfilt(bins[label_vector], 21)\n",
    "\n",
    "# quadrant_info = sub_data.loc[keep_vector, 'cricket_0_quadrant']\n",
    "quadrant_info = sub_data.loc[:, 'cricket_0_quadrant'].to_numpy()\n",
    "quadrant_info = quadrant_info[keep_vector]\n",
    "# quadrant_info = ss.medfilt(quadrant_info/np.max(quadrant_info),21)\n",
    "\n",
    "distance_info = distance_to_prey[keep_vector]\n",
    "# # filter the prediction by the quadrant\n",
    "# normalized_vis = normalized_prediction.copy()\n",
    "# normalized_vis[quadrant_info>0] = np.nan\n",
    "# normalized_nonvis = normalized_prediction.copy()\n",
    "# normalized_nonvis[quadrant_info<1] = np.nan\n",
    "\n",
    "# angle_info = sub_data.loc[keep_vector, 'cricket_0_visual_angle']\n",
    "# angle_info = ss.medfilt(angle_info/np.max(angle_info),21)\n",
    "\n",
    "# speed_info = sub_data.loc[keep_vector, 'mouse_speed']\n",
    "# speed_info = ss.medfilt(speed_info/np.max(speed_info),21)\n",
    "\n",
    "heading_info = sub_data.loc[:, 'cricket_0_delta_heading'].to_numpy()\n",
    "heading_info = heading_info[keep_vector]\n",
    "\n",
    "# prediction = hv.Curve((range(time_number), normalized_prediction)).opts(height=600, width=800)\n",
    "# prediction_vis = hv.Curve((range(time_number), normalized_vis)).opts(height=600, width=800)\n",
    "# prediction_nonvis = hv.Curve((range(time_number), normalized_nonvis)).opts(height=600, width=800)\n",
    "parameter1 = hv.Curve((x_axis, normalized_param[:, 0])).opts(height=300, width=800)\n",
    "parameter2 = hv.Curve((x_axis, normalized_param[:, 1])).opts(height=300, width=800)\n",
    "\n",
    "prediction1 = hv.Curve((x_axis, normalized_prediction[:, 0])).opts(height=300, width=800)\n",
    "prediction2 = hv.Curve((x_axis, normalized_prediction[:, 1])).opts(height=300, width=800)\n",
    "# labels = hv.Curve((range(time_number), normalized_labels)).opts(height=600, width=800)\n",
    "mse_info = np.sqrt((normalized_param - normalized_prediction)**2)\n",
    "mse1 = hv.Curve((x_axis, mse_info[:, 0])).opts(height=300, width=800)\n",
    "# quadrant = hv.Curve((range(time_number), quadrant_info)).opts(height=600, width=800)\n",
    "# quadrant = hv.Spikes(x_axis[quadrant_info==0]).opts(alpha=.3, spike_length=40)\n",
    "# distance = hv.Curve((x_axis, distance_info))\n",
    "# heading = hv.Curve((x_axis, heading_info))\n",
    "# angle = hv.Curve((range(time_number), angle_info)).opts(height=600, width=800, tools=['hover'])\n",
    "# speed = hv.Curve((range(time_number), speed_info)).opts(height=600, width=800)\n",
    "\n",
    "# prediction+parameter\n",
    "# compiled = parameter*prediction#*angle*speed\n",
    "# compiled = parameter*prediction_vis*prediction_nonvis\n",
    "# compiled = mse1*quadrant*distance*heading + parameter2*prediction2*quadrant\n",
    "# compiled = parameter1*prediction1 + parameter2*prediction2\n",
    "scatter_x = hv.Scatter((normalized_param[:, 0], normalized_prediction[:, 0]), kdims=['Real', 'Predicted'])\n",
    "scatter_y = hv.Scatter((normalized_param[:, 1], normalized_prediction[:, 1]), kdims=['Real', 'Predicted'])\n",
    "compiled = parameter1*prediction1 + parameter2*prediction2 + scatter_x + scatter_y\n",
    "compiled.cols(1)\n",
    "compiled"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-prey_capture] *",
   "language": "python",
   "name": "conda-env-.conda-prey_capture-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
