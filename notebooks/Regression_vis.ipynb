{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d08368",
   "metadata": {},
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath(r'D:\\Code Repos\\prey_capture'))\n",
    "\n",
    "import panel as pn\n",
    "import holoviews as hv\n",
    "from holoviews import opts, dim\n",
    "hv.extension('bokeh')\n",
    "from bokeh.resources import INLINE\n",
    "\n",
    "import paths\n",
    "import functions_plotting as fp\n",
    "import functions_loaders as fl\n",
    "import functions_bondjango as bd\n",
    "import processing_parameters\n",
    "\n",
    "import importlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import scipy.stats as stat\n",
    "import sklearn.preprocessing as prep\n",
    "from holoviews import Store\n",
    "import datetime\n",
    "import umap\n",
    "from rastermap import Rastermap\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c139afe8",
   "metadata": {},
   "source": [
    "# set up the figure config\n",
    "importlib.reload(fp)\n",
    "importlib.reload(processing_parameters)\n",
    "# define the target saving path\n",
    "save_path = os.path.join(paths.figures_path, 'Regression_vis')\n",
    "\n",
    "# define the printing mode\n",
    "save_mode = True\n",
    "# define the target document\n",
    "target_document = 'paper'\n",
    "# set up the figure theme\n",
    "fp.set_theme()\n",
    "# load the label dict\n",
    "label_dict = processing_parameters.label_dictionary\n",
    "variable_list = processing_parameters.variable_list"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1efecb",
   "metadata": {},
   "source": [
    "def match_cells(files):\n",
    "    # assemble the preproc path\n",
    "    files_preproc = files.replace('_combinedanalysis', '_preproc')\n",
    "    # open the file\n",
    "    with pd.HDFStore(files_preproc, 'r') as preproc:\n",
    "        if '/cell_matches' in preproc.keys():\n",
    "            # get the matches\n",
    "            cell_matches = preproc['cell_matches']\n",
    "\n",
    "            print(animal, day_s, files_preproc)\n",
    "            # get the idx for this file\n",
    "            current_matches = cell_matches[datetime.datetime.strftime(day, '%m_%d_%Y')].to_numpy()\n",
    "            current_idx = np.argsort(current_matches).astype(float)\n",
    "            # remove the nan entries\n",
    "            current_idx = current_idx[~np.isnan(np.sort(current_matches))]\n",
    "    return current_idx"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91f0664",
   "metadata": {},
   "source": [
    "# get the entries\n",
    "\n",
    "# Load the desired files\n",
    "importlib.reload(processing_parameters)\n",
    "\n",
    "# load the constants from the regression calculation\n",
    "time_shifts = processing_parameters.time_shifts\n",
    "shift_dict = {el: idx for idx, el in enumerate(time_shifts)}\n",
    "shift_number = len(time_shifts)\n",
    "shuffles = processing_parameters.regression_repeats\n",
    "\n",
    "# load the variable list\n",
    "variable_list = processing_parameters.variable_list\n",
    "# assemble the dataframe columns\n",
    "reals = ['real_'+str(el) for el in time_shifts]\n",
    "shuffle_means = ['smean_'+str(el) for el in time_shifts]\n",
    "columns = reals + shuffle_means + ['mouse', 'day']\n",
    "\n",
    "# get the search list\n",
    "search_list = processing_parameters.search_list\n",
    "\n",
    "# allocate a list for all paths (need to preload to get the dates)\n",
    "all_paths = []\n",
    "all_results = []\n",
    "# for all the search strings\n",
    "for search_string in search_list:\n",
    "\n",
    "    # query the database for data to plot\n",
    "    data_all = bd.query_database('analyzed_data', search_string)\n",
    "    data_path = [el['analysis_path'] for el in data_all if '_combinedanalysis' in el['slug']]\n",
    "    data_result = [el['result'] for el in data_all if '_combinedanalysis' in el['slug']]\n",
    "    all_paths.append(data_path)\n",
    "    all_results.append(data_result)\n",
    "# get the dates present\n",
    "data_dates = np.unique([os.path.basename(el)[:10] for el in np.concatenate(all_paths)])\n",
    "print(f'Dates present: {data_dates}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5481d2f",
   "metadata": {},
   "source": [
    "# main loop\n",
    "\n",
    "# allocate the outputs\n",
    "correlations = []\n",
    "# get the regression types\n",
    "regressors = processing_parameters.regressors\n",
    "\n",
    "# for all the list items\n",
    "for idx0, data_path in enumerate(all_paths):\n",
    "\n",
    "    # for all the files\n",
    "    for idx1, files in enumerate(data_path):\n",
    "        \n",
    "        # if a habi trial, skip\n",
    "        if 'habi' in files:\n",
    "            continue\n",
    "        \n",
    "        # get the animal and date from the slug\n",
    "        name_parts = os.path.basename(files).split('_')\n",
    "        animal = '_'.join(name_parts[7:10])\n",
    "        day_s = '_'.join(name_parts[:3])\n",
    "        time_s = '_'.join(name_parts[3:6])\n",
    "        day = datetime.datetime.strptime(day_s, '%m_%d_%Y')\n",
    "        # skip if the animal and day are already evaluated, \n",
    "        # since the CC is the same for the whole day\n",
    "        if animal+'_'+day_s in joint_list:\n",
    "            skip_flag = True\n",
    "        else:\n",
    "            skip_flag = False\n",
    "            animal_list.append(animal)\n",
    "            day_list.append(day)\n",
    "            joint_list.append(animal+'_'+day_s)\n",
    "        \n",
    "#         # get the cell matches (UNUSED FOR NOW)\n",
    "#         current_idx = match_cells(files)\n",
    "        \n",
    "        # load the data and the cell matches (wasteful, but cleaner I think)\n",
    "        with h5py.File(files, 'r') as h:\n",
    "            if 'regression' not in h.keys():\n",
    "                continue\n",
    "            # get the keys present\n",
    "            key_list = h['regression'].keys()\n",
    "            # for all the variables\n",
    "            for feature in variable_list:\n",
    "                cc_feature_list = []\n",
    "                # get the feature keys\n",
    "                current_feature = [el for el in key_list if feature in el]\n",
    "                # for all the regression types\n",
    "                for reg in regressors:\n",
    "                    # get the relevant keys\n",
    "                    current_regressor = [el for el in current_feature if reg in el]\n",
    "                    # for real vs shuffle\n",
    "                    for rvs in ['real', 'shuffle']:\n",
    "                        # get the real/shuffle keys\n",
    "                        current_rvs = [el for el in current_regressor if rvs in el]\n",
    "                        # for the time shifts\n",
    "                        for shift in time_shifts:\n",
    "                            # get the current time keys\n",
    "                            current_shift = [el for el in current_rvs if str(shift) in el]\n",
    "                            \n",
    "                            # process the correlations (exclude the std from reps)\n",
    "                            current_correlation = [el for el in current_shift if ('cc' in el) and ('_std' not in el)]\n",
    "                            assert len(current_correlation) == 1, 'more than one item in the cc list'\n",
    "                            cc_feature_list.append([feature, np.array(h['/regression/'+current_correlation[0]]), reg, rvs, str(shift), animal, day])\n",
    "                            \n",
    "                            # process the weights for the linear decoder\n",
    "                            if reg == 'linear':\n",
    "                                \n",
    "                # save the entries as a feature in the dict\n",
    "                correlations.extend(cc_feature_list) \n",
    "#                 print(correlations)\n",
    "#                 raise ValueError\n",
    "\n",
    "# convert to dataframe\n",
    "correlations = pd.DataFrame(correlations, columns=['feature', 'cc', 'regressor', 'rvs', 'shift', 'mouse', 'day'])\n",
    "correlations['cc'] = correlations['cc'].astype(float)\n",
    "\n",
    "print(f'Shape of the last feature dataframe: {correlations.shape}')            "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f649ba",
   "metadata": {},
   "source": [
    "# load the desired files and their associated regressions\n",
    "\n",
    "# Load the desired files\n",
    "importlib.reload(processing_parameters)\n",
    "\n",
    "# load the constants from the regression calculation\n",
    "time_shifts = processing_parameters.time_shifts\n",
    "shift_dict = {el: idx for idx, el in enumerate(time_shifts)}\n",
    "shift_number = len(time_shifts)\n",
    "shuffles = processing_parameters.regression_repeats\n",
    "\n",
    "# load the variable list\n",
    "variable_list = processing_parameters.variable_list\n",
    "# assemble the dataframe columns\n",
    "reals = ['real_'+str(el) for el in time_shifts]\n",
    "shuffle_means = ['smean_'+str(el) for el in time_shifts]\n",
    "columns = reals + shuffle_means + ['mouse', 'day']\n",
    "\n",
    "# get the search list\n",
    "search_list = processing_parameters.search_list\n",
    "\n",
    "# allocate a list for all paths (need to preload to get the dates)\n",
    "all_paths = []\n",
    "all_results = []\n",
    "# for all the search strings\n",
    "for search_string in search_list:\n",
    "\n",
    "    # query the database for data to plot\n",
    "    data_all = bd.query_database('analyzed_data', search_string)\n",
    "    data_path = [el['analysis_path'] for el in data_all if '_combinedanalysis' in el['slug']]\n",
    "    data_result = [el['result'] for el in data_all if '_combinedanalysis' in el['slug']]\n",
    "    all_paths.append(data_path)\n",
    "    all_results.append(data_result)\n",
    "# get the dates present\n",
    "data_dates = np.unique([os.path.basename(el)[:10] for el in np.concatenate(all_paths)])\n",
    "print(f'Dates present: {data_dates}')\n",
    "\n",
    "# allocate memory for the resulting dataframe\n",
    "data = {}\n",
    "weights = {}\n",
    "predictions = {}\n",
    "predictions_meta = {}\n",
    "\n",
    "day_list = []\n",
    "animal_list = []\n",
    "joint_list = []\n",
    "# for all the list items\n",
    "for idx0, data_path in enumerate(all_paths):\n",
    "\n",
    "    # for all the files\n",
    "    for idx1, files in enumerate(data_path):\n",
    "        \n",
    "        # if a habi trial, skip\n",
    "        if 'habi' in files:\n",
    "            continue\n",
    "        \n",
    "        # get the animal and date from the slug\n",
    "        name_parts = os.path.basename(files).split('_')\n",
    "        animal = '_'.join(name_parts[7:10])\n",
    "        day_s = '_'.join(name_parts[:3])\n",
    "        time_s = '_'.join(name_parts[3:6])\n",
    "        day = datetime.datetime.strptime(day_s, '%m_%d_%Y')\n",
    "        # skip if the animal and day are already evaluated, \n",
    "        # since the CC is the same for the whole day\n",
    "        if animal+'_'+day_s in joint_list:\n",
    "            skip_flag = True\n",
    "        else:\n",
    "            skip_flag = False\n",
    "            animal_list.append(animal)\n",
    "            day_list.append(day)\n",
    "            joint_list.append(animal+'_'+day_s)\n",
    "        # assemble the preproc path\n",
    "        files_preproc = files.replace('_combinedanalysis', '_preproc')\n",
    "        # open the file\n",
    "        with pd.HDFStore(files_preproc, 'r') as preproc:\n",
    "            if '/cell_matches' in preproc.keys():\n",
    "                # get the matches\n",
    "                cell_matches = preproc['cell_matches']\n",
    "                \n",
    "                print(animal, day_s, files_preproc)\n",
    "                # get the idx for this file\n",
    "                current_matches = cell_matches[datetime.datetime.strftime(day, '%m_%d_%Y')].to_numpy()\n",
    "                current_idx = np.argsort(current_matches).astype(float)\n",
    "                # remove the nan entries\n",
    "                current_idx = current_idx[~np.isnan(np.sort(current_matches))]\n",
    "        \n",
    "        # load the data and the cell matches (wasteful, but cleaner I think)\n",
    "        with h5py.File(files, 'r') as h:\n",
    "            \n",
    "            # for all the target variables\n",
    "            for target_variable in variable_list:\n",
    "                # create an empty list only if it's the same time this variable runs\n",
    "                if (target_variable not in data.keys()) and ('linear' in target_variable):\n",
    "                    data[target_variable] = []\n",
    "                    weights[target_variable] = []\n",
    "                    predictions[target_variable] = []\n",
    "                \n",
    "                \n",
    "                # allocate memory for the real and shuffled regressions\n",
    "                real_array = np.zeros((shift_number))\n",
    "                shuffle_array = np.zeros((shift_number))\n",
    "                real_weight = []\n",
    "                shuffle_weight = []\n",
    "                real_svr_array = np.zeros((shift_number))\n",
    "                shuffle_svr_array = np.zeros((shift_number))\n",
    "                \n",
    "#                 real_prediction = []\n",
    "#                 shuffle_prediction = []\n",
    "\n",
    "                if 'regression' not in h.keys():\n",
    "                    continue\n",
    "\n",
    "                # for all the keys (will iterate through shifts and reps for shuffle)\n",
    "                for key in h['/regression'].keys():\n",
    "                    print(key)\n",
    "                    # skip if it's not the target variable or is one of the error terms\n",
    "                    if (target_variable not in key) | ('_std' in key):\n",
    "                        continue\n",
    "                    # get the time shift \n",
    "                    key_parts = key.split('_')\n",
    "                    shift = int([el[5:] for el in key_parts if 'shift' in el][0])\n",
    "                    \n",
    "                    if ('cc' in key) & (~skip_flag):\n",
    "\n",
    "                        if 'real' in key_parts:\n",
    "                             # save the values\n",
    "                            real_array[shift_dict[shift]] = np.array(h['/regression/'+key])\n",
    "                        else:\n",
    "                            shuffle_array[shift_dict[shift]] = np.array(h['/regression/'+key])\n",
    "\n",
    "                    elif ('coefficients' in key and shift == 0) & (~skip_flag):\n",
    "                        if 'real' in key_parts:\n",
    "                            real_weight = np.array(h['/regression/'+key])\n",
    "                        else:\n",
    "                            shuffle_weight = np.array(h['/regression/'+key])\n",
    "                    elif ('prediction' in key and shift == 0):\n",
    "                        if 'real' in key_parts:\n",
    "                            real_prediction = np.array(h['/regression/'+key])\n",
    "                        else:\n",
    "                            shuffle_prediction = np.array(h['/regression/'+key])\n",
    "                    else:\n",
    "                        continue\n",
    "                if not skip_flag:\n",
    "                    # add the columns to the main list\n",
    "                    data[target_variable].append(list(real_array) + list(shuffle_array) + [animal, day])\n",
    "\n",
    "                    # take only the non time shift (need to check)\n",
    "                    if isinstance(real_weight, list):\n",
    "                        continue\n",
    "                    # assemble the weight dataframe\n",
    "                    temp_df = pd.DataFrame(np.vstack((real_weight, shuffle_weight)).T, columns=['weight', 'shuffle_weight'])\n",
    "                    temp_df['match_id'] = current_idx \n",
    "                    temp_df['animal'] = animal\n",
    "                    temp_df['day'] = day\n",
    "                    # store\n",
    "                    weights[target_variable].append(temp_df)\n",
    "                # format the datetime for the predictions\n",
    "                date_time = str(datetime.datetime.strptime('_'.join(name_parts[:6]), '%m_%d_%Y_%H_%M_%S'))\n",
    "                # store the predictions\n",
    "                predictions[target_variable].append([real_prediction, shuffle_prediction, animal, date_time])\n",
    "\n",
    "# for all the variables once more\n",
    "for target_variable in variable_list:\n",
    "    # turn the overall list into a dataframe\n",
    "    data[target_variable] = pd.DataFrame(data[target_variable], columns=columns).sort_values(['mouse', 'day'], axis=0)\n",
    "    # turn the weights into a dictionary\n",
    "    weights[target_variable] = pd.concat(weights[target_variable], axis=0)\n",
    "    # same with the predictions\n",
    "    predictions_meta[target_variable] = []\n",
    "\n",
    "    temp_data = []\n",
    "    temp_meta = []\n",
    "    for el in predictions[target_variable]:\n",
    "        # assemble the meta \n",
    "        mouse = el[2]\n",
    "        date_time = el[3]\n",
    "        temp_meta.append([mouse, date_time])\n",
    "\n",
    "        # get the actual predictions\n",
    "        temp_data.append([el[0], el[1]])\n",
    "    # pack the predictions and meta\n",
    "    predictions[target_variable] = temp_data\n",
    "    del temp_data\n",
    "    predictions_meta[target_variable] = pd.DataFrame(temp_meta, columns=['mouse', 'datetime'])\n",
    "\n",
    "    print(f'Shape of the data dictionary: {data[target_variable].shape}')\n",
    "    print(f'Shape of the weights dataframe: {weights[target_variable].shape}')\n",
    "    print(f'Shape of the prediction list: {len(predictions[target_variable])}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af87e88d",
   "metadata": {},
   "source": [
    "# plot the performance over days\n",
    "\n",
    "# define the target time point\n",
    "target_tpoint = '0'\n",
    "# allocate the plot list\n",
    "plot_list = []\n",
    "\n",
    "# for all the variables\n",
    "for target_variable in variable_list:\n",
    "    \n",
    "    # group the real and shuffle data from the center time point over days\n",
    "#     means = data[target_variable].groupby(['mouse', 'day'], as_index=False)[['real_'+target_tpoint, 'smean_'+target_tpoint]].mean()\n",
    "#     sems = data[target_variable].groupby(['mouse', 'day'], as_index=False)[['real_'+target_tpoint, 'smean_'+target_tpoint]].sem().fillna(0).drop(['mouse', 'day'], axis=1)\n",
    "#     sems = sems.rename({el:el+'_sem' for el in sems.columns}, axis=1)\n",
    "    # concatenate\n",
    "#     collapsed_data = pd.concat((means, sems), axis=1)\n",
    "#     collapsed_data = means\n",
    "    collapsed_data = data[target_variable][['real_'+target_tpoint, 'smean_'+target_tpoint, 'mouse', 'day']]\n",
    "    # allocate a list for this feature's plot\n",
    "    mouse_list = []\n",
    "    # for all the mice\n",
    "    for mouse_name, mouse_data in collapsed_data.groupby(['mouse']):\n",
    "        \n",
    "        # reformat day as a delta\n",
    "        day_data = mouse_data.loc[:, 'day'].to_numpy().copy()\n",
    "        delta_days = [(el-day_data[0]) for el in day_data]\n",
    "        delta_days = (delta_days/np.timedelta64(1, 'D')).astype(int)\n",
    "        mouse_data.loc[:, 'day'] = delta_days\n",
    "        \n",
    "        mouse_data = mouse_data.iloc[:10, :]\n",
    "\n",
    "        # plot\n",
    "        real_mean = hv.Scatter(mouse_data, kdims='day', vdims='real_'+target_tpoint)\n",
    "        real_mean.opts(width=400, color='r', title=target_variable)\n",
    "#         real_sem = hv.Spread(mouse_data, kdims='day', vdims=['real_'+target_tpoint, 'real_'+target_tpoint+'_sem'])\n",
    "        shuffle_mean = hv.Scatter(mouse_data, kdims='day', vdims='smean_'+target_tpoint)\n",
    "        shuffle_mean.opts(color='k')\n",
    "#         shuffle_sem = hv.Spread(mouse_data, kdims='day', vdims=['smean_'+target_tpoint, 'smean_'+target_tpoint+'_sem'])\n",
    "        \n",
    "#         mouse_list.append(real_mean*real_sem*shuffle_mean*shuffle_sem)\n",
    "        mouse_list.append(real_mean*shuffle_mean)\n",
    "    plot_list.append(hv.Overlay(mouse_list))\n",
    "hv.Layout(plot_list)\n",
    "#     raise ValueError"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db6a5b0",
   "metadata": {},
   "source": [
    "# performance across days and mice\n",
    "\n",
    "# plot the performance over days\n",
    "\n",
    "# define the target time point\n",
    "target_tpoint = '0'\n",
    "# allocate the plot list\n",
    "plot_list = []\n",
    "\n",
    "# for all the variables\n",
    "for target_variable in variable_list:\n",
    "    # get the current feature\n",
    "    current_feature = data[target_variable].copy()\n",
    "    \n",
    "    # replace the day column for a delta\n",
    "    # allocate memory for the new dates\n",
    "    new_days = []\n",
    "    # for all the mice\n",
    "    for mouse_name, mouse_data in current_feature.groupby(['mouse'])['day']:\n",
    "        # reformat day as a delta\n",
    "        day_data = mouse_data.to_numpy()\n",
    "        delta_days = [(el-day_data[0]) for el in day_data]\n",
    "        delta_days = (delta_days/np.timedelta64(1, 'D')).astype(int)\n",
    "        new_days.append(delta_days)\n",
    "    # replace the days\n",
    "    current_feature.loc[:, 'day'] = np.hstack(new_days)\n",
    "    \n",
    "    # group the real and shuffle data from the center time point over days\n",
    "    means = current_feature.groupby(['day'], as_index=False)[['real_'+target_tpoint, 'smean_'+target_tpoint]].mean()\n",
    "    sems = current_feature.groupby(['day'], as_index=False)[['real_'+target_tpoint, 'smean_'+target_tpoint]].sem().fillna(0).drop(['day'], axis=1)\n",
    "    sems = sems.rename({el:el+'_sem' for el in sems.columns}, axis=1)\n",
    "    # concatenate\n",
    "    collapsed_data = pd.concat((means, sems), axis=1).iloc[:10, :]\n",
    "\n",
    "    # plot\n",
    "    real_mean = hv.Curve(collapsed_data, kdims='day', vdims='real_'+target_tpoint)\n",
    "    real_mean.opts(width=400, color='r', title=target_variable)\n",
    "    real_sem = hv.Spread(collapsed_data, kdims='day', vdims=['real_'+target_tpoint, 'real_'+target_tpoint+'_sem'])\n",
    "    real_sem.opts(width=400, color='r', title=target_variable)\n",
    "    shuffle_mean = hv.Curve(collapsed_data, kdims='day', vdims='smean_'+target_tpoint)\n",
    "    shuffle_mean.opts(color='k')\n",
    "    shuffle_sem = hv.Spread(collapsed_data, kdims='day', vdims=['smean_'+target_tpoint, 'smean_'+target_tpoint+'_sem'])\n",
    "    shuffle_sem.opts(color='k')\n",
    "\n",
    "    plot_list.append(real_mean*real_sem*shuffle_mean*shuffle_sem)\n",
    "#     plot_list.append(hv.Overlay(mouse_list))\n",
    "hv.Layout(plot_list)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c4ff19",
   "metadata": {},
   "source": [
    "# plot the average kernel per animal compared to shuffle\n",
    "\n",
    "overlay_list = []\n",
    "# for all the target variables\n",
    "for target_variable in variable_list:\n",
    "    # average across days for each animal\n",
    "    averages = data[target_variable].groupby(['mouse',], as_index=False)[reals+shuffle_means].mean()\n",
    "    sems = data[target_variable].groupby(['mouse',], as_index=False)[reals+shuffle_means].sem().fillna(0)\n",
    "\n",
    "    # allocate a list for the plots\n",
    "    plot_list = []\n",
    "    # for all the mice\n",
    "    for idx, (mouse, df)  in enumerate(averages.groupby(['mouse'])):\n",
    "        real_plot = hv.Scatter((time_shifts, df.loc[:, reals].to_numpy().flatten()))\n",
    "        real_plot.opts(width=400, height=400, title=target_variable, color='red')\n",
    "#         real_sem = hv.Spread((time_shifts, df.loc[:, reals].to_numpy().flatten(), sems.loc[idx, reals].to_numpy().flatten()))\n",
    "#         real_sem.opts(color='red')\n",
    "        shuffle_plot = hv.Scatter((time_shifts, df.loc[:, shuffle_means].to_numpy().flatten()))\n",
    "        shuffle_plot.opts(color='black')\n",
    "#         shuffle_error = hv.Spread((time_shifts, df.loc[:, shuffle_means].to_numpy().flatten(), sems.loc[idx, shuffle_means].to_numpy().flatten()))\n",
    "#         shuffle_error.opts(color='black')\n",
    "\n",
    "#         plot_list.append(real_plot*real_sem*shuffle_plot*shuffle_error)\n",
    "        plot_list.append(real_plot*shuffle_plot)\n",
    "    \n",
    "\n",
    "    overlay_list.append(hv.Overlay(plot_list))\n",
    "\n",
    "hv.Layout(overlay_list).cols(3)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de415f49",
   "metadata": {},
   "source": [
    "# Average across mice and time\n",
    "\n",
    "overlay_list = []\n",
    "# for all the target variables\n",
    "for target_variable in variable_list:\n",
    "    # average across days for each animal\n",
    "#     averages = data[target_variable].groupby(, as_index=False)[reals+shuffle_means+shuffle_sems].mean()\n",
    "    averages = data[target_variable].loc[:, reals+shuffle_means].mean(axis=0)\n",
    "#     print(target_variable, data[target_variable])\n",
    "#     raise ValueError\n",
    "    sems = data[target_variable].loc[:, reals+shuffle_means].sem(axis=0)\n",
    "    \n",
    "    real_plot = hv.Curve((time_shifts, averages.loc[reals].to_numpy().flatten()), kdims='Time shift', vdims='Performance (cc)')\n",
    "    real_sem = hv.Spread((time_shifts, averages.loc[reals].to_numpy().flatten(), sems.loc[reals].to_numpy().flatten()))\n",
    "    real_plot.opts(width=400, height=400, title=target_variable)\n",
    "    shuffle_plot = hv.Curve((time_shifts, averages.loc[shuffle_means].to_numpy().flatten()))\n",
    "    shuffle_error = hv.Spread((time_shifts, averages.loc[shuffle_means].to_numpy().flatten(), sems.loc[shuffle_means].to_numpy().flatten()))\n",
    "\n",
    "    overlay_list.append(real_plot*real_sem*shuffle_plot*shuffle_error)\n",
    "    \n",
    "\n",
    "\n",
    "hv.Layout(overlay_list).cols(3)\n",
    "    "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd77287",
   "metadata": {},
   "source": [
    "print(hv.render(violin.BoxWhisker.I).renderers[0]._property_values['glyph'].__dict__)\n",
    "# hv.help(hv.BoxWhisker)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "9b889bf9",
   "metadata": {},
   "source": [
    "# Regression box plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e9355f",
   "metadata": {},
   "source": [
    "\n",
    "# define the target shift\n",
    "tshift = '0'\n",
    "\n",
    "# generate 2 plots with half of the variables each\n",
    "sub_dataframe = correlations.iloc[((correlations['rvs']=='real')&(correlations['regressor']=='linear')&(correlations['shift']==tshift)).values, :].copy()\n",
    "sub_dataframe['feature'] = [label_dict[el] for el in sub_dataframe['feature']]\n",
    "violin0 = hv.BoxWhisker(sub_dataframe, ['feature'], ['cc'])\n",
    "violin0.opts(width=800, height=800, xrotation=45, ylabel='CC (a.u.)', ylim=(-0.2, 0.6), box_fill_color='#00ffff')\n",
    "\n",
    "sub_dataframe = correlations.iloc[((correlations['rvs']=='shuffle')&(correlations['regressor']=='linear')&(correlations['shift']==tshift)).values, :].copy()\n",
    "sub_dataframe['feature'] = [label_dict[el] for el in sub_dataframe['feature']]\n",
    "violin1 = hv.BoxWhisker(sub_dataframe, ['feature'], ['cc'])\n",
    "violin1.opts(box_fill_color='#999999')\n",
    "\n",
    "violin = violin0 * violin1\n",
    "violin.opts(opts.BoxWhisker(xlabel='', box_line_width=1, whisker_line_width=1, outlier_line_width=0))\n",
    "\n",
    "# assemble the file name\n",
    "save_name = os.path.join(save_path, '_'.join((target_document, 'Reg_CC_linear')) + '.png')\n",
    "# save the figure\n",
    "violin = fp.save_figure(violin, save_name, fig_width=10, dpi=1200, fontsize=target_document, target='save')\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec94e06",
   "metadata": {},
   "source": [
    "\n",
    "# define the target shift\n",
    "tshift = '0'\n",
    "# 4aa2d9 f05236\n",
    "# generate 2 plots with half of the variables each\n",
    "sub_dataframe = correlations.iloc[((correlations['rvs']=='real')&(correlations['regressor']=='SVR')&(correlations['shift']==tshift)).values, :].copy()\n",
    "sub_dataframe['feature'] = [label_dict[el] for el in sub_dataframe['feature']]\n",
    "violin0 = hv.BoxWhisker(sub_dataframe, ['feature'], ['cc'])\n",
    "violin0.opts(width=800, height=800, xrotation=45, ylabel='CC (a.u.)', ylim=(-0.2, 0.6), box_fill_color='#ff00ff')\n",
    "\n",
    "sub_dataframe = correlations.iloc[((correlations['rvs']=='shuffle')&(correlations['regressor']=='SVR')&(correlations['shift']==tshift)).values, :].copy()\n",
    "sub_dataframe['feature'] = [label_dict[el] for el in sub_dataframe['feature']]\n",
    "violin1 = hv.BoxWhisker(sub_dataframe, ['feature'], ['cc'])\n",
    "violin1.opts(box_fill_color='#999999')\n",
    "\n",
    "violin = violin0 * violin1\n",
    "violin.opts(opts.BoxWhisker(xlabel='', box_line_width=1, whisker_line_width=1, outlier_line_width=0))\n",
    "\n",
    "# assemble the file name\n",
    "save_name = os.path.join(save_path, '_'.join((target_document, 'Reg_CC_SVR')) + '.png')\n",
    "# save the figure\n",
    "violin = fp.save_figure(violin, save_name, fig_width=10, dpi=1200, fontsize=target_document, target='save')\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77005120",
   "metadata": {},
   "source": [
    "# plot the SVR vs linear cc\n",
    "\n",
    "# define the target shift\n",
    "tshift = '0'\n",
    "\n",
    "# generate 2 plots with half of the variables each\n",
    "sub_dataframe = correlations.iloc[((correlations['rvs']=='real')&(correlations['regressor']=='SVR')&(correlations['shift']==tshift)).values, :].copy()\n",
    "sub_dataframe['feature'] = [label_dict[el] for el in sub_dataframe['feature']]\n",
    "violin0 = hv.BoxWhisker(sub_dataframe, ['feature'], ['cc'])\n",
    "violin0.opts(width=800, height=800, xrotation=45, ylabel='CC (a.u.)', ylim=(-0.2, 0.6), box_fill_color='#ff00ff', box_fill_alpha=1)\n",
    "\n",
    "sub_dataframe = correlations.iloc[((correlations['rvs']=='real')&(correlations['regressor']=='linear')&(correlations['shift']==tshift)).values, :].copy()\n",
    "sub_dataframe['feature'] = [label_dict[el] for el in sub_dataframe['feature']]\n",
    "violin1 = hv.BoxWhisker(sub_dataframe, ['feature'], ['cc'])\n",
    "violin1.opts(box_fill_color='#00ffff', box_fill_alpha=1)\n",
    "\n",
    "violin = violin0 * violin1\n",
    "violin.opts(opts.BoxWhisker(xlabel='', box_line_width=1, whisker_line_width=1, outlier_line_width=0))\n",
    "\n",
    "# assemble the file name\n",
    "save_name = os.path.join(save_path, '_'.join((target_document, 'Reg_CC_SVRvLinear')) + '.png')\n",
    "# save the figure\n",
    "violin = fp.save_figure(violin, save_name, fig_width=10, dpi=1200, fontsize=target_document, target='save')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8ac67a",
   "metadata": {},
   "source": [
    "for feature in variable_list:\n",
    "    x = correlations.iloc[((correlations['feature']==feature)&(correlations['rvs']=='real')&(correlations['regressor']=='linear')&(correlations['shift']==tshift)).values, :].loc[:, 'cc']\n",
    "    y = correlations.iloc[((correlations['feature']==feature)&(correlations['rvs']=='real')&(correlations['regressor']=='SVR')&(correlations['shift']==tshift)).values, :].loc[:, 'cc']\n",
    "    \n",
    "    test = stat.mannwhitneyu(x, y)\n",
    "    print(feature, test[1]*len(variable_list))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80334d45",
   "metadata": {},
   "source": [
    "for feature in variable_list:\n",
    "    x = correlations.iloc[((correlations['feature']==feature)&(correlations['rvs']=='real')&(correlations['regressor']=='linear')&(correlations['shift']==tshift)).values, :].loc[:, 'cc']\n",
    "    y = correlations.iloc[((correlations['feature']==feature)&(correlations['rvs']=='shuffle')&(correlations['regressor']=='linear')&(correlations['shift']==tshift)).values, :].loc[:, 'cc']\n",
    "    \n",
    "    test = stat.mannwhitneyu(x, y)\n",
    "    print(feature, test[1]*len(variable_list))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abda937a",
   "metadata": {},
   "source": [
    "for feature in variable_list:\n",
    "    x = correlations.iloc[((correlations['feature']==feature)&(correlations['rvs']=='real')&(correlations['regressor']=='SVR')&(correlations['shift']==tshift)).values, :].loc[:, 'cc']\n",
    "    y = correlations.iloc[((correlations['feature']==feature)&(correlations['rvs']=='shuffle')&(correlations['regressor']=='SVR')&(correlations['shift']==tshift)).values, :].loc[:, 'cc']\n",
    "    \n",
    "    test = stat.mannwhitneyu(x, y)\n",
    "    print(feature, test[1]*len(variable_list))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077cb944",
   "metadata": {},
   "source": [
    "plot_list = []\n",
    "for feature in variable_list:\n",
    "    x = correlations.iloc[((correlations['feature']==feature)&(correlations['rvs']=='real')&(correlations['regressor']=='linear')&(correlations['shift']==tshift)).values, :]\n",
    "    y = correlations.iloc[((correlations['feature']==feature)&(correlations['rvs']=='real')&(correlations['regressor']=='SVR')&(correlations['shift']==tshift)).values, :]\n",
    "\n",
    "    plot = hv.Scatter((x['cc'], y['cc']))\n",
    "    corr = stat.spearmanr(x['cc'], y['cc'], nan_policy='omit')[0]\n",
    "    mean_cc = x['cc'].mean()\n",
    "    plot.opts(width=400, height=400, tools=['hover'], xlabel='Linear', ylabel='SVR', title=f'{feature} {corr:.2f} {mean_cc:0.2f}', xlim=(-0.2, 0.6), ylim=(-0.2, 0.6))\n",
    "    plot_list.append(plot)\n",
    "\n",
    "layout = hv.Layout(plot_list).cols(4).opts(shared_axes=True)\n",
    "layout\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f546c7e1",
   "metadata": {},
   "source": [
    "# plot the distributions per variable\n",
    "importlib.reload(fp)\n",
    "importlib.reload(processing_parameters)\n",
    "variable_list = processing_parameters.variable_list\n",
    "label_dict = processing_parameters.label_dictionary\n",
    "\n",
    "# define the target time point\n",
    "tpoint = 0\n",
    "real_list = []\n",
    "shuffle_list = []\n",
    "\n",
    "# def format_whisker()\n",
    "\n",
    "# define the variables to include\n",
    "include_variables = variable_list\n",
    "# for all the variables\n",
    "for target_variable in include_variables:\n",
    "    # get only the 0 lag value for each day\n",
    "    real_data = data[target_variable].loc[:, ['real_'+str(tpoint)]]\n",
    "    real_data['Feature'] = label_dict[target_variable]\n",
    "    real_list.append(real_data)\n",
    "    \n",
    "    shuffle_data = data[target_variable].loc[:, ['smean_'+str(tpoint)]]\n",
    "#     shuffle_data = shuffle_data.rename({'smean_'+str(tpoint): 'real_'+str(tpoint)}, axis=1)\n",
    "    shuffle_data['Feature'] = label_dict[target_variable]#+' shuffle'\n",
    "    shuffle_list.append(shuffle_data)\n",
    "    \n",
    "    x_mwu = real_data.loc[:, 'real_'+str(tpoint)].to_numpy()\n",
    "    x_mwu = x_mwu[~np.isnan(x_mwu)]\n",
    "    y_mwu = shuffle_data.loc[:, 'smean_'+str(tpoint)].to_numpy()\n",
    "    y_mwu = y_mwu[~np.isnan(y_mwu)]\n",
    "    test = stat.mannwhitneyu(x_mwu, y_mwu)\n",
    "    print(target_variable, test[1]*len(include_variables))\n",
    "\n",
    "# get the number of plots\n",
    "number_plots = len(real_list)\n",
    "# get the half index\n",
    "half_index = int(np.ceil(number_plots/2)) + 1\n",
    "# generate 2 plots with half of the variables each\n",
    "violin0 = hv.BoxWhisker(pd.concat(real_list, axis=0), ['Feature'], ['real_'+str(tpoint)])\n",
    "violin0.opts(width=800, height=800, xrotation=45, ylabel='CC (a.u.)')\n",
    "\n",
    "violin1 = hv.BoxWhisker(pd.concat(shuffle_list, axis=0), ['Feature'], ['smean_'+str(tpoint)])\n",
    "\n",
    "violin = violin0 * violin1\n",
    "violin.opts(opts.BoxWhisker(xlabel='', box_line_width=1, whisker_line_width=1, outlier_line_width=0))\n",
    "\n",
    "# assemble the file name\n",
    "save_name = os.path.join(save_path, '_'.join((target_document, 'Reg_CC')) + '.png')\n",
    "# save the figure\n",
    "violin = fp.save_figure(violin, save_name, fig_width=10, dpi=1200, fontsize=target_document, target='screen')\n",
    "# violin\n",
    "# violin0\n",
    "# violin1 = hv.BoxWhisker(pd.concat(real_list[half_index:], axis=0), ['Feature'], ['real_'+str(tpoint)])\n",
    "# violin1.opts(width=100, height=800, xrotation=45, ylabel='CC (a.u.)')\n",
    "\n",
    "# (violin0+violin1).cols(1).opts(shared_axes=False)\n",
    "# violin0\n",
    "# violin1 = hv.Violin(pd.concat(shuffle_list, axis=0), ['Feature'], ['smean_'+str(tpoint)])\n",
    "# violin1.opts(width=800, height=800, xrotation=45)\n",
    "\n",
    "# (violin0*violin1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9280062",
   "metadata": {},
   "source": [
    "# plot the CC over time\n",
    "\n",
    "# initialize a list for the plots\n",
    "time_plot = []\n",
    "\n",
    "# define the target timepoint\n",
    "tpoint = 10\n",
    "# for all the variables\n",
    "for target_variable in variable_list:\n",
    "    temp_list = []\n",
    "    # for all the mice\n",
    "    for mouse, df_ori in data[target_variable].groupby(['mouse']):\n",
    "        # copy to not mess the original dataframe\n",
    "        df = df_ori.copy()\n",
    "        df = df.sort_values(['day'], axis=0).reset_index(drop=True)\n",
    "        # get the delta time\n",
    "        delta_time = [(el-df['day'][0]).days for el in df['day']]\n",
    "\n",
    "        real_plot = hv.Curve((delta_time, df.loc[:, 'real_'+str(tpoint)]))\n",
    "        real_plot.opts(title=target_variable, width=400, height=400, xlabel='Time (days)', ylabel='CC (a.u.)')\n",
    "        shuffle_plot = hv.Curve((delta_time, df.loc[:, 'smean_'+str(tpoint)]))\n",
    "#         shuffle_error = hv.Spread((delta_time, df.loc[:, 'smean_'+str(tpoint)], df.loc[:, 'ssem_'+str(tpoint)]))\n",
    "        \n",
    "        \n",
    "\n",
    "        temp_list.append(real_plot*shuffle_plot)\n",
    "#         temp_list.append(real_plot)\n",
    "    time_plot.append(hv.Overlay(temp_list))\n",
    "\n",
    "hv.Layout(time_plot).cols(3)\n",
    "    \n",
    "    \n",
    "    "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80843565",
   "metadata": {},
   "source": [
    "# get only the weights\n",
    "output_df = []\n",
    "\n",
    "# for all the variables\n",
    "for idx, target_feature in enumerate(weights.keys()):\n",
    "    # get the dataframe for this feature\n",
    "    current_feature = weights[target_feature]\n",
    "    # if it's the first one, also get the mouse and day\n",
    "    if idx == 0:\n",
    "        mouse_day = current_feature.loc[:, ['animal', 'day']]\n",
    "        # allocate memory for the output\n",
    "        temp_time = []\n",
    "        # for all the mice\n",
    "        for mouse_name, mouse_data in mouse_day.groupby(['animal'], as_index=False):\n",
    "            # reformat day as a delta\n",
    "            day_data = mouse_data.loc[:, 'day'].to_numpy().copy()\n",
    "            delta_days = [(el-day_data[0]) for el in day_data]\n",
    "            delta_days = (delta_days/np.timedelta64(1, 'D')).astype(int)\n",
    "#             mouse_data.loc[:, 'day'] = delta_days\n",
    "            temp_time.append(delta_days)\n",
    "#             raise ValueError\n",
    "        mouse_day['day'] = np.concatenate(temp_time, axis=0)\n",
    "        output_df.append(mouse_day.to_numpy()) \n",
    "    \n",
    "    # get only the weights and store in the output dataframe\n",
    "    output_df.append(current_feature.loc[:, ['weight']].to_numpy())    \n",
    "\n",
    "# convert to dataframe\n",
    "output_df = pd.DataFrame(np.hstack(output_df), columns=['animal', 'day']+list(weights.keys()))\n",
    "print(output_df.shape)\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac999c37",
   "metadata": {},
   "source": [
    "# calculate the correlation matrix for the variables\n",
    "\n",
    "feature_matrix = output_df.loc[:, variable_list]\n",
    "correlation_matrix, pvalue_matrix = stat.spearmanr(feature_matrix)\n",
    "\n",
    "print(correlation_matrix.shape)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cc6d18ab",
   "metadata": {},
   "source": [
    "# Correlation plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14774ff7",
   "metadata": {},
   "source": [
    "# ticks = [(idx+0.5, el) for idx, el in enumerate(variable_list)]\n",
    "\n",
    "# raster = hv.Raster(correlation_matrix)\n",
    "# raster.opts(width=800, height=600, yticks=ticks, xticks=ticks, xrotation=45, colorbar=True, cmap='RdBu', clim=(-1, 1), tools=['hover'])\n",
    "\n",
    "ticks = [(idx+0.5, label_dict[el]) for idx, el in enumerate(variable_list)]\n",
    "# ticks = [(idx+0.5, idx) for idx, el in enumerate(variable_list)]\n",
    "plot_matrix = correlation_matrix.copy()\n",
    "plot_matrix = np.tril(plot_matrix, k=0)\n",
    "plot_matrix[plot_matrix==0] = np.nan\n",
    "# hv.Raster(correlation_matrix)\n",
    "raster = hv.Raster(plot_matrix)\n",
    "# format the plot\n",
    "raster = fp.format_figure(raster, width=950, height=800, yticks=ticks, xticks=ticks, colorbar=True, cmap='RdBu', clim=(-1, 1), xrotation=45)\n",
    "\n",
    "# assemble the file name\n",
    "save_name = os.path.join(save_path, '_'.join((target_document, 'Reg_correlation')) + '.png')\n",
    "# save the figure\n",
    "fig = fp.save_figure(raster, save_name, fig_width=15, dpi=1200, fontsize=target_document, target='save')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027d96a6",
   "metadata": {},
   "source": [
    "%%time\n",
    "# perform a umap decomposition of the weights across variables\n",
    "\n",
    "# format the data\n",
    "umap_process = output_df.drop(['animal', 'day'], axis=1).to_numpy()\n",
    "\n",
    "# normalize the umap data per column\n",
    "# scaler = prep.StandardScaler()\n",
    "# umap_process = scaler.fit_transform(umap_process)\n",
    "umap_process = np.abs(umap_process)/np.abs(umap_process).max(axis=0)\n",
    "\n",
    "# umap_data = output_df.to_numpy()\n",
    "\n",
    "# run the decomposition\n",
    "reducer = umap.UMAP(min_dist=0.5, n_neighbors=30)\n",
    "embedded_data = reducer.fit_transform(umap_process)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d964d5b6",
   "metadata": {},
   "source": [
    "# UMAP regression plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bea3dc9",
   "metadata": {},
   "source": [
    "# plot the decomposition\n",
    "\n",
    "# define the interval between points\n",
    "interv = 1\n",
    "perc = 95\n",
    "\n",
    "importlib.reload(fp)\n",
    "\n",
    "umap_list = []\n",
    "# target_key = 'cricket_0_mouse_distance'\n",
    "# for all the variables\n",
    "for target_key in variable_list + ['animal', 'day']:\n",
    "    if target_key in ['animal', 'day']:\n",
    "        counts, raw_labels = np.unique(output_df.loc[:, target_key].to_numpy(), return_inverse=True)\n",
    "        raw_labels = (raw_labels - raw_labels.min())/(raw_labels.max() - raw_labels.min())\n",
    "        title = target_key\n",
    "    else:\n",
    "#     #     counts, raw_labels = np.unique(output_df.loc[:, target_key].to_numpy(), return_inverse=True)\n",
    "        raw_labels = np.abs(output_df.loc[:, target_key].to_numpy().astype(np.float64))\n",
    "        raw_labels[raw_labels>np.percentile(raw_labels, perc)] = np.percentile(raw_labels, perc)\n",
    "        raw_labels[raw_labels<np.percentile(raw_labels, 100-perc)] = np.percentile(raw_labels, 100-perc)\n",
    "        title = label_dict[target_key]\n",
    "    #     raw_labels /= np.nanmax(raw_labels)\n",
    "\n",
    "    compiled_labels = np.expand_dims(raw_labels, axis=1)\n",
    "\n",
    "    umap_data = np.concatenate((embedded_data,compiled_labels),axis=1)\n",
    "\n",
    "    compiled_labels = compiled_labels[::interv]\n",
    "    umap_data = umap_data[::interv, :]\n",
    "    \n",
    "    umap_plot = hv.Scatter(umap_data, vdims=['Dim 2', target_key], kdims=['Dim 1'])\n",
    "    umap_plot.opts(color=target_key, colorbar=True, cmap='Spectral', size=1, tools=['hover'])\n",
    "    umap_plot.opts(height=600, width=800, xaxis=None, yaxis=None, colorbar=False, title=title)\n",
    "    \n",
    "    save_name = os.path.join(save_path, '_'.join((target_document, 'Reg_UMAP', target_key)) + '.png')\n",
    "    # save the figure\n",
    "    fig = fp.save_figure(umap_plot, save_name, fig_width=7.7, dpi=1200, fontsize=target_document, target='save', display_factor=0.3)\n",
    "    umap_list.append(umap_plot)\n",
    "\n",
    "# hv.Layout(umap_list).cols(2)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "151be103",
   "metadata": {},
   "source": [
    "# Colorbar generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379d2188",
   "metadata": {},
   "source": [
    "# generate a colorbar\n",
    "\n",
    "# define the target color map\n",
    "target_cmap = 'Spectral_r'\n",
    "\n",
    "cbar_data = np.array([np.arange(0, 255, 1), np.arange(0, 255, 1)])\n",
    "cbar = hv.Raster(1-cbar_data.T)\n",
    "cbar.opts(tools=['hover'], cmap=target_cmap, xaxis=None, yaxis=None, width=100, height=400)\n",
    "\n",
    "save_name = os.path.join(save_path, '_'.join((target_document, 'Colorbar', target_cmap)) + '.png')\n",
    "# save the figure\n",
    "cbar = fp.save_figure(cbar, save_name, fig_width=1, dpi=1200, fontsize=target_document, target='save', display_factor=0.3)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d76f22",
   "metadata": {},
   "source": [
    "def gini(array):\n",
    "    \"\"\"Calculate the Gini coefficient of a numpy array. From https://neuroplausible.com/gini\"\"\"\n",
    "    # All values are treated equally, arrays must be 1d:\n",
    "    array = array.flatten()\n",
    "    if np.amin(array) < 0:\n",
    "        # Values cannot be negative:\n",
    "        array -= np.amin(array)\n",
    "    # Values cannot be 0:\n",
    "    array += 0.0000001\n",
    "    # Values must be sorted:\n",
    "    array = np.sort(array)\n",
    "    # Index per array element:\n",
    "    index = np.arange(1,array.shape[0]+1)\n",
    "    # Number of array elements:\n",
    "    n = array.shape[0]\n",
    "    # Gini coefficient:\n",
    "    return ((np.sum((2 * index - n  - 1) * array)) / (n * np.sum(array)))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916d1612",
   "metadata": {},
   "source": [
    "def gini2(array, bins=30):\n",
    "    \"\"\"Calculate the Gini coefficient according to de Oliveira and Kim et al.\"\"\"\n",
    "    # bin the data\n",
    "    counts, bin_edges, _ = stat.binned_statistic(np.abs(array), array, bins=bins, statistic='count')\n",
    "    \n",
    "    # get the fractions\n",
    "    fractions = counts/counts.sum()\n",
    "    # multiply by the counts\n",
    "    values = (bin_edges[1:] + bin_edges[:-1])/2\n",
    "    s = np.cumsum(fractions * values)\n",
    "    s0 = np.concatenate(([0], s[:-1]), axis=0)\n",
    "\n",
    "    # calculate the coefficient\n",
    "    gini_coefficient = 1 - np.sum(fractions*(s0 + s))/s[-1]\n",
    "    \n",
    "    return gini_coefficient\n",
    "    "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d85a45b",
   "metadata": {},
   "source": [
    "# Calculate and plot Gini coefficient\n",
    "\n",
    "# allocate memory for the calculation\n",
    "gini_array = []\n",
    "# for all the variables\n",
    "for animal_date, current_day in output_df.groupby(['animal', 'day'], as_index=False):\n",
    "    # allocate memory for the day\n",
    "    day_list = []\n",
    "    # for all the features\n",
    "    for feature in variable_list:\n",
    "        # get the feature\n",
    "        current_feat = current_day[feature].to_numpy().astype(np.float64)\n",
    "#         print(current_feat.shape)\n",
    "        \n",
    "        # calculate the gini coefficient and store\n",
    "        current_gini = gini2(current_feat, bins=20)\n",
    "\n",
    "        day_list.append(pd.DataFrame([[label_dict[feature], current_gini]], columns=['Feature', 'Gini']))\n",
    "        \n",
    "    # store\n",
    "    gini_array.append(pd.concat(day_list, axis=0))\n",
    "    \n",
    "gini_array = pd.concat(gini_array, axis=0)\n",
    "print(gini_array)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ac6b65",
   "metadata": {},
   "source": [
    "%%time\n",
    "# calculate the Gini coefficient based on resampled weights\n",
    "\n",
    "# define the number of shuffles\n",
    "number_shuffles = 100\n",
    "# allocate a list for the output\n",
    "shuffle_gini = []\n",
    "# for all the shuffles\n",
    "for shuff in np.arange(number_shuffles):\n",
    "\n",
    "    # for all the variables\n",
    "    for animal_date, current_day in output_df.groupby(['animal', 'day'], as_index=False):\n",
    "        # allocate memory for the day\n",
    "        day_list = []\n",
    "        # for all the features\n",
    "        for feature in variable_list:\n",
    "            # get the feature\n",
    "            current_feat = current_day[feature].to_numpy().astype(np.float64)\n",
    "            a = np.min(current_feat)\n",
    "            b = np.max(current_feat)\n",
    "            current_feat = (b - a) * np.random.random_sample(current_feat.shape[0]) + a\n",
    "            # draw randomly from the feature\n",
    "#             current_feat = np.random.choice(current_feat, current_feat.shape[0], replace=True)\n",
    "#             current_feat = np.mean(current_feat)*np.ones_like(current_feat)\n",
    "#             current_feat = np.random.randn(current_feat.shape[0])\n",
    "#             print(current_feat)\n",
    "\n",
    "            # calculate the gini coefficient and store\n",
    "            current_gini = gini2(current_feat, bins=20)\n",
    "\n",
    "            day_list.append(pd.DataFrame([[label_dict[feature], current_gini]], columns=['Feature', 'Gini']))\n",
    "\n",
    "        # store\n",
    "        shuffle_gini.append(pd.concat(day_list, axis=0))\n",
    "# concatenate the dataframes\n",
    "shuffle_gini = pd.concat(shuffle_gini, axis=0)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "6546c1d5",
   "metadata": {},
   "source": [
    "# Gini coefficient plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57de6864",
   "metadata": {},
   "source": [
    "# Plot the gini coefficients\n",
    "\n",
    "importlib.reload(fp)\n",
    "# print(plot_array.columns)\n",
    "# print(plot_array)\n",
    "\n",
    "# print(plot_array)\n",
    "# ticks = [(idx+0.5, label_dict[el]) for idx, el in enumerate(variable_list)]\n",
    "\n",
    "whisker0 = hv.BoxWhisker(gini_array, ['Feature'], ['Gini'])\n",
    "whisker0.opts(width=800, height=800, xrotation=45, ylabel='Sparsity', xlabel='')\n",
    "\n",
    "whisker1 = hv.BoxWhisker(shuffle_gini, ['Feature'], ['Gini'])\n",
    "whisker = hv.Overlay([whisker0, whisker1])\n",
    "whisker.opts(opts.BoxWhisker(box_line_width=1, whisker_line_width=1, outlier_line_width=1))\n",
    "\n",
    "# assemble the file name\n",
    "save_name = os.path.join(save_path, '_'.join((target_document, 'Reg_gini')) + '.png')\n",
    "# save the figure\n",
    "fig = fp.save_figure(whisker, save_name, fig_width=10, dpi=1200, fontsize=target_document, target='save')\n",
    "\n",
    "# hv.Scatter((variable_list, gini_array)).opts(xrotation=45, width=1000, height=600)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3a408b",
   "metadata": {},
   "source": [
    "# load the behavior\n",
    "\n",
    "importlib.reload(processing_parameters)\n",
    "importlib.reload(fl)\n",
    "\n",
    "# get the paths from the database using search_list\n",
    "all_paths, all_queries = fl.query_search_list()\n",
    "# print(all_paths)\n",
    "\n",
    "behavior_list = []\n",
    "# load the data\n",
    "for path, queries in zip(all_paths, all_queries):\n",
    "    \n",
    "    temp_data, _, _  = fl.load_preprocessing(path, queries)\n",
    "    behavior_list.append(temp_data)\n",
    "\n",
    "variable_list = processing_parameters.variable_list\n",
    "# for all the data, keep only the behavioral variables of interest\n",
    "data_behavior = []\n",
    "\n",
    "for idx, el in enumerate(behavior_list):\n",
    "    current_paths = all_paths[idx]\n",
    "    for idx2, el2 in enumerate(el):\n",
    "        \n",
    "        if 'habi' in current_paths[idx2]:\n",
    "            continue\n",
    "        try:\n",
    "            data_behavior.append(el2[variable_list+['mouse', 'datetime']])\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "del behavior_list"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8572dc",
   "metadata": {},
   "source": [
    "print(len(data_behavior))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17d9781",
   "metadata": {},
   "source": [
    "%%time\n",
    "# calculate prediction accuracy under defined conditions\n",
    "\n",
    "# define the parameter of interest\n",
    "target_parameter = 'cricket_0_mouse_distance'\n",
    "# get its ranges\n",
    "parameter_ranges = processing_parameters.tc_params[target_parameter]\n",
    "# define the number of splits\n",
    "number_bins = 3\n",
    "# determine the bin edges based on the range and number of splits\n",
    "bin_edges = np.linspace(parameter_ranges[0], parameter_ranges[1], number_bins+1)\n",
    "bin_centers = (bin_edges[1:] + bin_edges[:-1])/2\n",
    "\n",
    "# get the data meta\n",
    "data_meta = [[el.loc[0, 'mouse'], el.loc[0, 'datetime']] for el in data_behavior]\n",
    "data_meta = pd.DataFrame(data_meta, columns=['mouse', 'datetime'])\n",
    "# allocate memory for the output\n",
    "binned_prediction = []\n",
    "# for all the variables\n",
    "for feature in variable_list:\n",
    "    # get the meta\n",
    "    meta = predictions_meta[feature]\n",
    "    \n",
    "    # for all the trials\n",
    "    for idx, trial in enumerate(predictions[feature]):\n",
    "        # get the current meta\n",
    "        current_mouse = meta.loc[idx, 'mouse']\n",
    "        current_datetime = meta.loc[idx, 'datetime']\n",
    "\n",
    "        # get the data\n",
    "        current_data = [el for trial_idx, el in enumerate(data_behavior) \n",
    "                        if (current_mouse == data_meta.loc[trial_idx, 'mouse']) & (current_datetime == data_meta.loc[trial_idx, 'datetime'])]\n",
    "        if len(current_data) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            current_data = current_data[0]\n",
    "        \n",
    "        # get the target and feature\n",
    "        current_target = current_data[target_parameter]\n",
    "        current_data = current_data[feature]\n",
    "        # get the prediction\n",
    "        current_pred = trial[0]\n",
    "        # get the shuffle\n",
    "        current_shuffle = trial[1]\n",
    "        # split the data\n",
    "        bin_vector = np.digitize(current_target, bin_edges)\n",
    "\n",
    "        # for all the ranges\n",
    "        for bin_ in np.arange(1, number_bins+1):\n",
    "            # get the selection vector\n",
    "            selection_vector = bin_vector == bin_\n",
    "            # skip calculation if too few samples\n",
    "            if np.sum(selection_vector) >= 10:\n",
    "                # get the relevant portions of the data\n",
    "                bin_data = current_data[bin_vector == bin_]\n",
    "                bin_pred = current_pred[bin_vector == bin_]\n",
    "                bin_shuffle = current_shuffle[bin_vector == bin_]\n",
    "                # calculate the accuracy of the prediction in this range\n",
    "                pred_accuracy = stat.spearmanr(bin_data, bin_pred, nan_policy='omit')[0]\n",
    "                # calculate the accuracy of the shuffle in this range\n",
    "                shuffle_accuracy = stat.spearmanr(bin_data, bin_shuffle, nan_policy='omit')[0]\n",
    "            else:\n",
    "                pred_accuracy = np.nan\n",
    "                shuffle_accuracy = np.nan\n",
    "            # assemble the element to save\n",
    "            save_element = [pred_accuracy, shuffle_accuracy, bin_, current_mouse, current_datetime, feature]\n",
    "            # store\n",
    "            binned_prediction.append(save_element)\n",
    "# turn the output into a dataframe\n",
    "binned_prediction = pd.DataFrame(binned_prediction, columns=['prediction', 'shuffle', 'bin', 'mouse', 'datetime', 'feature'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "3efe2f48",
   "metadata": {},
   "source": [
    "# Average binned decoding matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58c8144",
   "metadata": {},
   "source": [
    "# plot the decoding accuracies across variables and bins\n",
    "\n",
    "# plot a matrix with the average accuracy across trials\n",
    "average_real = binned_prediction.drop(columns=['shuffle']).groupby(['bin', 'feature'], as_index=False).mean()\n",
    "average_shuffle = binned_prediction.drop(columns=['prediction']).groupby(['bin', 'feature'], as_index=False).mean()\n",
    "\n",
    "# allocate memory for the matrix\n",
    "real_matrix = np.zeros((number_bins, len(variable_list)))\n",
    "shuffle_matrix = np.zeros((number_bins, len(variable_list)))\n",
    "\n",
    "for idx, feature in enumerate(variable_list):\n",
    "    for bin_ in np.arange(1, number_bins+1):\n",
    "        target_idx = [row_idx for row_idx, el in average_real.iterrows() if (el['bin'] == bin_) & (el['feature'] == feature)][0]\n",
    "\n",
    "        real_matrix[bin_-1, idx] = average_real.loc[target_idx, 'prediction']\n",
    "        shuffle_matrix[bin_-1, idx] = average_shuffle.loc[target_idx, 'shuffle']\n",
    "\n",
    "raster0 = hv.Raster(real_matrix)\n",
    "raster1 = hv.Raster(shuffle_matrix)\n",
    "\n",
    "xticks = [(idx+0.5, label_dict[el]) for idx, el in enumerate(variable_list)]\n",
    "yticks = [(idx+0.5, f'{el:0.2f}') for idx, el in enumerate(bin_centers)]\n",
    "raster0.opts(width=1000, height=500, tools=['hover'], cmap='Magma', \n",
    "             xticks=xticks, yticks=yticks, xrotation=45, ylabel=label_dict[target_parameter], xlabel='', colorbar=True, clim=(-0.05, 0.4))\n",
    "raster1.opts(width=1000, height=500, tools=['hover'], cmap='Magma', \n",
    "             xticks=xticks, yticks=yticks, xrotation=45, ylabel=label_dict[target_parameter], xlabel='', colorbar=True, clim=(-0.05, 0.4))\n",
    "# raster = (raster0+raster1).cols(1)\n",
    "# raster.opts(opts.Raster(width=800, height=400, tools=['hover'], cmap='Magma', xticks=xticks, yticks=yticks, xrotation=45, ylabel=label_dict[target_parameter], xlabel='', colorbar=True))\n",
    "# raster\n",
    "\n",
    "# assemble the file name\n",
    "save_name = os.path.join(save_path, '_'.join((target_document, 'Reg_binned', target_parameter, 'real')) + '.png')\n",
    "# save the figure\n",
    "fig0 = fp.save_figure(raster0, save_name, fig_width=15, dpi=1200, fontsize=target_document, target='both')\n",
    "\n",
    "# assemble the file name\n",
    "save_name = os.path.join(save_path, '_'.join((target_document, 'Reg_binned', target_parameter, 'shuffle')) + '.png')\n",
    "# save the figure\n",
    "fig1 = fp.save_figure(raster1, save_name, fig_width=15, dpi=1200, fontsize=target_document, target='both')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06077c84",
   "metadata": {},
   "source": [
    "def layout_function(input_data, target_field, color_dict, marker):\n",
    "    # reset index\n",
    "    current_data = input_data.reset_index(drop=True)\n",
    "    # filter the days after 10\n",
    "    selection_vector = current_data['datetime'].to_numpy() < 6\n",
    "    current_data = current_data.iloc[selection_vector, :]\n",
    "    # slightly shift the datetime to see all points based on bin\n",
    "    current_data['datetime'] += 0.3*(bin_name-1)\n",
    "#     # rectify the values\n",
    "#     selection_vector = np.argwhere(current_data[target_field].to_numpy() < 0).flatten()\n",
    "#     current_data.loc[selection_vector, [target_field]] = 0\n",
    "\n",
    "    # create the plot\n",
    "    curve = hv.Scatter(current_data, kdims='datetime', vdims=target_field, label='bin' + str(bin_name))\n",
    "    curve.opts(width=400, height=300, color=color_dict[bin_name], title=label_dict[feature_name], marker=marker, size=5)\n",
    "    \n",
    "    return curve"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e639f838",
   "metadata": {},
   "source": [
    "# plot lines to show error\n",
    "\n",
    "# define the colors for the bins\n",
    "bin_colors = {\n",
    "    1: '#0000FF',\n",
    "    2: '#00FF00',\n",
    "    3: '#FF0000',\n",
    "}\n",
    "\n",
    "bin_colors_shuffle = {\n",
    "    1: '#880088',\n",
    "    2: '#008800',\n",
    "    3: '#880000',\n",
    "}\n",
    "# allocate the plot dict\n",
    "plot_dict = {}\n",
    "# for all the mice\n",
    "for mouse_name, mouse_data in binned_prediction.groupby(['mouse'], as_index=False):\n",
    "    # copy the mouse data\n",
    "    current_mouse = mouse_data.copy()\n",
    "    # get the datetime info and 0 it\n",
    "    date_time = np.array([datetime.datetime.strptime(el, '%Y-%m-%d %H:%M:%S') for el in current_mouse['datetime'].to_numpy()])\n",
    "    date_time -= np.min(date_time)\n",
    "    date_time = [el.days for el in date_time]\n",
    "    # replace the original time\n",
    "    current_mouse['datetime'] = date_time\n",
    "    # average across dates\n",
    "    average_real_mouse = current_mouse.drop(columns=['shuffle']).groupby(['datetime', 'bin', 'feature'], as_index=False).mean()\n",
    "    average_shuffle_mouse = current_mouse.drop(columns=['prediction']).groupby(['datetime', 'bin', 'feature'], as_index=False).mean()\n",
    "    \n",
    "    # for all the features\n",
    "    for (feature_name, bin_name), real_data in average_real_mouse.groupby(['feature', 'bin']):\n",
    "        \n",
    "        # create a new key if not there\n",
    "        if feature_name not in plot_dict.keys():\n",
    "            plot_dict[feature_name] = []\n",
    "        \n",
    "        # plot and save\n",
    "        plot_dict[feature_name].append(layout_function(real_data, 'prediction', bin_colors, 'o'))\n",
    "\n",
    "    # for all the features\n",
    "    for (feature_name, bin_name), shuffle_data in average_shuffle_mouse.groupby(['feature', 'bin']):\n",
    "        \n",
    "        # create a new key if not there\n",
    "        if feature_name not in plot_dict.keys():\n",
    "            plot_dict[feature_name] = []\n",
    "        # plot and save\n",
    "        plot_dict[feature_name].append(layout_function(shuffle_data, 'shuffle', bin_colors_shuffle, '*'))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb37b4f",
   "metadata": {},
   "source": [
    "# combine the overlays and plot\n",
    "\n",
    "# allocate the layout list\n",
    "layout_list = []\n",
    "# for all the features\n",
    "for feature in variable_list:\n",
    "    layout_list.append(hv.Overlay(plot_dict[feature]))\n",
    "    \n",
    "layout = hv.Layout(layout_list).cols(4).opts(shared_axes=False)\n",
    "layout"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bc1020",
   "metadata": {},
   "source": [
    "print(binned_prediction.shape)"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-prey_capture] *",
   "language": "python",
   "name": "conda-env-.conda-prey_capture-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
