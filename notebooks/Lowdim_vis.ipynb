{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5444f61a",
   "metadata": {},
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath(r'D:\\Code Repos\\prey_capture'))\n",
    "sys.path.insert(0, os.path.abspath(r'D:\\Code Repos\\prey_capture\\mine_pub'))\n",
    "\n",
    "import panel as pn\n",
    "import holoviews as hv\n",
    "from holoviews import opts, dim\n",
    "hv.extension('bokeh')\n",
    "from bokeh.resources import INLINE\n",
    "\n",
    "import paths\n",
    "import importlib\n",
    "import functions_plotting as fp\n",
    "from functions_plotting import format_figure as ff\n",
    "import functions_bondjango as bd\n",
    "import functions_loaders as fl\n",
    "import processing_parameters\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import scipy.stats as stat\n",
    "import scipy.signal as signal\n",
    "import datetime\n",
    "import umap\n",
    "import sklearn.decomposition as decomp\n",
    "import sklearn.preprocessing as preproc\n",
    "import sklearn.cross_decomposition as xdecomp\n",
    "import sklearn.cluster as cluster\n",
    "\n",
    "from mine_pub.mine import Mine\n",
    "\n",
    "import PSID\n",
    "from PSID.evaluation import evalPrediction"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9b7e49",
   "metadata": {},
   "source": [
    "# set up the figure config\n",
    "importlib.reload(fp)\n",
    "importlib.reload(processing_parameters)\n",
    "# define the target saving path\n",
    "save_path = os.path.join(paths.figures_path, 'Low_dim')\n",
    "\n",
    "# define the printing mode\n",
    "save_mode = True\n",
    "# define the target document\n",
    "target_document = 'paper'\n",
    "# set up the figure theme\n",
    "fp.set_theme()\n",
    "# load the label dict\n",
    "label_dict = processing_parameters.label_dictionary\n",
    "variable_list = processing_parameters.variable_list"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786589c3",
   "metadata": {},
   "source": [
    "importlib.reload(processing_parameters)\n",
    "importlib.reload(fl)\n",
    "\n",
    "# get the paths from the database using search_list\n",
    "all_paths, all_queries = fl.query_search_list()\n",
    "# print(all_paths)\n",
    "\n",
    "data_list = []\n",
    "# load the data\n",
    "for path, queries in zip(all_paths, all_queries):\n",
    "    \n",
    "    data, _, _  = fl.load_preprocessing(path, queries)\n",
    "    data_list.append(data)\n",
    "\n",
    "# print(all_paths)\n",
    "print(f'Number of trials: {len(data_list)}')\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3372e6",
   "metadata": {},
   "source": [
    "def cvpca_variance(train_trajectories, test_trajectories):\n",
    "    \"\"\"Calculate the covariance between the test and train sets\"\"\"\n",
    "    # get the relevant dimensions\n",
    "    time_points = test_trajectories.shape[0]\n",
    "    components = test_trajectories.shape[1]\n",
    "    \n",
    "    # allocate memory for the covariances\n",
    "    covariances = np.zeros((components, 1))\n",
    "    # for all the components\n",
    "    for component in np.arange(components):\n",
    "        # get the components\n",
    "        current_test = test_trajectories[component, :]\n",
    "        current_train = train_trajectories[component, :]\n",
    "        # get the means\n",
    "        mean_test = np.mean(current_test)\n",
    "        mean_train = np.mean(current_train)\n",
    "        current_component = (1/(time_points - 1)) * np.sum((current_test - mean_test)*(current_train - mean_train))\n",
    "        \n",
    "        covariances[component] = current_component\n",
    "    return covariances "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a935c43b",
   "metadata": {},
   "source": [
    "# Run cvPCA on the data\n",
    "\n",
    "# define a day and mouse\n",
    "# target_day = '2021-04-02'\n",
    "# target_mouse = 'DG_210202_a'\n",
    "\n",
    "# target_day = '2020-08-19'\n",
    "# target_mouse = 'DG_200701_a'\n",
    "# ['2020-08-06' 'DG_200617_b']\n",
    "# target_day = '2020-08-06'\n",
    "# target_mouse = 'DG_200617_b'\n",
    "# 2021-05-03' 'DG_210202_a\n",
    "target_day = '2021-05-03' \n",
    "target_mouse = 'DG_210202_a'\n",
    "\n",
    "# get the relevant trials\n",
    "target_trials = [el for el in data_list[0] if (target_day in el.loc[0, 'datetime']) & (target_mouse in el.loc[0, 'mouse'])]\n",
    "\n",
    "# test and train with one trial at a time\n",
    "# zscore the whole data\n",
    "\n",
    "# get the training data\n",
    "train_trials = [el for idx1, el in enumerate(target_trials) if idx1%2 == 0]\n",
    "train_trials = pd.concat(train_trials, axis=0)\n",
    "# get the calcium\n",
    "cells = [el for el in train_trials.columns if 'cell' in el]\n",
    "not_cells = [el for el in train_trials.columns if 'cell' not in el]\n",
    "train_calcium = train_trials.loc[:, cells].to_numpy()\n",
    "# generate the scaler for zscoring\n",
    "scaler = preproc.StandardScaler().fit(train_calcium)\n",
    "# scaler = preproc.MinMaxScaler().fit(train_calcium)\n",
    "# zscore the train data\n",
    "# train_calcium = scaler.transform(train_calcium)\n",
    "# create the fitter\n",
    "pca = decomp.PCA(whiten=True)\n",
    "# fit the train data\n",
    "train_components = pca.fit_transform(train_calcium)\n",
    "\n",
    "# preprocess the test data\n",
    "test_trials = [el for idx1, el in enumerate(target_trials) if idx1%2 != 0]\n",
    "test_trials = pd.concat(test_trials, axis=0)\n",
    "test_calcium = test_trials.loc[:, cells].to_numpy()\n",
    "test_calcium = scaler.transform(test_calcium)\n",
    "\n",
    "# transform the test data\n",
    "test_components = pca.transform(test_calcium)\n",
    "\n",
    "# calculate explained variance to select components\n",
    "covariances = cvpca_variance(train_components, test_components)\n",
    "\n",
    "print(covariances.shape)\n",
    "# determine the explained variance\n",
    "# store\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ba29b1",
   "metadata": {},
   "source": [
    "# plot the sorted explained covariances\n",
    "hv.extension('bokeh')\n",
    "x = np.arange(covariances.shape[0])\n",
    "hv.Scatter((x, np.sort(covariances[:, 0])))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e897f29a",
   "metadata": {},
   "source": [
    "# calculate the sorted cvPCs\n",
    "\n",
    "# get the sorted indices\n",
    "idx_sort = np.argsort(covariances, axis=0).flatten()\n",
    "\n",
    "# get the full data components\n",
    "full_data = pd.concat(target_trials, axis=0)\n",
    "full_calcium = full_data.loc[:, cells].to_numpy()\n",
    "# full_calcium = scaler.transform(full_calcium)\n",
    "full_components = pca.transform(full_calcium)\n",
    "\n",
    "# sort the components\n",
    "sorted_comps = full_components[:, np.flip(idx_sort)]\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da7d8fb",
   "metadata": {},
   "source": [
    "# plot the calcium activity\n",
    "hv.extension('bokeh')\n",
    "plot_calcium = full_calcium.copy()\n",
    "normalized_calcium = (plot_calcium-plot_calcium.min(axis=0))/(plot_calcium.max(axis=0)-plot_calcium.min(axis=0))\n",
    "\n",
    "raster = hv.Raster((normalized_calcium).T)\n",
    "raster.opts(width=1000, height=800, cmap='Viridis', tools=['hover'])\n",
    "\n",
    "raster"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86a3518",
   "metadata": {},
   "source": [
    "# plot the sorted cvPCs\n",
    "hv.extension('bokeh')\n",
    "\n",
    "normalized_comps = (sorted_comps-sorted_comps.min(axis=0))/(sorted_comps.max(axis=0)-sorted_comps.min(axis=0))\n",
    "normalized_comps = normalized_comps[:, :3]\n",
    "raster = hv.Raster((normalized_comps).T)\n",
    "raster.opts(width=1000, height=800, cmap='Viridis')\n",
    "\n",
    "raster"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d52a70",
   "metadata": {},
   "source": [
    "# do line plots\n",
    "hv.extension('bokeh')\n",
    "x = np.arange(normalized_comps.shape[0])\n",
    "plot_list = []\n",
    "for el in np.arange(normalized_comps.shape[1]):\n",
    "    plot = hv.Curve((x, normalized_comps[:, el]))\n",
    "    plot.opts(width=1000)\n",
    "    plot_list.append(plot)\n",
    "    \n",
    "hv.Overlay(plot_list)\n",
    "    "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec380c2d",
   "metadata": {},
   "source": [
    "# plot the actual trajectories\n",
    "hv.extension('plotly')\n",
    "\n",
    "target_parameter = 'cricket_0_mouse_distance'\n",
    "\n",
    "# define the target dimensions\n",
    "tar_dim = np.array([0, 1, 2]) + 0\n",
    "\n",
    "# allocate memory for the output list\n",
    "plot_list = []\n",
    "\n",
    "trial = normalized_comps.copy()\n",
    "\n",
    "parameter = full_data[target_parameter].to_numpy()\n",
    "parameter[np.isnan(parameter)] = 0\n",
    "parameter = (parameter-parameter.min())/(parameter.max()-parameter.min())\n",
    "#         parameter = np.log(parameter)\n",
    "\n",
    "plot = hv.Scatter3D((trial[:, tar_dim[0]], trial[:, tar_dim[1]], trial[:, tar_dim[2]]))\n",
    "# plot.opts(size=2, height=800, width=800, color=parameter, colorbar=True)\n",
    "plot.opts(height=800, width=800, color=parameter, colorbar=True)\n",
    "#     plot_list[idx].opts(hv.opts.Scatter3D(cmap='viridis'))\n",
    "plot\n",
    "\n",
    "# ov = hv.Overlay(plot_list)\n",
    "# ov\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb6a22f",
   "metadata": {},
   "source": [
    "%%time\n",
    "# run PSID on the data\n",
    "\n",
    "# define the test/train percentage\n",
    "test_perc = 0.3\n",
    "\n",
    "# get the unique dates and mice\n",
    "unique_dates_mice = np.unique([(el.loc[0, 'datetime'][:10], el.loc[0, 'mouse']) for el in data_list[0]], axis=0)\n",
    "\n",
    "# allocate a list to store the psid objects\n",
    "psid_list = []\n",
    "\n",
    "# for all the pairs\n",
    "for pair in unique_dates_mice:\n",
    "    # get the relevant trials\n",
    "    # target_trials = [el for el in data_list[0] if (target_day in el.loc[0, 'datetime']) & (target_mouse in el.loc[0, 'mouse'])]\n",
    "    target_trials = [el for el in data_list[0] if (pair[0] in el.loc[0, 'datetime']) & (pair[1] in el.loc[0, 'mouse'])]\n",
    "#     target_trials = data_list[0]\n",
    "\n",
    "    # define the target behaviors\n",
    "    target_behavior = variable_list\n",
    "\n",
    "    # allocate memory for the training and test sets\n",
    "    ca_train = []\n",
    "    ca_test = []\n",
    "    beh_train = []\n",
    "    beh_test = []\n",
    "    # for all the trials\n",
    "    for trial in target_trials:\n",
    "\n",
    "        # get the available columns\n",
    "        labels = list(trial.columns)\n",
    "        cells = [el for el in labels if 'cell' in el]\n",
    "        # get the cell data\n",
    "        calcium_data = np.array(trial[cells].copy())\n",
    "        # get rid of the super small values\n",
    "        calcium_data[np.isnan(calcium_data)] = 0\n",
    "\n",
    "        try:\n",
    "            # get the parameter\n",
    "            beh_data = trial[target_behavior].to_numpy()\n",
    "\n",
    "            # smooth the parameter\n",
    "            beh_data = signal.medfilt(beh_data, (21, 1))\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "        # skip if empty\n",
    "        if (calcium_data.shape[0] == 0) | (calcium_data.shape[1] < 3):\n",
    "            continue\n",
    "\n",
    "    #     downsamp = 1\n",
    "    #     # bin the data\n",
    "    #     if downsamp > 1:\n",
    "    #         beh_data = ss.decimate(beh_data, downsamp, axis=0)\n",
    "    #         calcium_data = ss.decimate(calcium_data, downsamp, axis=0)\n",
    "\n",
    "        # get the threshold index\n",
    "        threshold_idx = int(calcium_data.shape[0]*(test_perc))\n",
    "        # split the data\n",
    "        ca_trial_train = calcium_data[threshold_idx:, :] \n",
    "        ca_trial_test = calcium_data[:threshold_idx, :] \n",
    "        beh_trial_train = beh_data[threshold_idx:, :]\n",
    "        beh_trial_test = beh_data[:threshold_idx, :] \n",
    "\n",
    "        # store the data\n",
    "        ca_train.append(ca_trial_train)\n",
    "        ca_test.append(ca_trial_test)\n",
    "        beh_train.append(beh_trial_train)\n",
    "        beh_test.append(beh_trial_test)    \n",
    "\n",
    "    # skip if empty arrays\n",
    "    if len(ca_train) == 0:\n",
    "        continue\n",
    "    # scale the data\n",
    "    # ca_scaler = preprocessing.StandardScaler().fit(np.concatenate(ca_train))\n",
    "    # beh_scaler = preprocessing.StandardScaler().fit(np.concatenate(beh_train))\n",
    "\n",
    "    # ca_train = [ca_scaler.transform(el) for el in ca_train]\n",
    "    # ca_test = [ca_scaler.transform(el) for el in ca_test]\n",
    "    # beh_train = [beh_scaler.transform(el) for el in beh_train]\n",
    "    # beh_test = [beh_scaler.transform(el) for el in beh_test]\n",
    "\n",
    "    # scale each trial separately\n",
    "    ca_scaler_list = [preproc.StandardScaler().fit(el) for el in ca_train]\n",
    "    beh_scaler_list = [preproc.StandardScaler().fit(el) for el in beh_train]\n",
    "\n",
    "    ca_train = [ca_scaler_list[idx].transform(el) for idx, el in enumerate(ca_train)]\n",
    "    ca_test = [ca_scaler_list[idx].transform(el) for idx, el in enumerate(ca_test)]\n",
    "    beh_train = [beh_scaler_list[idx].transform(el) for idx, el in enumerate(beh_train)]\n",
    "    beh_test = [beh_scaler_list[idx].transform(el) for idx, el in enumerate(beh_test)]\n",
    "\n",
    "\n",
    "    # train the PSID model\n",
    "    idSys = PSID.PSID(ca_train, beh_train, nx=20, n1=10, i=20)\n",
    "    # idSys = PSID.PSID(ca_train, beh_train, nx=1, n1=1, i=20) # for cricket distance\n",
    "    # idSys = PSID.PSID(ca_train, beh_train, nx=20, n1=10, i=35)\n",
    "    \n",
    "    # store the element\n",
    "    psid_list.append([pair, idSys, ca_scaler_list])\n",
    "\n",
    "    # allocate memory for the predictions\n",
    "    beh_pred = []\n",
    "    ca_pred = []\n",
    "    latent_pred = []\n",
    "    # predict each trial\n",
    "    for trial in ca_test:\n",
    "        beh_p, ca_p, latent_p = idSys.predict(trial)\n",
    "        beh_pred.append(beh_p)\n",
    "        ca_pred.append(ca_p)\n",
    "        latent_pred.append(latent_p)\n",
    "\n",
    "    combo_beh_test = np.vstack(beh_test)\n",
    "    combo_beh_pred = np.vstack(beh_pred)\n",
    "\n",
    "    combo_ca_test = np.vstack(ca_test)\n",
    "    combo_ca_pred = np.vstack(ca_pred)\n",
    "\n",
    "    R2TrialBased_beh = evalPrediction(combo_beh_test, combo_beh_pred, 'CC')\n",
    "    R2TrialBased_ca = evalPrediction(combo_ca_test, combo_ca_pred, 'CC')\n",
    "\n",
    "    print('Number of cells that have a larger than 0 CC:', np.sum(R2TrialBased_ca != 0))\n",
    "    print('Mean Ca CC:', np.nanmean(R2TrialBased_ca))\n",
    "    print('CC of behavior:', R2TrialBased_beh)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87b709e",
   "metadata": {},
   "source": [
    "print(psid_list[0][1].Cz.shape)\n",
    "plot_list = []\n",
    "# for all the models\n",
    "for model in psid_list[:10]:\n",
    "    # get the idsys element\n",
    "    idsys = model[1]\n",
    "    \n",
    "    # get the target matrix\n",
    "    target_matrix = idsys.Cz\n",
    "    \n",
    "    plot = hv.Raster(target_matrix)\n",
    "    \n",
    "    plot_list.append(plot)\n",
    "layout = hv.Layout(plot_list)\n",
    "layout"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa0a7da",
   "metadata": {},
   "source": [
    "%%time\n",
    "# run the model on all experiments\n",
    "\n",
    "# allocate memory for the predictions\n",
    "final_beh = []\n",
    "final_ca = []\n",
    "final_latent = []\n",
    "final_pairs = []\n",
    "final_scaled_beh = []\n",
    "# for all the pairs\n",
    "for pair in unique_dates_mice:\n",
    "    # get the trials\n",
    "    target_trials = [el for el in data_list[0] if (pair[0] in el.loc[0, 'datetime']) & (pair[1] in el.loc[0, 'mouse'])]\n",
    "    tag_vector = [False if (el[0][0] == pair[0]) & (el[0][1] == pair[1]) else True for el in psid_list]\n",
    "    # see if the pair was calculated\n",
    "    if all(tag_vector):\n",
    "        continue\n",
    "        \n",
    "    # get the index of the corresponding psid element\n",
    "    idx = np.argwhere(~np.array(tag_vector))[0][0]\n",
    "    \n",
    "    # get the corresponding psid element\n",
    "    idSys = psid_list[idx][1]\n",
    "    scalers = psid_list[idx][2]\n",
    "    # predict each trial\n",
    "    for trial_idx, trial in enumerate(target_trials):\n",
    "\n",
    "        # get the available columns\n",
    "        labels = list(trial.columns)\n",
    "        cells = [el for el in labels if 'cell' in el]\n",
    "        \n",
    "        # get the cell data\n",
    "        calcium_data = np.array(trial[cells].copy())\n",
    "        # skip if empty\n",
    "        if (calcium_data.shape[0] == 0) | (calcium_data.shape[1] < 3):\n",
    "            continue\n",
    "        \n",
    "        # scale the data\n",
    "        # get rid of the super small values\n",
    "        calcium_data[np.isnan(calcium_data)] = 0\n",
    "        calcium_data = scalers[trial_idx].transform(calcium_data)\n",
    "        \n",
    "        # predict and store\n",
    "        beh_p, ca_p, latent_p = idSys.predict(calcium_data)\n",
    "        final_beh.append(beh_p)\n",
    "        final_ca.append(ca_p)\n",
    "        final_latent.append(latent_p)\n",
    "        final_pairs.append(pair)\n",
    "        # get the behavior\n",
    "        final_scaled_beh.append(trial[variable_list].to_numpy())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49403b14",
   "metadata": {},
   "source": [
    "# plot individual PSID latents\n",
    "\n",
    "hv.extension('bokeh')\n",
    "plot_data = np.vstack(final_latent).copy()\n",
    "x = np.arange(plot_data.shape[0])\n",
    "plot_list = []\n",
    "for el in np.arange(plot_data.shape[1]):\n",
    "    plot = hv.Curve((x, plot_data[:, el]))\n",
    "    plot.opts(width=1000)\n",
    "    plot_list.append(plot)\n",
    "    \n",
    "hv.Overlay(plot_list)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7252e1",
   "metadata": {},
   "source": [
    "def hook_fun(plot, element):\n",
    "    b = plot.state\n",
    "#     print(element.colorscale)\n",
    "#     print(plot.colorscale)\n",
    "    \n",
    "#     print(element.__dict__)\n",
    "#     print(b['layout'])\n",
    "    b['layout']['colorway']='Viridis'"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777a2fa3",
   "metadata": {},
   "source": [
    "# plot the PSID dynamics reconstruction\n",
    "# hv.extension('plotly')\n",
    "    \n",
    "# # opts.defaults(opts.Scatter3D(cmap='viridis'))\n",
    "# # define the target parameter\n",
    "# target_parameter = 'cricket_0_delta_heading'\n",
    "# # print(normal_data[0].columns[:40])\n",
    "\n",
    "# # define the target dimensions\n",
    "# tar_dim = np.array([0, 1, 2]) + 0\n",
    "\n",
    "# # allocate memory for the output list\n",
    "# plot_list = []\n",
    "\n",
    "# target_trial = [0, 8]\n",
    "# # for idx, trial in enumerate(final_latent[target_trial[0]:target_trial[1]+1]):\n",
    "# for idx, trial in enumerate(final_latent):\n",
    "    \n",
    "#     # get a parameter\n",
    "#     try:\n",
    "#         parameter = target_trials[idx][target_parameter]\n",
    "#         parameter = (parameter-parameter.min())/(parameter.max()-parameter.min())\n",
    "# #         parameter = np.log(parameter)\n",
    "    \n",
    "#         plot_list.append(hv.Scatter3D((trial[:, tar_dim[0]], trial[:, tar_dim[1]], trial[:, tar_dim[2]])))\n",
    "#         plot_list[idx].opts(size=2, height=800, width=800, color=parameter, colorbar=True)\n",
    "# #     plot_list[idx].opts(hv.opts.Scatter3D(cmap='viridis'))\n",
    "#     except KeyError:\n",
    "# #             parameter = np.zeros(trial.shape[0])\n",
    "#         continue\n",
    "\n",
    "# ov = hv.Overlay(plot_list)\n",
    "# ov\n",
    "# hv.help(hv.Scatter3D)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41a9be9",
   "metadata": {},
   "source": [
    "# calculate a correlation matrix between latents and behavioral variables\n",
    "\n",
    "correlation_list = []\n",
    "pvalue_list = []\n",
    "for idx, trial in enumerate(final_latent):\n",
    "#     behavior = trial[variable_list].to_numpy().copy()\n",
    "    behavior = final_scaled_beh[idx]\n",
    "    latents = trial\n",
    "    correlation_matrix, pvalue_matrix = stat.spearmanr(behavior, latents, nan_policy='omit')\n",
    "    correlation_list.append(correlation_matrix)\n",
    "    pvalue_list.append(pvalue_matrix)\n",
    "\n",
    "correlation_matrix = np.nanmean(np.array(correlation_list), axis=0)\n",
    "pvalue_matrix = np.nanmean(np.array(pvalue_list), axis=0)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84da8cf0",
   "metadata": {},
   "source": [
    "# plot the correlation\n",
    "hv.extension('bokeh')\n",
    "\n",
    "yticks = [(idx+0.5, label_dict[el]) for idx, el in enumerate(variable_list)]\n",
    "# ticks += [(idx+0.5 + len(variable_list), ' '.join(('latent', str(el)))) for idx, el in enumerate(np.arange(latents.shape[1]))]\n",
    "xticks = [(idx+0.5, ' '.join(('latent', str(el)))) for idx, el in enumerate(np.arange(latents.shape[1]))]\n",
    "\n",
    "\n",
    "raster = hv.Raster(correlation_matrix[:len(variable_list), len(variable_list):])\n",
    "# raster = hv.Raster(pvalue_matrix)\n",
    "raster.opts(width=900, height=800, colorbar=True, cmap='RdBu', tools=['hover'], xticks=xticks, yticks=yticks, xrotation=45, clim=(-1, 1))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b9f073",
   "metadata": {},
   "source": [
    "# plot an array of correlation matrices\n",
    "\n",
    "# allocate the plot list\n",
    "plot_list = []\n",
    "# get the correlations and turn into an array\n",
    "correlation_array = np.array(correlation_list).copy()\n",
    "# for all the plots\n",
    "for pair in unique_dates_mice:\n",
    "    \n",
    "    # find the indexes corresponding to this pair\n",
    "    pair_idx = [idx for idx, el in enumerate(final_pairs) if (el[0] == pair[0]) & (el[1] == pair[1])]\n",
    "    # get the current matrix\n",
    "    current_matrix = np.nanmean(correlation_array[pair_idx], axis=0)[:len(variable_list), len(variable_list):]\n",
    "    # create the plot\n",
    "    raster = hv.Raster(current_matrix)\n",
    "    title = ' '.join((pair[0], pair[1]))\n",
    "    raster.opts(width=400, height=400, tools=['hover'], cmap='RdBu', clim=(-1, 1), xticks=xticks, yticks=yticks, xrotation=45, xlabel='', ylabel='', title=title)\n",
    "    \n",
    "    #store\n",
    "    plot_list.append(raster)\n",
    "\n",
    "overall = hv.Layout(plot_list).cols(4)\n",
    "overall"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3c59c1",
   "metadata": {},
   "source": [
    "# plot the marginalized correlations for each variable\n",
    "\n",
    "# allocate the plot list\n",
    "plot_list = []\n",
    "correlation_array = np.array(correlation_list).copy()\n",
    "\n",
    "yticks = [(idx, label_dict[el]) for idx, el in enumerate(variable_list)]\n",
    "xticks = [(idx+0.5, ' '.join(('latent', str(el)))) for idx, el in enumerate(np.arange(latents.shape[1]))]\n",
    "\n",
    "# allocate a df to generate a box plot\n",
    "latents_boxplot = []\n",
    "# for all the plots\n",
    "for pair in unique_dates_mice:\n",
    "    \n",
    "    # find the indexes corresponding to this pair\n",
    "    pair_idx = [idx for idx, el in enumerate(final_pairs) if (el[0] == pair[0]) & (el[1] == pair[1])]\n",
    "    # get the current matrix\n",
    "#     current_matrix = np.nanmax(np.abs(np.nanmean(correlation_array[pair_idx], axis=0)[:len(variable_list), len(variable_list):]), axis=1)\n",
    "    # for the single trial matrices\n",
    "    for trial_idx in pair_idx:\n",
    "        current_matrix = np.nanmax(np.abs(correlation_array[trial_idx][:len(variable_list), len(variable_list):]), axis=1)\n",
    "        # add as a dataframe to the list\n",
    "        latents_boxplot.append(pd.DataFrame(np.vstack([variable_list, current_matrix]).T, columns=['Feature', 'Average_corr']))\n",
    "#     # create the plot\n",
    "#     raster = hv.Scatter(current_matrix)\n",
    "#     title = ' '.join((pair[0], pair[1]))\n",
    "#     raster.opts(width=400, height=400, tools=['hover'], xticks=yticks, yticks=None, xrotation=45, xlabel='', ylabel='', title=title)\n",
    "    \n",
    "#     #store\n",
    "#     plot_list.append(raster)\n",
    "\n",
    "# overall = hv.Layout(plot_list).cols(4)\n",
    "# overall"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33189f3",
   "metadata": {},
   "source": [
    "%%time\n",
    "# generate shuffles for the average correlation calculation\n",
    "\n",
    "# define the number of shuffles\n",
    "number_shuffles = 100\n",
    "# allocate memory for the shuffles\n",
    "shuffle_list = []\n",
    "\n",
    "# for all the shuffles\n",
    "for shuff in np.arange(number_shuffles):\n",
    "    # allocate a list for these shuffles\n",
    "    current_shuffle = []\n",
    "    # for all the plots\n",
    "    for pair in unique_dates_mice:\n",
    "\n",
    "        # find the indexes corresponding to this pair\n",
    "        pair_idx = [idx for idx, el in enumerate(final_pairs) if (el[0] == pair[0]) & (el[1] == pair[1])]\n",
    "#         # get the current matrix\n",
    "#         current_matrix = np.nanmean(correlation_array[pair_idx], axis=0)[:len(variable_list), len(variable_list):]\n",
    "        # for the single trial matrices\n",
    "        for trial_idx in pair_idx:\n",
    "            current_matrix = correlation_array[trial_idx][:len(variable_list), len(variable_list):]\n",
    "            # shuffle the rows of the matrix\n",
    "            row_idx = np.random.choice(np.arange(len(variable_list)), size=len(variable_list), replace=False)\n",
    "            current_matrix = np.nanmax(np.abs(current_matrix[row_idx, :]), axis=1)\n",
    "            # create a dataframe\n",
    "            current_shuffle.append(pd.DataFrame(np.vstack([variable_list, current_matrix]).T, columns=['Feature', 'Average_corr']))\n",
    "    # store\n",
    "    shuffle_list.append(pd.concat(current_shuffle, axis=0).reset_index(drop=True))\n",
    "# combine into a final dataframe\n",
    "shuffle_list = pd.concat(shuffle_list, axis=0).reset_index(drop=True)\n",
    "\n",
    "    \n",
    "    "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4aad0a9",
   "metadata": {},
   "source": [
    "# get a boxplot of the relationship between latents and variables\n",
    "\n",
    "latents_df = pd.concat(latents_boxplot, axis=0).reset_index(drop=True)\n",
    "latents_df['Average_corr'] = latents_df['Average_corr'].astype(float)\n",
    "shuffle_list['Average_corr'] = shuffle_list['Average_corr'].astype(float)\n",
    "\n",
    "whisker0 = hv.BoxWhisker(latents_df, ['Feature'], ['Average_corr'])\n",
    "whisker0.opts(width=800, height=800, xrotation=45, ylabel='Average Correlation', xlabel='', box_line_width=1, whisker_line_width=1, tools=['hover'])\n",
    "\n",
    "whisker1 = hv.BoxWhisker(shuffle_list, ['Feature'], ['Average_corr'])\n",
    "whisker = whisker0*whisker1\n",
    "whisker"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1cddc0",
   "metadata": {},
   "source": [
    "# align cross day latents using CCA\n",
    "\n",
    "# define the target mouse\n",
    "target_mouse = 'DG_210202_a'\n",
    "\n",
    "\n",
    "pair_idx = [idx for idx, el in enumerate(final_pairs) if el[1] == target_mouse]\n",
    "\n",
    "target_latents = [final_latent[el] for el in pair_idx]\n",
    "target_days = [final_pairs[el][0] for el in pair_idx]\n",
    "\n",
    "\n",
    "    \n"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
