{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Functions to make hls maps\n",
    "\n",
    "Created on Mon Jan 4, 2022\n",
    "\n",
    "@author: pgoltstein\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import importlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.morphology\n",
    "import skimage.filters\n",
    "from skimage.transform import resize as imresize\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from skimage.io import imread\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath(r'C:/Users/mmccann/repos/bonhoeffer/prey_capture/'))\n",
    "\n",
    "import paths\n",
    "import processing_parameters\n",
    "import functions_bondjango as bd\n",
    "import functions_data_handling as fdh\n",
    "from functions_kinematic import wrap\n",
    "\n",
    "# import xarray as xr\n",
    "# from minian.motion_correction import apply_transform, estimate_motion\n",
    "# from minian.preprocessing import denoise, remove_background\n",
    "# from minian.utilities import get_optimal_chk, optimize_chunk, rechunk_like, load_videos\n",
    "# from dask.distributed import Client, LocalCluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trialimages(plane_no, datapath):\n",
    "    \"\"\" Loads a saved dfof responsemap of each trial into a 3d matrix [y_res,x_res,trials]\n",
    "    \"\"\"\n",
    "\n",
    "    # Prepare filenames\n",
    "    savedatapath = os.path.join(datapath,\"hls\")\n",
    "    bsimages_file = os.path.join(savedatapath, \"trialimages-bs-plane{}.npy\".format(plane_no))\n",
    "    stimimages_file = os.path.join(savedatapath, \"trialimages-stim-plane{}.npy\".format(plane_no))\n",
    "\n",
    "    # Load df_images\n",
    "    print(\"Loading baseline images from: {}\".format(bsimages_file))\n",
    "    bs_images = np.load(bsimages_file)\n",
    "    print(\"Loading stimulus images from: {}\".format(stimimages_file))\n",
    "    stim_images = np.load(stimimages_file)\n",
    "\n",
    "    # Return data\n",
    "    return bs_images, stim_images\n",
    "\n",
    "\n",
    "def process_trialimages( Imagestack, plane_no, frame_ixs, bs_frame_range, stim_frame_range, lightleak_value, datapath=None):\n",
    "    \"\"\" Creates a for each trial a dfof responsemap and stores these in a 3d matrix [y_res,x_res,trials]\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Creating trial images for: {}\".format(datapath))\n",
    "\n",
    "    # Create a folder for saving the data\n",
    "    if datapath is not None:\n",
    "        savedatapath = os.path.join(datapath,\"hls\")\n",
    "        if not os.path.isdir(savedatapath):\n",
    "            os.mkdir(savedatapath)\n",
    "        bsimages_file = os.path.join(savedatapath, \"trialimages-bs-plane{}.npy\".format(plane_no))\n",
    "        stimimages_file = os.path.join(savedatapath, \"trialimages-stim-plane{}.npy\".format(plane_no))\n",
    "\n",
    "    # Get info on dimensions etc\n",
    "    x_res, y_res = Imagestack.resolution\n",
    "    n_trials = len(frame_ixs)\n",
    "\n",
    "    # Prepare data containers\n",
    "    bs_images = np.zeros( (x_res, y_res, n_trials) )\n",
    "    stim_images = np.zeros( (x_res, y_res, n_trials) )\n",
    "\n",
    "    # Suppress needles output and set plane\n",
    "    old_verbose = Imagestack.verbose\n",
    "    Imagestack.verbose = False\n",
    "    Imagestack.plane = plane_no\n",
    "\n",
    "    # Get the trial-wise images\n",
    "    with tqdm(total=n_trials, desc=\"Reading\", unit=\"trial\") as bar:\n",
    "        for trial,frame in enumerate(frame_ixs):\n",
    "\n",
    "            # Load baseline images, take mean and add to data container\n",
    "            I = Imagestack[ (frame+bs_frame_range[0]):(frame+bs_frame_range[1]) ]\n",
    "            bs_images[:,:,trial] = np.mean(I,axis=2)-lightleak_value\n",
    "\n",
    "            # Load stimulus images, take mean and add to data container\n",
    "            I = Imagestack[ (frame+stim_frame_range[0]):(frame+stim_frame_range[1]) ]\n",
    "            stim_images[:,:,trial] = np.mean(I,axis=2)-lightleak_value\n",
    "\n",
    "            bar.update(1)\n",
    "\n",
    "    # Convert images to 8 bit with global max at 255 and global min at 0\n",
    "    min_all = np.min([np.min(bs_images),np.min(stim_images)])\n",
    "    max_all = np.max([np.max(bs_images),np.max(stim_images)])\n",
    "    bs_images = (((bs_images-min_all) / (max_all-min_all)) * 255).astype(np.uint8)\n",
    "    stim_images = (((stim_images-min_all) / (max_all-min_all)) * 255).astype(np.uint8)\n",
    "\n",
    "    # Save data if datapath was supplied\n",
    "    if datapath is not None:\n",
    "        print(\"Saving baseline images to: {}\".format(bsimages_file))\n",
    "        np.save(bsimages_file, bs_images)\n",
    "        print(\"Saving stimulus images to: {}\".format(stimimages_file))\n",
    "        np.save(stimimages_file, stim_images)\n",
    "\n",
    "    # Set verbose flag back to old value\n",
    "    Imagestack.verbose = old_verbose\n",
    "\n",
    "    # Return data\n",
    "    return bs_images, stim_images\n",
    "\n",
    "\n",
    "def process_trialimages_2(stack, bs_frame_idxs, stim_frame_idxs, lightleak_value=0, datapath=None):\n",
    "    \"\"\" Creates for each trial a dfof responsemap and stores these in a 3d matrix [y_res,x_res,trials]\n",
    "    \"\"\"\n",
    "\n",
    "    # Get info on dimensions etc\n",
    "    x_res, y_res = stack.shape[1:]\n",
    "    n_trials = len(stim_frame_idxs)\n",
    "\n",
    "    # Prepare data containers\n",
    "    bs_images = np.zeros( (x_res, y_res, n_trials) )\n",
    "    stim_images = np.zeros( (x_res, y_res, n_trials) )\n",
    "\n",
    "    # Get the trial-wise images\n",
    "    with tqdm(total=n_trials, desc=\"Reading\", unit=\"trial\") as bar:\n",
    "        for trial, (bs_frames, stim_frames) in enumerate(zip(bs_frame_idxs, stim_frame_idxs)):\n",
    "\n",
    "            # Load baseline images, take mean and add to data container\n",
    "            I = stack[bs_frames, :, :]\n",
    "            bs_images[:,:,trial] = np.mean(I,axis=0) - lightleak_value\n",
    "\n",
    "            # Load stimulus images, take mean and add to data container\n",
    "            I = stack[stim_frames, :, :]\n",
    "            stim_images[:,:,trial] = np.mean(I,axis=0) - lightleak_value\n",
    "\n",
    "            bar.update(1)\n",
    "\n",
    "    # Convert images to 8 bit with global max at 255 and global min at 0\n",
    "    min_all = np.min([np.min(bs_images),np.min(stim_images)])\n",
    "    max_all = np.max([np.max(bs_images),np.max(stim_images)])\n",
    "    bs_images = (((bs_images-min_all) / (max_all-min_all)) * 255).astype(np.uint8)\n",
    "    stim_images = (((stim_images-min_all) / (max_all-min_all)) * 255).astype(np.uint8)\n",
    "\n",
    "    # Return data\n",
    "    return bs_images, stim_images\n",
    "\n",
    "\n",
    "def hlsmap( hlsname, stimuli, bs_images, stim_images, max_dfof=0, colormap=\"hsv\", datapath=None, aspect_ratio=1.0, scale_by_im=None, show_colorbar=True ):\n",
    "    \"\"\" Creates an HLS map as RGB array [yres,xres,rgb]\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Creating hlsmap for: {}\".format(hlsname))\n",
    "\n",
    "    # Create a folder for saving the data\n",
    "    if (datapath is not None) and (len(hlsname.strip()) > 0):\n",
    "        savedatapath = os.path.join(datapath,\"hls\")\n",
    "        if not os.path.isdir(savedatapath):\n",
    "            os.mkdir(savedatapath)\n",
    "    else:\n",
    "        savedatapath = \"\"\n",
    "\n",
    "    # Get info on dimensions etc\n",
    "    x_res, y_res, n_trials = bs_images.shape\n",
    "    unique_stimuli = np.unique(stimuli)\n",
    "    n_stimuli = len(unique_stimuli)\n",
    "\n",
    "    # Prepare colorspace\n",
    "    colors = np.zeros((n_stimuli,3))\n",
    "    if colormap.lower() == \"eye\":\n",
    "        colors[0,:] = [1.0, 0.0, 0.0]\n",
    "        colors[1,:] = [0.0, 0.0, 1.0]\n",
    "    elif colormap.lower() == \"eye-reverse\":\n",
    "        colors[0,:] = [0.0, 0.0, 1.0]\n",
    "        colors[1,:] = [1.0, 0.0, 0.0]\n",
    "    elif colormap.lower() == \"category\":\n",
    "        colors[0,:] = [0.0, 0.8, 1.0]\n",
    "        colors[1,:] = [1.0, 0.0, 0.4]\n",
    "    elif colormap.lower() == \"category-reverse\":\n",
    "        colors[0,:] = [1.0, 0.0, 0.4]\n",
    "        colors[1,:] = [0.0, 0.8, 1.0]\n",
    "    elif colormap.lower() == \"3colors\":\n",
    "        colors[0,:] = [1.0, 0.0, 0.0]\n",
    "        colors[1,:] = [0.0, 1.0, 0.0]\n",
    "        colors[2,:] = [0.0, 0.0, 1.0]\n",
    "    elif colormap.lower() == \"3colors-reverse\":\n",
    "        colors[2,:] = [1.0, 0.0, 0.0]\n",
    "        colors[1,:] = [0.0, 1.0, 0.0]\n",
    "        colors[0,:] = [0.0, 0.0, 1.0]\n",
    "    elif colormap.lower() == \"5colors\":\n",
    "        colors[0,:] = [1.0, 0.0, 0.0]\n",
    "        colors[1,:] = [1.0, 1.0, 0.0]\n",
    "        colors[2,:] = [0.0, 1.0, 0.0]\n",
    "        colors[3,:] = [0.0, 1.0, 1.0]\n",
    "        colors[4,:] = [0.0, 0.0, 1.0]\n",
    "    elif colormap.lower() == \"5colors-reverse\":\n",
    "        colors[4,:] = [1.0, 0.0, 0.0]\n",
    "        colors[3,:] = [1.0, 1.0, 0.0]\n",
    "        colors[2,:] = [0.0, 1.0, 0.0]\n",
    "        colors[1,:] = [0.0, 1.0, 1.0]\n",
    "        colors[0,:] = [0.0, 0.0, 1.0]\n",
    "    else:\n",
    "        cmap = matplotlib.cm.get_cmap(colormap)\n",
    "        if colormap == \"hsv\":\n",
    "            for s in range(n_stimuli):\n",
    "                colors[s,:] = cmap(float(s)/n_stimuli)[:3]\n",
    "        else:\n",
    "            for s in range(n_stimuli):\n",
    "                colors[s,:] = cmap(float(s)/(n_stimuli-1))[:3]\n",
    "\n",
    "    print(\"bs_images: {}  {}\".format(np.nanmin(bs_images),np.nanmax(bs_images)))\n",
    "    print(\"stim_images: {}  {}\".format(np.nanmin(stim_images),np.nanmax(stim_images)))\n",
    "\n",
    "    # Filter images (smoothing)\n",
    "    disk_kernel = skimage.morphology.disk(radius=1)\n",
    "    with tqdm(total=n_trials, desc=\"Filtering\", unit=\"trial\") as bar:\n",
    "        for t in range(n_trials):\n",
    "            bs_images[:,:,t] = skimage.filters.rank.median(bs_images[:,:,t], selem=disk_kernel)\n",
    "            stim_images[:,:,t] = skimage.filters.rank.median(stim_images[:,:,t], selem=disk_kernel)\n",
    "            bar.update(1)\n",
    "\n",
    "    print(\"bs_images: {}  {}\".format(np.nanmin(bs_images),np.nanmax(bs_images)))\n",
    "    print(\"stim_images: {}  {}\".format(np.nanmin(stim_images),np.nanmax(stim_images)))\n",
    "\n",
    "    # Prepare data containers\n",
    "    df_images = np.zeros( (x_res, y_res, n_stimuli) )\n",
    "\n",
    "    # Get one average baseline image and set minimum value to 1\n",
    "    bs_im = np.mean(bs_images, axis=2)\n",
    "    bs_im[bs_im<1] = 1.0\n",
    "    print(\"bs_im: {}  {}\".format(np.nanmin(bs_im), np.nanmax(bs_im)))\n",
    "\n",
    "    # Get one df/f image per stimulus\n",
    "    for stim_nr,stim_id in enumerate(unique_stimuli):\n",
    "\n",
    "        # Get indices for this stimulus id\n",
    "        stim_indices = stimuli==stim_id\n",
    "\n",
    "        # Mean, and df/f\n",
    "        df_images[:,:,stim_nr] = (np.nanmean(stim_images[:,:,stim_indices], axis=2) - bs_im) / bs_im\n",
    "\n",
    "    print(\"df_images: {}  {}\".format(np.nanmin(df_images), np.nanmax(df_images)))\n",
    "\n",
    "    # Change to correct aspect ratio\n",
    "    if aspect_ratio != 1.0:\n",
    "        print(\"Correcting to aspect ratio {}\".format(aspect_ratio))\n",
    "        new_x = int(x_res * aspect_ratio)\n",
    "        df_images_new = np.zeros((y_res,new_x,n_stimuli))\n",
    "        for s in range(n_stimuli):\n",
    "            df_images_new[:,:,s] = imresize(df_images[:,:,s], (y_res,new_x), order=0)\n",
    "        x_res = new_x\n",
    "        df_images = df_images_new\n",
    "\n",
    "    # Create H map (hue=stim preference)\n",
    "    H = np.argmax(df_images, axis=2)\n",
    "    # print(\"H: {}  {}\".format(np.nanmin(H),np.nanmax(H)))\n",
    "\n",
    "    # Create L map (lightness=response amplitude) and clip responses to range\n",
    "    if max_dfof == 0:\n",
    "        max_dfof = np.round(np.percentile(df_images.ravel(), 99) * 100)\n",
    "        if scale_by_im is not None:\n",
    "            max_dfof = np.round(max_dfof * 0.75)\n",
    "    L = np.nanmax(df_images, axis=2) * (100/float(max_dfof))\n",
    "    L[L<0] = 0.0\n",
    "    # print(\"L: {}  {}\".format(np.nanmin(L),np.nanmax(L)))\n",
    "\n",
    "    # Calculate resultant and circular variance for 'S' (saturation=selectivity)\n",
    "    phasor = np.zeros( (y_res, x_res, n_stimuli), dtype=np.complex_)\n",
    "    amp_sorted = np.sort(df_images, axis=2)\n",
    "    amp_sorted[amp_sorted<0.0001] = 0.0001\n",
    "    min_as = np.nanmin(amp_sorted)\n",
    "    max_as = np.nanmax(amp_sorted)\n",
    "    amp_sorted = (amp_sorted-min_as) / (max_as-min_as)\n",
    "    for s in range(n_stimuli):\n",
    "        phasor[:,:,s] = amp_sorted[:,:,s] * np.exp( 1j * (s/n_stimuli) * 2 * np.pi)\n",
    "    resultant = np.abs(np.sum(phasor,axis=2)) / np.sum(np.abs(phasor),axis=2)\n",
    "    circvar = 1-resultant\n",
    "\n",
    "    # Convert H map to RGB\n",
    "    H = colors[H]\n",
    "\n",
    "    # Stack L, circvar and resultant to 3d arrays\n",
    "    L = np.stack([L,L,L], axis=2)\n",
    "    resultant = np.stack([resultant,resultant,resultant], axis=2)\n",
    "    circvar = np.stack([circvar,circvar,circvar], axis=2)\n",
    "\n",
    "    # Add amplitude and saturarion by elementwise multiplications\n",
    "    HLS = (H * L * resultant) + L * circvar\n",
    "    # print(\"HLS {}  {}\".format(np.nanmin(HLS),np.nanmax(HLS)))\n",
    "    \n",
    "    # If scale_by_im is set, scale the brightness by the supplied image\n",
    "    if scale_by_im is not None:\n",
    "        scale_max = np.percentile(scale_by_im.ravel(),99)\n",
    "        scale_min = np.percentile(scale_by_im.ravel(),1)\n",
    "        scale_by_im = (scale_by_im - scale_min) / (scale_max - scale_min)\n",
    "\n",
    "        if aspect_ratio != 1.0:\n",
    "            scale_by_im = imresize(scale_by_im, (y_res,x_res), order=0)\n",
    "\n",
    "        scale_by_im[scale_by_im>1.0] = 1.0\n",
    "        scale_by_im[scale_by_im<0.0] = 0.0\n",
    "        scale_by_im = np.stack([scale_by_im,scale_by_im,scale_by_im], axis=2)\n",
    "        HLS = HLS * scale_by_im\n",
    "\n",
    "    HLSmax = np.nanmax(HLS,axis=2)\n",
    "    HLSmax[HLSmax<1.0] = 1.0\n",
    "    HLSmax = np.stack([HLSmax,HLSmax,HLSmax], axis=2)\n",
    "    HLS = HLS / HLSmax\n",
    "    # print(\"HLS after scaling {}  {}\".format(np.nanmin(HLS),np.nanmax(HLS)))\n",
    "\n",
    "    if show_colorbar:\n",
    "        color_bar = np.zeros((10,x_res)).astype(int)\n",
    "        x_offset = int(0.1 * x_res)\n",
    "        x_res_offsetted = int(x_res - (2*x_offset))\n",
    "        \n",
    "        # Create full bar\n",
    "        for stim_nr in range(n_stimuli):\n",
    "            x_start = int( (stim_nr/n_stimuli) * x_res_offsetted ) + x_offset\n",
    "            x_end = int( ((stim_nr+1)/n_stimuli) * x_res_offsetted ) + x_offset\n",
    "            color_bar[:,x_start:x_end] = stim_nr\n",
    "        color_bar = colors[color_bar]\n",
    "\n",
    "        # Now black out edges\n",
    "        color_bar[:,:x_offset,:] = 0\n",
    "        color_bar[:,-x_offset:,:] = 0\n",
    "        for stim_nr in range(n_stimuli):\n",
    "            x_start = int( ((stim_nr/n_stimuli) * x_res_offsetted) ) + x_offset\n",
    "            x_end   = int( ((stim_nr/n_stimuli) * x_res_offsetted) + (0.2*(x_res_offsetted/n_stimuli)) ) + x_offset\n",
    "            color_bar[:,x_start:x_end,:] = 0\n",
    "            x_start = int( ( ((stim_nr+1)/n_stimuli) * x_res_offsetted) - (0.2*(x_res_offsetted/n_stimuli)) ) + x_offset\n",
    "            x_end   = int( ( ((stim_nr+1)/n_stimuli) * x_res_offsetted) ) + x_offset\n",
    "            color_bar[:,x_start:x_end,:] = 0\n",
    "        HLS = np.concatenate([HLS,color_bar], axis=0)\n",
    "\n",
    "    if savedatapath:\n",
    "        hlsmap_name = \"hls-\" + hlsname + '-max{:02.0f}'.format(max_dfof)\n",
    "        if scale_by_im is not None:\n",
    "            hlsmap_name = hlsmap_name + \"-adjusted\"\n",
    "        hlsmap_file = os.path.join(savedatapath, hlsmap_name + '.png')\n",
    "        print(\"Saving hlsmap to file: {}\".format(hlsmap_file))\n",
    "        plt.imsave(hlsmap_file, HLS)\n",
    "\n",
    "    return HLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(processing_parameters)\n",
    "# get the search query\n",
    "search_string = processing_parameters.search_string\n",
    "parsed_search_string = fdh.parse_search_string(search_string)\n",
    "\n",
    "# get the paths from the database\n",
    "all_processed = bd.query_database('analyzed_data', search_string + ', analysis_type:preprocessing')\n",
    "preproc_paths = [el['analysis_path'] for el in all_processed if ('_preproc' in el['slug']) and (parsed_search_string['mouse'].lower() in el['slug'])]\n",
    "\n",
    "all_raw = bd.query_database('vr_experiment', search_string)\n",
    "ca_paths = [el['tif_path'] for el in all_raw if (parsed_search_string['mouse'].lower() in el['slug'])]\n",
    "\n",
    "# get the day, animal and rig\n",
    "day = '_'.join(all_processed[0]['slug'].split('_')[0:3])\n",
    "rig = all_processed[0]['rig']\n",
    "animal = all_processed[0]['slug'].split('_')[3:6]\n",
    "animal = '_'.join([animal[0].upper()] + animal[1:])\n",
    "\n",
    "# assemble the output path\n",
    "out_path = os.path.join(paths.analysis_path, '_'.join((day, animal, rig, 'tcday.hdf5')))\n",
    "\n",
    "# allocate memory for the data\n",
    "preproc_data = []\n",
    "ca_stacks = []\n",
    "\n",
    "# allocate memory for excluded trials\n",
    "excluded_trials = []\n",
    "\n",
    "# for all the files\n",
    "for preproc_file, ca_file in zip(preproc_paths, ca_paths):\n",
    "    # load the data\n",
    "    with pd.HDFStore(preproc_file, mode='r') as h:\n",
    "        if ('/matched_calcium' in h.keys()):\n",
    "            # concatenate the latents\n",
    "            dataframe = h['matched_calcium']\n",
    "            # store\n",
    "            preproc_data.append(dataframe)\n",
    "\n",
    "            # Load the ca stack as well\n",
    "            stack = imread(ca_file).astype(np.uint8)\n",
    "\n",
    "            # replace nans with 0\n",
    "            stack = np.nan_to_num(stack, 0)\n",
    "\n",
    "            # if it's 2d (i.e 1 frame), expand 1 dimension\n",
    "            if len(stack.shape) == 2:\n",
    "                stack = np.expand_dims(stack, 2)\n",
    "                stack = np.transpose(stack, [2, 0, 1])\n",
    "\n",
    "            ca_stacks.append(stack)\n",
    "\n",
    "        else:\n",
    "            excluded_trials.append(preproc_file)\n",
    "\n",
    "print(f'Number of files loaded: {len(preproc_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_angles(ds):\n",
    "    # explicitly calculate direction and orienation in the dataset\n",
    "    if 'direction_wrapped' not in ds.columns:\n",
    "        ds['direction_wrapped'] = ds['direction'].copy()\n",
    "        mask = ds['direction_wrapped'] > -1000\n",
    "        ds.loc[mask, 'direction_wrapped'] = ds.loc[mask, 'direction_wrapped'].apply(wrap, bound=360.1)\n",
    "\n",
    "    if 'orientation' not in ds.columns:\n",
    "        ds['orientation'] = ds['direction_wrapped'].copy()\n",
    "        mask = ds['orientation'] > -1000\n",
    "        ds.loc[mask, 'orientation'] = ds.loc[mask, 'orientation'].apply(wrap, bound=180.1)\n",
    "    return ds\n",
    "\n",
    "def parse_trial_frames(df, prestim=0, poststim=0, get_baseline=False):\n",
    "    fr = processing_parameters.wf_frame_rate\n",
    "\n",
    "    if get_baseline:\n",
    "        trial_idx_frames = df[df.trial_num > 0].groupby(['trial_num']).apply(lambda x: [x.index[0]-int(fr*prestim), x.index[1]])\n",
    "    else:\n",
    "        trial_idx_frames = df[df.trial_num > 0].groupby(['trial_num']).apply(lambda x: [x.index[0]-int(fr*prestim), x.index[-1]+int(fr*poststim)])\n",
    "        \n",
    "    trial_idx_frames = np.array(trial_idx_frames.to_list())\n",
    "\n",
    "    if trial_idx_frames[-1, 1] > df.index[-1]:\n",
    "        trial_idx_frames[-1, 1] = df.index[-1]\n",
    "    if trial_idx_frames[0, 0] < 0:\n",
    "        trial_idx_frames[0, 0] = 0\n",
    "\n",
    "    traces = []\n",
    "    for i, frame in enumerate(trial_idx_frames):\n",
    "        df_slice = df.iloc[frame[0]:frame[1], :].copy()\n",
    "        df_slice['trial_num'] = i + 1\n",
    "        df_slice['direction'] = df_slice['direction'].max()\n",
    "        df_slice['direction_wrapped'] = df_slice['direction_wrapped'].max()\n",
    "        df_slice['orientation'] = df_slice['orientation'].max()\n",
    "        traces.append(df_slice)\n",
    "    \n",
    "    traces = pd.concat(traces, axis=0).reset_index(drop=False)\n",
    "    return traces\n",
    "\n",
    "def drop_partial_or_long_trials(df, min_trial_length=4.5, max_trial_length=5.5):\n",
    "    trial_lengths = df[df.trial_num > 0].groupby('trial_num').apply(lambda x: x.shape[0] / processing_parameters.wf_frame_rate)\n",
    "\n",
    "    # Drop trials that are shorter than min_trial_length (partial trials)\n",
    "    short_trials = trial_lengths[trial_lengths < min_trial_length].index\n",
    "    df = df.drop(df[df.trial_num.isin(short_trials)].index)\n",
    "\n",
    "    # Drop trials that are longer than max_trial_length (errors in trial number indexing)\n",
    "    long_trials = trial_lengths[trial_lengths > max_trial_length].index\n",
    "    df = df.drop(df[df.trial_num.isin(long_trials)].index)\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_stim_and_basline_idxs(ds, variable):\n",
    "    # Do a groupby to get the trial frames\n",
    "    stim_df = parse_trial_frames(ds)\n",
    "    stim_frames = stim_df[stim_df.trial_num > 0].groupby([variable, 'trial_num']).index.agg(list).droplevel(-1).to_list()\n",
    "    stims = stim_df[stim_df.trial_num > 0].groupby([variable, 'trial_num']).index.agg(list).droplevel(-1).index.to_numpy()\n",
    "\n",
    "    # Do another groupby to get the trial baseline frames\n",
    "    bs_df = parse_trial_frames(ds, prestim=2, get_baseline=True)\n",
    "    bs_frames = bs_df[bs_df.trial_num > 0].groupby([variable, 'trial_num']).index.agg(list).droplevel(-1).to_list()\n",
    "\n",
    "    return stim_frames, bs_frames, stims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and trim the tif stack\n",
    "ds = preproc_data[0]\n",
    "stack = ca_stacks[0]\n",
    "processed_frames = ds['ca_tif_frames'].to_numpy()\n",
    "stack = stack[processed_frames, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster = LocalCluster(\n",
    "#     n_workers=4,\n",
    "#     memory_limit=\"5GB\",\n",
    "#     resources={\"MEM\": 1},\n",
    "#     threads_per_worker=2,\n",
    "#     dashboard_address=\":8787\",\n",
    "# )\n",
    "# client = Client(cluster)\n",
    "\n",
    "# # Use minian to do motion correction on the stack\n",
    "# varr = xr.DataArray(stack, dims=('frame', 'height', 'width'), name='varr')\n",
    "# varr = varr.assign_coords(frame=np.arange(varr.shape[0]), height=np.arange(varr.shape[1]), width=np.arange(varr.shape[2]))\n",
    "# chk, _ = get_optimal_chk(varr, dtype=float)\n",
    "# varr = optimize_chunk(varr, chk)\n",
    "\n",
    "# # remove glow\n",
    "# varr_ref = varr.sel(None) \n",
    "# varr_ref.rename(\"varr_ref\")\n",
    "# varr_min = varr_ref.chunk({\"frame\": -1, \"height\": -1, \"width\": -1}).quantile(0.0025, dim=\"frame\", skipna=True).compute()\n",
    "# varr_min = varr_min - np.min(varr_min)\n",
    "# varr_ref = varr_ref - varr_min\n",
    "# varr_ref = varr_ref.where(varr_ref > 0, 0).astype(np.uint8)\n",
    "# varr_ref = rechunk_like(varr_ref, varr)\n",
    "\n",
    "# param_denoise = {\"method\": \"median\", \"ksize\": 5}\n",
    "# param_background_removal = {\"method\": \"tophat\", \"wnd\": 15}\n",
    "# param_estimate_motion = {\"dim\": \"frame\", \"npart\": 5, \"aggregation\": \"mean\"}\n",
    "\n",
    "# # denoise\n",
    "# varr_ref = denoise(varr_ref, **param_denoise)\n",
    "\n",
    "# # remove background\n",
    "# varr_ref = remove_background(varr_ref, **param_background_removal)\n",
    "\n",
    "# # estimate motion\n",
    "# motion = estimate_motion(varr_ref, **param_estimate_motion)\n",
    "# Y = apply_transform(varr_ref, motion, fill=0)\n",
    "\n",
    "# aligned_stack = Y.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a groupby to get the baseline frames\n",
    "var = 'orientation'\n",
    "ds = calculate_angles(ds)\n",
    "ds = drop_partial_or_long_trials(ds)\n",
    "stim_frames, bs_frames, stims = get_stim_and_basline_idxs(ds, var)\n",
    "baseline_idx_frames = ds[ds.trial_num == 0].ca_tif_frames.to_numpy()\n",
    "mean_im = np.mean(stack, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_images, stim_images = process_trialimages_2(stack, bs_frames, stim_frames, lightleak_value=0, datapath=None)\n",
    "hls = hlsmap(var, stims, bs_images, stim_images, max_dfof=0, colormap=\"hsv\", datapath=None, aspect_ratio=1.0, scale_by_im=mean_im, show_colorbar=True)\n",
    "plt.imshow(hls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prey_capture_old",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
