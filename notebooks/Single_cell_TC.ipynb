{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9207741c",
   "metadata": {},
   "source": [
    "# imports\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath(r'C:\\Users\\mmccann\\repos\\bonhoeffer\\prey_capture'))\n",
    "\n",
    "\n",
    "import panel as pn\n",
    "import holoviews as hv\n",
    "from holoviews import opts, dim\n",
    "from holoviews.operation import histogram\n",
    "hv.extension('bokeh')\n",
    "from bokeh.resources import INLINE\n",
    "\n",
    "import paths\n",
    "import functions_bondjango as bd\n",
    "import functions_misc as fm\n",
    "import functions_plotting as fp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.mixture as mix\n",
    "import sklearn.decomposition as decomp\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_predict, train_test_split\n",
    "from sklearn import svm, datasets\n",
    "from sklearn import preprocessing\n",
    "import sklearn.linear_model as lin\n",
    "import sklearn.metrics as smet\n",
    "import scipy.signal as ss\n",
    "import scipy.stats as stat\n",
    "import scipy.optimize as opt\n",
    "from sklearn.neighbors import KernelDensity as kds\n",
    "\n",
    "import random\n",
    "# import functions_data_handling as fd\n",
    "# import functions_vame as fv\n",
    "import importlib\n",
    "import processing_parameters\n",
    "# import PSID\n",
    "# from PSID.evaluation import evalPrediction\n",
    "# import sklearn.cross_decomposition as cros\n",
    "# import umap\n",
    "np.seterr(divide='ignore', invalid='ignore')\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f77ce8",
   "metadata": {},
   "source": [
    "def maxmin(array_in):\n",
    "    return (array_in-np.nanmin(array_in))/(np.nanmax(array_in)-np.nanmin(array_in))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55dfd35",
   "metadata": {},
   "source": [
    "%%time\n",
    "# Load the desired files\n",
    "importlib.reload(processing_parameters)\n",
    "\n",
    "# define the threshold for matched cells\n",
    "match_threshold = 10\n",
    "# get the data paths\n",
    "try: \n",
    "    data_path = snakemake.input[0]\n",
    "except NameError:\n",
    "    # get the search list\n",
    "    search_list = processing_parameters.search_string\n",
    "    # allocate memory for the data\n",
    "    pre_data = []\n",
    "    \n",
    "    # allocate a list for all paths (need to preload to get the dates)\n",
    "    all_paths = []\n",
    "    # for all the search strings\n",
    "    for search_string in search_list:\n",
    "\n",
    "        # query the database for data to plot\n",
    "        data_all = bd.query_database('analyzed_data', search_string)\n",
    "        data_all = [el for el in data_all if 'preproc' in el['slug']]\n",
    "        data_path = [el['analysis_path'] for el in data_all if '_preproc' in el['slug']]\n",
    "        all_paths.append(data_path)\n",
    "    # get the dates present\n",
    "    data_dates = np.unique([os.path.basename(el)[:10] for el in np.concatenate(all_paths)])\n",
    "    print(f'Dates present: {data_dates}')\n",
    "    # now load the files\n",
    "    for data_path in all_paths:\n",
    "        # load the calcium data\n",
    "        beh_data = []\n",
    "        # for all the files\n",
    "        for files in data_path:\n",
    "            # load the data\n",
    "            with pd.HDFStore(files) as h:\n",
    "                beh_data.append(h['full_traces'])\n",
    "                if '/matched_calcium' in h.keys():\n",
    "                    # get the cell matches\n",
    "                    cell_matches = h['cell_matches']\n",
    "                    \n",
    "                    # perform only if there are more files\n",
    "                    if len(data_dates) > 1:\n",
    "#                         print('Successful match')\n",
    "                        match_dates = [el for el in data_dates if el in cell_matches.columns]\n",
    "                        # get only the days present in the search\n",
    "                        cell_matches = cell_matches[match_dates]\n",
    "                        # generate a list with the number of days and the number of cells kept\n",
    "\n",
    "                        # get the unique cell combinations\n",
    "                        # unique contains the unique patterns followed by cells across days\n",
    "                        # inverse indicates which pattern is followed by each cell\n",
    "                        # count contains the number of times each pattern is found\n",
    "                        unique, inverse, counts = np.unique(~np.isnan(cell_matches.to_numpy()), axis=0, \n",
    "                                        return_counts=True, return_inverse=True)\n",
    "                        # remove the single day and no day cases\n",
    "                        counts[np.sum(unique, axis=1)==0] = 0\n",
    "                        counts[np.sum(unique, axis=1)==1] = 0\n",
    "\n",
    "                        # get an index vector with only the most popular pattern\n",
    "                        # (regardless of how many cells share it)\n",
    "                        cell_idx = np.array(inverse==np.argmax(counts))\n",
    "\n",
    "                        cell_matches = cell_matches.iloc[cell_idx, :]\n",
    "                    else:\n",
    "                        counts = 1\n",
    "                        cell_idx = cell_matches[data_dates].to_numpy()\n",
    "                        cell_idx = ~np.isnan(cell_idx)\n",
    "                        cell_matches = cell_matches.iloc[cell_idx, :]\n",
    "                        unique = np.array([[1]])\n",
    "                    # concatenate the latents\n",
    "                    dataframe = pd.concat([h['matched_calcium'], h['latents']], axis=1)\n",
    "                    # store\n",
    "                    pre_data.append((files, dataframe,  cell_matches))\n",
    "                    \n",
    "print(f'Number of matched cells: {np.sum(cell_idx)}')\n",
    "print(f'Number of matched trials: {unique[np.argmax(counts)].sum()}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9865bdc8",
   "metadata": {},
   "source": [
    "# Leave only common cells across all datasets\n",
    "\n",
    "# allocate memory for the cleaned up data\n",
    "data = []\n",
    "trial_dates = []\n",
    "\n",
    "# print(pre_normal_data[0][2])\n",
    "# for all the normal trials\n",
    "for idx, el in enumerate(pre_data):\n",
    "    # get the date\n",
    "    current_date = os.path.basename(el[0])[:10]\n",
    "    # get the corresponding indexes\n",
    "    current_idx = el[2][current_date].to_numpy()\n",
    "    # if they're all nans, skip the day\n",
    "    if np.isnan(np.sum(current_idx)):\n",
    "        continue\n",
    "    # store the date\n",
    "    trial_dates.append(current_date)\n",
    "    # get the current df\n",
    "    current_df = el[1]\n",
    "    labels = list(current_df.columns)\n",
    "    cells = [el for el in labels if 'cell' in el]\n",
    "    not_cells = [el for el in labels if 'cell' not in el]\n",
    "    # get the non-cell data\n",
    "    non_cell_data = current_df[not_cells]\n",
    "    # get the current calcium data\n",
    "    cell_data = current_df[cells]\n",
    "    # remove the non matched cells\n",
    "#     cell_data = cell_data.iloc[:, current_idx]\n",
    "    # rename the cell fields\n",
    "    cell_names = ['cell_' + str(el) for el in np.arange(cell_data.shape[1])]\n",
    "    cell_data.columns = cell_names\n",
    "    # normalize the single trial activity\n",
    "#     cell_data = (cell_data-cell_data.mean())/cell_data.std()\n",
    "#     cell_data = cell_data/cell_data.std()\n",
    "#     cell_data = (cell_data-cell_data.min())/(cell_data.max()-cell_data.min())\n",
    "\n",
    "    # calculate a baseline for all cells\n",
    "    for name, single in cell_data.items():\n",
    "        # skip if there are only zeros\n",
    "        if np.sum(single) == 0:\n",
    "            continue\n",
    "        # get the baseline\n",
    "        baseline = np.percentile(single[single>0], 8)\n",
    "        # get the dF/F\n",
    "#         single = (single-baseline)/baseline\n",
    "        # clip the trace\n",
    "        single[single<baseline] = 0\n",
    "        # store\n",
    "        cell_data[name] = single\n",
    "\n",
    "    # remove the nans after normalization\n",
    "    cell_data[np.isnan(cell_data)] = 0\n",
    "    # assemble a new data frame with only the matched cells and the rest of the data\n",
    "    data.append(pd.concat((non_cell_data, cell_data), axis=1))\n",
    "    \n",
    "print(data[0].shape)\n",
    "print(data[0].columns)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf17566f",
   "metadata": {},
   "source": [
    "# set up the feature and calcium matrices\n",
    "# list the radial features in the dataset\n",
    "radial_features = ['cricket_0_delta_heading', 'cricket_0_visual_angle', 'mouse_heading', \n",
    "                   'cricket_0_delta_head', 'cricket_0_heading', 'head_direction']\n",
    "# define the design matrix\n",
    "feature_list = ['mouse_speed', 'cricket_0_speed', 'mouse_x', 'mouse_y', 'cricket_0_x', 'cricket_0_y',\n",
    "                'cricket_0_delta_heading', 'cricket_0_mouse_distance', 'cricket_0_visual_angle',\n",
    "               'mouse_heading', 'cricket_0_delta_head', 'cricket_0_heading', 'head_direction',\n",
    "               'latent_0', 'latent_1', 'latent_2', 'latent_3', 'latent_4',\n",
    "               'latent_5', 'latent_6', 'latent_7', 'latent_8', 'latent_9']\n",
    "# feature_list = ['mouse_speed']\n",
    "\n",
    "# define the frame rate (fps)\n",
    "frame_rate = 10\n",
    "# define the width of the kernel (s), multiplied to convert to frames\n",
    "sigma = 1*frame_rate\n",
    "# calculate the kernel\n",
    "kernel = ss.gaussian(sigma*5, sigma)\n",
    "# define the number of basis functions per regressor\n",
    "basis_number = 9\n",
    "# define the kernel spacing (in s)\n",
    "kernel_spacing = 0.2*frame_rate\n",
    "# get the total length of the kernel\n",
    "total_length = kernel_spacing*(basis_number-1) + kernel.shape[0]\n",
    "# # get the start positions of the basis functions (assume sigma defines the interval)\n",
    "# basis_starts = [int(el) for el in np.arange(-sigma*((basis_number-1)/2), \n",
    "#                                        sigma*((basis_number-1)/2)+1, sigma)]\n",
    "# allocate memory for the output\n",
    "feature_trials = []\n",
    "# allocate memory for a data frame without the encoding model features\n",
    "feature_raw_trials = []\n",
    "# allocate memory for the calcium\n",
    "calcium_trials = []\n",
    "# get the number of trials\n",
    "trial_number = len(data)\n",
    "# get the features\n",
    "for idx, el in enumerate(data):\n",
    "    # get the intersection of the labels\n",
    "    label_intersect = [feat for feat in feature_list if feat in el.columns]\n",
    "    \n",
    "    if len(label_intersect) != len(feature_list):\n",
    "        continue\n",
    "    # get the features of interest\n",
    "    target_features = el.loc[:, feature_list]\n",
    "    # save the original features for simpler calculations\n",
    "#     feature_raw_trials.append(target_features.copy())\n",
    "    # get the original columns\n",
    "    original_columns = target_features.columns\n",
    "    \n",
    "    # turn the radial variables into linear ones\n",
    "    # for all the columns\n",
    "    for label in original_columns:\n",
    "        # calculate head speed\n",
    "        if label == 'head_direction':\n",
    "            # get the head direction\n",
    "            head = target_features[label].copy().to_numpy()\n",
    "            # get the angular speed and acceleration of the head\n",
    "            speed = np.concatenate(([0], np.diff(ss.medfilt(head, 21))), axis=0)\n",
    "            acceleration = np.concatenate(([0], np.diff(head)), axis=0)\n",
    "            # add to the features\n",
    "            target_features['head_speed'] = speed\n",
    "            target_features['head_acceleration'] = acceleration\n",
    "#         # check if the feature is radial\n",
    "#         if label in radial_features:\n",
    "#             # get the feature\n",
    "#             rad_feature = target_features[label].copy().to_numpy()\n",
    "#             # convert to radians\n",
    "#             rad_feature = np.deg2rad(rad_feature)\n",
    "#             # perform angular decomposition (assume unit circle)\n",
    "#             x = np.cos(rad_feature)\n",
    "#             y = np.sin(rad_feature)\n",
    "#             # replace the original column by the extracted ones\n",
    "#             target_features[label+'_x'] = x\n",
    "#             target_features[label+'_y'] = y\n",
    "#             # drop the original column\n",
    "#             target_features.drop(labels=label, axis=1, inplace=True)\n",
    "        # check if the label is a speed and calculate acceleration\n",
    "        if 'speed' in label:\n",
    "            # get the speed\n",
    "            speed = target_features[label].copy().to_numpy()\n",
    "            # calculate the acceleration with the smoothed speed\n",
    "            acceleration = np.concatenate(([0], np.diff(ss.medfilt(speed, 21))), axis=0)\n",
    "            # add to the features\n",
    "            target_features[label.replace('speed', 'acceleration')] = acceleration\n",
    "        # smooth the feature\n",
    "        target_features[label] = ss.medfilt(target_features[label], 21)\n",
    "\n",
    "    # store the features\n",
    "    feature_raw_trials.append(target_features)\n",
    "    \n",
    "    # get the calcium data\n",
    "    cells = [cell for cell in el.columns if 'cell' in cell]\n",
    "    cells = el.loc[:, cells].to_numpy()\n",
    "\n",
    "    # store\n",
    "    calcium_trials.append(cells)\n",
    "    \n",
    "\n",
    "print(f'Time by features: {feature_raw_trials[0].shape}')\n",
    "print(f'Time by ROIs: {calcium_trials[0].shape}')\n",
    "print(feature_raw_trials[0].columns)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d159b36",
   "metadata": {},
   "source": [
    "# combine trials for single days\n",
    "\n",
    "# get the unique dates\n",
    "unique_dates = np.unique(trial_dates)\n",
    "print(unique_dates)\n",
    "# allocate memory for the output\n",
    "merged_features = []\n",
    "merged_calcium = []\n",
    "# for all the unique dates\n",
    "for dates in unique_dates:\n",
    "    # accumulate the trials for that date\n",
    "    day_features = [el for idx, el in enumerate(feature_raw_trials) if dates in trial_dates[idx]]\n",
    "    day_calcium = [el for idx, el in enumerate(calcium_trials) if dates in trial_dates[idx]]\n",
    "\n",
    "    # concatenate and store\n",
    "    merged_features.append(pd.concat(day_features))\n",
    "    merged_calcium.append(np.vstack(day_calcium))\n",
    "# replace the original list\n",
    "feature_raw_trials = merged_features\n",
    "calcium_trials = merged_calcium\n",
    "    "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79723307",
   "metadata": {},
   "source": [
    "%%time\n",
    "# Calculate the half and full tuning of each cell \n",
    "\n",
    "# define the number of calcium shuffles\n",
    "shuffle_number = 100\n",
    "# define the confidence interval cutoff\n",
    "percentile = 95\n",
    "# define the number of bins for the TCs\n",
    "bins = 10\n",
    "# allocate memory for the output\n",
    "tc_all = []\n",
    "tc_full = []\n",
    "tc_resp = []\n",
    "# get the number of features\n",
    "feature_number = feature_raw_trials[0].shape[1]\n",
    "# for all the trials\n",
    "for idx, trial in enumerate(feature_raw_trials):\n",
    "    # get the calcium data\n",
    "    current_calcium = calcium_trials[idx]\n",
    "    # get the number of cells\n",
    "    cell_number = current_calcium.shape[1]\n",
    "    # allocate memory for the trial TCs\n",
    "    tc_trial = {}\n",
    "    tc_trial_full = {}\n",
    "    tc_trial_resp = {}\n",
    "    # for all the features\n",
    "    for feature in np.arange(feature_number):\n",
    "        # get the current feature\n",
    "        current_feature = trial.to_numpy()[:, feature]\n",
    "        # get the name\n",
    "        feature_name = trial.columns[feature]\n",
    "        # allocate a list for the 2 halves\n",
    "        tc_half = []\n",
    "        # exclude nan values\n",
    "        keep_vector_full = ~np.isnan(current_feature)\n",
    "        counts_feature = current_feature[keep_vector_full]\n",
    "        # get the counts\n",
    "        feature_counts = stat.binned_statistic(counts_feature, counts_feature, \n",
    "                                               statistic='count', bins=bins)[0]\n",
    "        # zero the positions with less than 3 counts\n",
    "        feature_counts[feature_counts<3] = 0\n",
    "        # for first and second half\n",
    "        for half in np.arange(2):\n",
    "            # get the half vector\n",
    "#             print(current_feature.shape)\n",
    "            half_bound = int(np.floor(current_feature.shape[0]/2))\n",
    "            half_vector = np.arange(half_bound) + half_bound*half\n",
    "            half_feature = current_feature[half_vector]\n",
    "            # exclude nan values\n",
    "            keep_vector = ~np.isnan(half_feature)\n",
    "            keep_feature = half_feature[keep_vector]\n",
    "\n",
    "            # allocate a list for the cells\n",
    "            tc_cell = []\n",
    "            # for all the cells\n",
    "            for cell in np.arange(cell_number):\n",
    "                # get the current cell\n",
    "                half_cell = current_calcium[half_vector, cell]\n",
    "                keep_cell = half_cell[keep_vector]\n",
    "\n",
    "                # calculate the TC\n",
    "                current_tc = stat.binned_statistic(keep_feature, keep_cell, \n",
    "                                                   statistic='sum', bins=bins)[0]\n",
    "                # normalize the TC\n",
    "                norm_tc = current_tc/feature_counts\n",
    "                # remove nans\n",
    "                norm_tc[np.isnan(norm_tc)] = 0\n",
    "                # store\n",
    "                tc_cell.append(norm_tc)\n",
    "            # store the cells\n",
    "            tc_half.append(tc_cell)\n",
    "        # allocate memory for the full tc per cell\n",
    "        tc_cell_full = []\n",
    "        tc_cell_resp = np.zeros((cell_number, 2))\n",
    "        # calculate the full TC\n",
    "        for cell in np.arange(cell_number):\n",
    "            keep_cell = current_calcium[keep_vector_full, cell]\n",
    "            tc_cell = stat.binned_statistic(counts_feature, keep_cell, statistic='sum')[0]\n",
    "            tc_cell = tc_cell/feature_counts\n",
    "            tc_cell[np.isnan(tc_cell)] = 0\n",
    "            tc_cell[np.isinf(tc_cell)] = 0\n",
    "            # allocate memory for the shuffles\n",
    "            shuffle_array = np.zeros((shuffle_number, bins))\n",
    "            # generate the shuffles\n",
    "            for shuffle in np.arange(shuffle_number):\n",
    "                # randomize the calcium activity\n",
    "                random_cell = keep_cell.copy()\n",
    "                np.random.shuffle(random_cell)\n",
    "                tc_random = stat.binned_statistic(counts_feature, random_cell, statistic='sum')[0]\n",
    "                tc_random = tc_random/feature_counts\n",
    "                tc_random[np.isnan(tc_random)] = 0\n",
    "                tc_random[np.isinf(tc_random)] = 0\n",
    "                shuffle_array[shuffle, :] = tc_random\n",
    "            # get the threshold\n",
    "            resp_threshold = np.percentile(np.abs(shuffle_array.flatten()), percentile)\n",
    "            # threshold the TC\n",
    "#             tc_cell[np.abs(tc_cell)<resp_threshold] = 0\n",
    "            # fill up the responsivity matrix\n",
    "            tc_cell_resp[cell, 0] = np.abs(np.max(np.abs(tc_cell)) - resp_threshold)/resp_threshold\n",
    "            tc_cell_resp[cell, 1] = np.sum(np.abs(tc_cell)>resp_threshold) > 0\n",
    "            # store\n",
    "            tc_cell_full.append(tc_cell)\n",
    "#             tc_cell_resp.append(resp_threshold)\n",
    "        # store the halfs and fulls\n",
    "        tc_trial[feature_name] = tc_half\n",
    "        tc_trial_full[feature_name] = tc_cell_full\n",
    "        tc_trial_resp[feature_name] = tc_cell_resp\n",
    "    # store the trials\n",
    "    tc_all.append(tc_trial)\n",
    "    tc_full.append(tc_trial_full)\n",
    "    tc_resp.append(tc_trial_resp)\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3d09c0",
   "metadata": {},
   "source": [
    "%%time\n",
    "# calculate consistency\n",
    "\n",
    "# define the number of shuffles\n",
    "shuffle_number = 100\n",
    "# define the percentile\n",
    "percentile = 95\n",
    "# allocate memory for the output\n",
    "tc_cons = []\n",
    "# get the number of features\n",
    "feature_number = feature_raw_trials[0].shape[1]\n",
    "# for all the trials\n",
    "for idx, trial in enumerate(feature_raw_trials):\n",
    "#     # get the calcium data\n",
    "#     current_calcium = calcium_trials[idx]\n",
    "    # get the number of cells\n",
    "    cell_number = calcium_trials[idx].shape[1]\n",
    "    # allocate memory for the trial TCs\n",
    "    tc_trial = {}\n",
    "    # for all the features\n",
    "    for feature in np.arange(feature_number):\n",
    "\n",
    "        # get the name\n",
    "        feature_name = trial.columns[feature]\n",
    "        # allocate an array for the correlations and tests\n",
    "        tc_half = np.zeros([cell_number, 2])\n",
    "        # get the two halves\n",
    "        halves = tc_all[idx][feature_name]\n",
    "        \n",
    "        # calculate the real and shuffle correlation\n",
    "        for cell in np.arange(cell_number):\n",
    "            # get the current cell first and second half\n",
    "            current_first = halves[0][cell]\n",
    "            current_second = halves[1][cell]\n",
    "            # real correlation\n",
    "            real_correlation = np.corrcoef(current_first, current_second)[1][0]\n",
    "            \n",
    "            # shuffle array\n",
    "            shuffle_array = np.zeros([shuffle_number, 1])\n",
    "            # calculate the confidence interval\n",
    "            for shuffle in np.arange(shuffle_number):\n",
    "                random_second = halves[1][random.randrange(cell_number)]\n",
    "                shuffle_array[shuffle] = np.corrcoef(current_first, random_second)[1][0]\n",
    "            # turn nans into 0\n",
    "            shuffle_array[np.isnan(shuffle_array)] = 0\n",
    "            # get the confidence interval\n",
    "            conf_interval = np.percentile(shuffle_array, percentile)\n",
    "            # store the correlation and whether it passes the criterion\n",
    "            tc_half[cell, 0] = real_correlation\n",
    "            tc_half[cell, 1] = real_correlation > conf_interval\n",
    "#             freq, edges = np.histogram(shuffle_array.flatten())\n",
    "#             hist = hv.Histogram((edges, freq)).opts(title=str(real_correlation))\n",
    "#             raise ValueError\n",
    "            \n",
    "        # store for the variable\n",
    "        tc_trial[feature_name] = tc_half\n",
    "    # store for the trial\n",
    "    tc_cons.append(tc_trial)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa14aa7",
   "metadata": {},
   "source": [
    "# %%time\n",
    "# # calculate responsivity\n",
    "# # define the number of shuffles\n",
    "# shuffle_number = 1000\n",
    "# # define the percentile\n",
    "# percentile = 80\n",
    "# # allocate memory for the output\n",
    "# tc_resp = []\n",
    "# # get the number of features\n",
    "# feature_number = feature_raw_trials[0].shape[1]\n",
    "# # for all the trials\n",
    "# for idx, trial in enumerate(feature_raw_trials):\n",
    "\n",
    "#     # get the number of cells\n",
    "#     cell_number = calcium_trials[idx].shape[1]\n",
    "#     # allocate memory for the trial TCs\n",
    "#     tc_trial = {}\n",
    "#     # for all the features\n",
    "#     for feature in np.arange(feature_number):\n",
    "\n",
    "#         # get the name\n",
    "#         feature_name = trial.columns[feature]\n",
    "#         # allocate an array for the correlations and tests\n",
    "#         tc_resp_cell = np.zeros([cell_number, 2])\n",
    "#         # get the current TCs\n",
    "#         trial_tcs = np.array(tc_full[idx][feature_name])\n",
    "#         # build a distribution with the data for the whole trial\n",
    "#         null_estimator = kds().fit(trial_tcs.flatten().reshape(-1, 1))\n",
    "# #         null_estimator = kds().fit(trial_tcs)\n",
    "#         # allocate memory for the ref cosim\n",
    "#         ref_cosim = np.zeros([shuffle_number, 1])\n",
    "#         # get the unity line\n",
    "#         unity_line = np.ones_like(trial_tcs[:1, :])\n",
    "#         # generate samples from the distribution\n",
    "#         for shuffle in np.arange(shuffle_number):\n",
    "#             # generate a sample TC\n",
    "#             sample_tc = null_estimator.sample(10).reshape(1, -1)\n",
    "# #             sample_tc = null_estimator.sample(1)\n",
    "#             # calculate the ref cosim and save\n",
    "# #             ref_cosim[shuffle] = 1 - np.abs(smet.pairwise.cosine_similarity(sample_tc, unity_line)[0][0])\n",
    "#         # get the threshold\n",
    "#         cosim_threshold = np.percentile(ref_cosim, percentile)\n",
    "#         # calculate the real and shuffle responsivity\n",
    "#         for cell in np.arange(cell_number):    \n",
    "\n",
    "#             # calculate the real cosim\n",
    "#             current_tc = trial_tcs[cell:cell+1, :]\n",
    "#             # calculate the cosine similarity to the unity line\n",
    "#             real_cosim = 1 - np.abs(smet.pairwise.cosine_similarity(current_tc, unity_line)[0][0])\n",
    "#             # determine if it passes the threshold\n",
    "#             test_cosim = real_cosim > cosim_threshold\n",
    "#             # store both values\n",
    "#             tc_resp_cell[cell, 0] = real_cosim\n",
    "#             tc_resp_cell[cell, 1] = test_cosim\n",
    "# #             freq, edges = np.histogram(ref_cosim.flatten())\n",
    "# #             hist = hv.Histogram((edges, freq)).opts(title=str(real_cosim))\n",
    "# #             print(smet.pairwise.euclidean_distances(current_tc.T))\n",
    "#             hist = hv.Image((smet.pairwise.euclidean_distances(current_tc.T)))\n",
    "#             hist.opts(tools=['hover'])\n",
    "#             raise ValueError\n",
    "            \n",
    "#         # store the feature\n",
    "#         tc_trial[feature_name] = tc_resp_cell\n",
    "#     # store the trial\n",
    "#     tc_resp.append(tc_trial)\n",
    "            "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6323abf3",
   "metadata": {},
   "source": [
    "# %%time\n",
    "# plot the results\n",
    "\n",
    "# get the number of features\n",
    "feature_number = feature_raw_trials[0].shape[1]\n",
    "plot_list = []\n",
    "# for all features\n",
    "for feature in np.arange(feature_number):\n",
    "     # get the name\n",
    "    feature_name = feature_raw_trials[0].columns[feature]\n",
    "    # collect data across trials\n",
    "    across_cons = np.vstack([el[feature_name] for el in tc_cons])\n",
    "    across_resp = np.vstack([el[feature_name] for el in tc_resp])\n",
    "    \n",
    "    # also generate maps to identify the trial and cell\n",
    "    trial_map = np.hstack([np.ones([el[feature_name].shape[0]])*idx \n",
    "                           for idx, el in enumerate(tc_cons)]).T.astype(int)\n",
    "    cell_map = np.hstack([np.arange(el[feature_name].shape[0]) \n",
    "                          for el in tc_cons]).T.astype(int)\n",
    "    \n",
    "    # plot\n",
    "    both_pass = (across_cons[:, 1]==1) & (across_resp[:, 1]==1)\n",
    "    both_plot = hv.Scatter((across_cons[both_pass, 0], across_resp[both_pass, 0]), \n",
    "                      kdims=['Consistency'], vdims=['Responsivity'])\n",
    "    both_plot.opts(title=feature_name, shared_axes=False)\n",
    "    none_plot = hv.Scatter((across_cons[~both_pass, 0], across_resp[~both_pass, 0]), \n",
    "                      kdims=['Consistency'], vdims=['Responsivity'])\n",
    "    plot_list.append(both_plot*none_plot)\n",
    "hv.Layout(plot_list).opts(shared_axes=False)\n",
    "    \n",
    "    "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8895b1c2",
   "metadata": {},
   "source": [
    "def plot_map(feature_0, feature_1, current_calcium, target_features, cmap='kbc_r', bins=10):\n",
    "    # remove nans\n",
    "    keep_vector = ~np.isnan(feature_0) & ~np.isnan(feature_1)\n",
    "    feature_0 = feature_0[keep_vector]\n",
    "    feature_1 = feature_1[keep_vector]\n",
    "    current_calcium = current_calcium[keep_vector]\n",
    "    \n",
    "    # get the counts and actual maps\n",
    "    counts = stat.binned_statistic_2d(feature_0, feature_1, feature_1, statistic='count', bins=bins)[0]\n",
    "    values, x_edge, y_edge, idx = \\\n",
    "        stat.binned_statistic_2d(feature_0, feature_1, current_calcium, statistic='sum', bins=bins)\n",
    "    \n",
    "    # generate the map\n",
    "    counts[counts<3] = 0\n",
    "    norm_map = values/counts\n",
    "    norm_map[np.isnan(norm_map)] = 0\n",
    "    norm_map[np.isinf(norm_map)] = 0\n",
    "    \n",
    "    plot = hv.Image((x_edge, y_edge, norm_map.T), kdims=target_features)\n",
    "    plot.opts(tools=['hover'], cmap=cmap)\n",
    "    return plot, x_edge, y_edge, counts"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92a5ef3",
   "metadata": {},
   "source": [
    "# Plot examples\n",
    "\n",
    "# define the target variables (first is used for the thresholds)\n",
    "target_features = ['cricket_0_mouse_distance', 'cricket_0_delta_heading']\n",
    "\n",
    "# define the number of cells to plot\n",
    "example_number = 20\n",
    "# get the pass vector\n",
    "across_cons_0 = np.vstack([el[target_features[0]] for el in tc_cons])\n",
    "across_resp_0 = np.vstack([el[target_features[0]] for el in tc_resp])\n",
    "both_pass_0 = (across_cons_0[:, 1]==1) & (across_resp_0[:, 1]==1)\n",
    "\n",
    "across_cons_1 = np.vstack([el[target_features[1]] for el in tc_cons])\n",
    "across_resp_1 = np.vstack([el[target_features[1]] for el in tc_resp])\n",
    "both_pass_1 = (across_cons_1[:, 1]==1) & (across_resp_1[:, 1]==1)\n",
    "\n",
    "both_pass = (both_pass_0)# & (both_pass_1)\n",
    "\n",
    "# find the top cells\n",
    "pass_cells = np.argwhere(both_pass).flatten()\n",
    "# sort based on responsivity\n",
    "sort_idx = np.argsort(across_cons[both_pass, 0])\n",
    "pass_cells = pass_cells[sort_idx[::-1]]\n",
    "# get the corresponding trial and cell idx\n",
    "trial_idx = trial_map[pass_cells]\n",
    "cell_idx = cell_map[pass_cells]\n",
    "print(f'Trials with target cells: {trial_idx}')\n",
    "print(f'Target cells: {cell_idx}')\n",
    "# define the number to plot based on the available cells\n",
    "plot_number = np.min([example_number, pass_cells.shape[0]])\n",
    "print(f'Number of cells found: {pass_cells.shape[0]}')\n",
    "# allocate a list for the map\n",
    "plot_list = []\n",
    "\n",
    "# for each cell\n",
    "for cell in np.arange(plot_number):\n",
    "#     # get the calcium activity\n",
    "#     current_calcium = calcium_trials[trial_idx[cell]][:, cell_idx[cell]]\n",
    "#     # get the values for both properties\n",
    "#     feature_0 = feature_raw_trials[trial_idx[cell]].loc[:, target_features[0]].to_numpy()\n",
    "#     feature_1 = feature_raw_trials[trial_idx[cell]].loc[:, target_features[1]].to_numpy()\n",
    "    \n",
    "#     plot, x_edge, y_edge, counts = \\\n",
    "#         plot_map(feature_0, feature_1, current_calcium, target_features, bins=20)\n",
    "    # get the full TC\n",
    "    current_tc = tc_full[trial_idx[cell]][target_features[0]][cell_idx[cell]]\n",
    "    plot = hv.Curve((current_tc))\n",
    "    plot.opts(color='red')\n",
    "    plot_list.append(plot)\n",
    "\n",
    "# count_plot = hv.Image((x_edge, y_edge, np.log10(counts.T)), kdims=target_features)\n",
    "# count_plot.opts(cmap='viridis')\n",
    "# plot_list.append(count_plot)\n",
    "hv.Layout(plot_list).opts(shared_axes=True).cols(3)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01479e57",
   "metadata": {},
   "source": [
    "# plot all maps on a day\n",
    "# define the target variables \n",
    "# target_features = ['mouse_speed','latent_0']\n",
    "# define the target trial\n",
    "target_trial = 0\n",
    "# get the number of cells\n",
    "cell_number = calcium_trials[target_trial].shape[1]\n",
    "# allocate the plot list\n",
    "plot_list = []\n",
    "# for each cell\n",
    "for cell in np.arange(cell_number):\n",
    "#     # get the calcium activity\n",
    "#     current_calcium = calcium_trials[target_trial][:, cell]\n",
    "#     # get the values for both properties\n",
    "#     feature_0 = feature_raw_trials[target_trial].loc[:, target_features[0]].to_numpy()\n",
    "#     feature_1 = feature_raw_trials[target_trial].loc[:, target_features[1]].to_numpy()\n",
    "\n",
    "    # define the colormap based on the the criteria\n",
    "    if (target_trial in trial_idx) & (cell in cell_idx):\n",
    "        cmap = 'cet_linear_kry_5_98_c75_r'\n",
    "        color = 'red'\n",
    "    else:\n",
    "        cmap = 'kbc_r'\n",
    "        color = 'blue'\n",
    "    # get the full TC\n",
    "    current_tc = tc_full[target_trial][target_features[0]][cell]\n",
    "    plot = hv.Curve((current_tc))\n",
    "    plot.opts(color=color)\n",
    "#     plot, x_edge, y_edge, counts = \\\n",
    "#         plot_map(feature_0, feature_1, current_calcium, target_features, cmap=cmap, bins=20)\n",
    "    plot_list.append(plot)\n",
    "# count_plot = hv.Image((x_edge, y_edge, np.log10(counts.T)), kdims=target_features)\n",
    "# count_plot.opts(cmap='viridis')\n",
    "# plot_list.append(count_plot)\n",
    "hv.Layout(plot_list).opts(shared_axes=True).cols(3)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8219e8",
   "metadata": {},
   "source": [
    "# visualize the two halves\n",
    "# define the target variables \n",
    "# target_features = ['mouse_speed','latent_0']\n",
    "# define the target trial\n",
    "target_trial = 0\n",
    "# get the number of cells\n",
    "cell_number = calcium_trials[target_trial].shape[1]\n",
    "# get the interval length\n",
    "interval = int(np.floor(calcium_trials[target_trial].shape[0]/2))\n",
    "# allocate the plot list\n",
    "plot_list = []\n",
    "# for each cell\n",
    "for cell in np.arange(cell_number):\n",
    "#     # get the calcium activity\n",
    "#     current_calcium_all = calcium_trials[target_trial][:, cell]\n",
    "#     # get the values for both properties\n",
    "#     feature_0_all = feature_raw_trials[target_trial].loc[:, target_features[0]].to_numpy()\n",
    "#     feature_1_all = feature_raw_trials[target_trial].loc[:, target_features[1]].to_numpy()\n",
    "    # for both halves\n",
    "#     for half in np.arange(2):\n",
    "#         selection_vector = np.array(np.arange(interval)) + half*interval\n",
    "#         feature_0 = feature_0_all[selection_vector]\n",
    "#         feature_1 = feature_1_all[selection_vector]\n",
    "#         current_calcium = current_calcium_all[selection_vector]\n",
    "        # define the colormap based on the the criteria\n",
    "#         if (target_trial in trial_idx) & (cell in cell_idx):\n",
    "#             cmap = 'cet_linear_kry_5_98_c75_r'\n",
    "#         else:\n",
    "#             cmap = 'kbc_r'\n",
    "#         plot, x_edge, y_edge, counts = \\\n",
    "#             plot_map(feature_0, feature_1, current_calcium, target_features, cmap=cmap, bins=20)\n",
    "        # get the half tuning curves\n",
    "        first_half = tc_all[target_trial][target_features[0]][0][cell]\n",
    "        second_half = tc_all[target_trial][target_features[0]][1][cell]\n",
    "        if (target_trial in trial_idx) & (cell in cell_idx):\n",
    "            color = 'red'\n",
    "        else:\n",
    "            color = 'blue'\n",
    "        current_cons = np.round(tc_cons[target_trial][target_features[0]][cell, 0], 2)\n",
    "        current_resp = np.round(tc_resp[target_trial][target_features[0]][cell, 0], 2)\n",
    "        first_plot = hv.Curve((first_half))\n",
    "        first_plot.opts(color=color, title=str(current_cons)+'_'+str(current_resp))\n",
    "        second_plot = hv.Curve((second_half)).opts(color=color)\n",
    "        \n",
    "        plot = first_plot*second_plot\n",
    "        plot_list.append(plot)\n",
    "# count_plot = hv.Image((x_edge, y_edge, np.log10(counts.T)), kdims=target_features)\n",
    "# count_plot.opts(cmap='viridis')\n",
    "# plot_list.append(count_plot)\n",
    "hv.Layout(plot_list).opts(shared_axes=False).cols(2)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93d5ce8",
   "metadata": {},
   "source": [
    "# Generate 2D \"tuning curves\"\n",
    "raise ValueError\n",
    "# define trial groups\n",
    "n_groups = 2\n",
    "interval = np.floor(len(calcium_trials)/n_groups)\n",
    "temp_vector = np.array(np.arange(interval))\n",
    "trial_groups = [(temp_vector+idx*interval).astype(int) for idx in np.arange(n_groups)]\n",
    "\n",
    "# define the features to use\n",
    "tc_features = ['cricket_0_mouse_distance', 'cricket_0_delta_heading']\n",
    "# tc_features = ['mouse_speed', 'latent_0']\n",
    "# tc_features = ['latent_1', 'latent_0']\n",
    "# tc_features = ['cricket_0_x', 'cricket_0_y']\n",
    "# tc_features = ['mouse_speed', 'cricket_0_mouse_distance']\n",
    "# tc_features = ['mouse_y', 'mouse_x']\n",
    "# tc_features = ['cricket_0_visual_angle', 'cricket_0_delta_head']\n",
    "\n",
    "print(feature_raw_trials[0].columns)\n",
    "# allocate the meta plot list\n",
    "meta_list = []\n",
    "hist_list = []\n",
    "# define the font size\n",
    "fontsize = {\n",
    "    'ticks': 11,\n",
    "    'labels': 13\n",
    "}\n",
    "\n",
    "# define the plot labels\n",
    "label_x = tc_features[1]\n",
    "label_y = tc_features[0]\n",
    "# for all the trial groups\n",
    "for trial_idx in trial_groups:\n",
    "    # allocate the plot list\n",
    "    tc_list = []\n",
    "    hist_temp_list = []\n",
    "    \n",
    "    feature_raw = [feature_raw_trials[el] for el in trial_idx]\n",
    "    calcium_matrix = np.concatenate([calcium_trials[el] for el in trial_idx], axis=0)\n",
    "    # get the relevant features\n",
    "    feature0 = pd.concat([el[tc_features[0]] for el in feature_raw])\n",
    "    feature0[np.isnan(feature0)] = 0\n",
    "    feature0 = ss.medfilt(feature0, 21)\n",
    "    feature1 = pd.concat([el[tc_features[1]] for el in feature_raw])\n",
    "    feature1[np.isnan(feature1)] = 0\n",
    "    feature1 = ss.medfilt(feature1, 21)\n",
    "\n",
    "    print(calcium_matrix.shape)\n",
    "    roi_number = calcium_matrix.shape[1]\n",
    "\n",
    "\n",
    "    st_base, x_edge, y_edge, idx = \\\n",
    "        stat.binned_statistic_2d(feature0, feature1, [], bins=20, statistic='count')\n",
    "    st_base[st_base==0] = 1\n",
    "    plot_st_base = np.log10(st_base)\n",
    "    plot_st_base[np.isinf(plot_st_base)] = 0\n",
    "    # st_base = (st_base-np.nanmin(st_base))/(np.nanmax(st_base)-np.nanmin(st_base))\n",
    "#     im_plot = hv.Image((plot_st_base), kdims=[tc_features[0], tc_features[1]], bounds=[0, 10, 0, 20])\n",
    "    im_plot = hv.Image((y_edge, x_edge, plot_st_base), kdims=[label_x, label_y])\n",
    "    im_plot.opts(width=250, tools=['hover'], fontsize=fontsize, xrotation=45)#, clim=(0, 0.2))\n",
    "\n",
    "    tc_list.append(im_plot)\n",
    "    # st_base = 1\n",
    "    # store the matrices for averaging\n",
    "    st_store = []\n",
    "    # for all the cells\n",
    "    for cells in np.arange(roi_number)[:]:\n",
    "        # get the calcium\n",
    "        current_calcium = calcium_matrix[:, cells]\n",
    "    #     current_calcium = ss.medfilt(pd.concat([el['mouse_speed'] for el in feature_raw]), 21)\n",
    "\n",
    "        # build 2d distribution\n",
    "        st, x_edge, y_edge, idx = \\\n",
    "            stat.binned_statistic_2d(feature0, feature1, current_calcium, bins=20, statistic='sum')\n",
    "    #     st[np.isnan(st)] = 0\n",
    "    #     st = (st-np.nanmin(st))/(np.nanmax(st)-np.nanmin(st))\n",
    "    #     st[np.isnan(st)] = 0\n",
    "        st[st_base<3] = 0\n",
    "        st_plot = (st/st_base)\n",
    "        st_plot[np.isnan(st_plot)] = 0\n",
    "        im_plot = hv.Image((y_edge, x_edge, st_plot), kdims=[label_x, label_y])\n",
    "        im_plot.opts(width=250, tools=['hover'], cmap='viridis', fontsize=fontsize, xrotation=45)#, clim=(0, 0.2))\n",
    "        tc_list.append(im_plot)\n",
    "        st_store.append(st)\n",
    "        \n",
    "        # store the actual map too\n",
    "        hist_temp_list.append(st_plot)\n",
    "\n",
    "    plot_ave = np.mean(st_store, axis=0)/st_base\n",
    "    # plot_ave[np.isinf(plot_ave)] = 0\n",
    "    im_plot = hv.Image((y_edge, x_edge, plot_ave), kdims=[label_x, label_y])\n",
    "    im_plot.opts(width=250, tools=['hover'], fontsize=fontsize, xrotation=45)#, clim=(0, 0.2))\n",
    "\n",
    "    tc_list.append(im_plot)\n",
    "    meta_list.append(tc_list)\n",
    "    hist_list.append(hist_temp_list)\n",
    "# reorder the list\n",
    "\n",
    "# top_half = [el for idx, el in enumerate(tc_list) if idx < len(tc_list)/2]\n",
    "# bottom_half = [el for idx, el in enumerate(tc_list) if idx >= len(tc_list)/2]\n",
    "# tc_list = [val for pair in zip(top_half, bottom_half) for val in pair]\n",
    "# lists = [l1, l2, ...]\n",
    "# [val for tup in zip(*lists) for val in tup]\n",
    "meta_list = [val for tup in zip(*meta_list) for val in tup]\n",
    "# create the layout\n",
    "hv.Layout(meta_list).opts(shared_axes=False)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78d7360",
   "metadata": {},
   "source": [
    "# calculate correlations between maps and plot\n",
    "\n",
    "# allocate memory for the correlations\n",
    "correlation_array = []\n",
    "# for all the cells\n",
    "for one, two in zip(hist_list[0], hist_list[1]):\n",
    "    # calculate the correlation between the pairs\n",
    "    corr_coef = np.corrcoef(one.flatten(), two.flatten())[1][0]\n",
    "    if np.isnan(corr_coef):\n",
    "#         continue\n",
    "        corr_coef = 0\n",
    "    correlation_array.append(corr_coef)\n",
    "\n",
    "correlation_array = np.array(correlation_array)\n",
    "\n",
    "# define the number of shuffles\n",
    "shuffle_number = 100\n",
    "# allocate memory for the shuffle results\n",
    "shuffle_array = np.zeros((correlation_array.shape[0], shuffle_number))\n",
    "# for all the shuffles\n",
    "for shuffle in np.arange(shuffle_number):\n",
    "    # allocate memory for the correlations\n",
    "    correlation_shuffle = []\n",
    "    # shuffle the lists\n",
    "    list_0 = random.sample(hist_list[0], len(hist_list[0]))\n",
    "    list_1 = random.sample(hist_list[1], len(hist_list[0]))\n",
    "    # for all the cells\n",
    "    for one, two in zip(list_0, list_1):\n",
    "        # calculate the correlation between the pairs\n",
    "        corr_coef = np.corrcoef(one.flatten(), two.flatten())[1][0]\n",
    "        if np.isnan(corr_coef):\n",
    "            corr_coef = 0\n",
    "#             continue\n",
    "        correlation_shuffle.append(corr_coef)\n",
    "    shuffle_array[:, shuffle] = correlation_shuffle\n",
    "\n",
    "    \n",
    "# turn into an array\n",
    "freq, bins = np.histogram(correlation_array, density=True, bins=20)\n",
    "bin_centers = bins[:-1] + np.diff(bins)/2\n",
    "hist_ori = hv.Curve((bin_centers, np.cumsum(freq)), kdims=['Correlation', 'Probability'])\n",
    "hist_ori.opts(fontsize=fontsize)\n",
    "\n",
    "freq, bins = np.histogram(shuffle_array.flatten(), density=True, bins=bins)\n",
    "bin_centers = bins[:-1] + np.diff(bins)/2\n",
    "hist_shuffle = hv.Curve((bin_centers, np.cumsum(freq)))\n",
    "\n",
    "\n",
    "(hist_ori*hist_shuffle)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaeb7da3",
   "metadata": {},
   "source": [
    "%%time\n",
    "# Calculate the half and full tuning of each cell \n",
    "importlib.reload(processing_parameters)\n",
    "\n",
    "# define the pairs to quantify\n",
    "target_pairs = [['cricket_0_mouse_distance', 'cricket_0_delta_heading'], ['mouse_x', 'mouse_y']]\n",
    "# target_pairs = [['mouse_speed', 'cricket_0_speed']]\n",
    "# get the number of pairs\n",
    "pair_number = len(target_pairs)\n",
    "# define the number of calcium shuffles\n",
    "shuffle_number = 100\n",
    "# define the confidence interval cutoff\n",
    "percentile = 95\n",
    "# define the number of bins for the TCs\n",
    "bin_number = 10\n",
    "# allocate memory for the output\n",
    "tc_all = []\n",
    "tc_full = []\n",
    "tc_resp = []\n",
    "# get the number of features\n",
    "feature_number = feature_raw_trials[0].shape[1]\n",
    "# for all the trials\n",
    "for idx, trial in enumerate(feature_raw_trials):\n",
    "    # get the calcium data\n",
    "    current_calcium = calcium_trials[idx]\n",
    "    # get the number of cells\n",
    "    cell_number = current_calcium.shape[1]\n",
    "    # allocate memory for the trial TCs\n",
    "    tc_trial = {}\n",
    "    tc_trial_full = {}\n",
    "    tc_trial_resp = {}\n",
    "    # for all the features\n",
    "    for pair_idx in np.arange(pair_number):\n",
    "        # get the current feature\n",
    "#         current_feature = trial.to_numpy()[:, feature]\n",
    "        feature_names = target_pairs[pair_idx]\n",
    "        # concatenate for the dict\n",
    "        feature_name = '|'.join(feature_names)\n",
    "        current_feature_0 = trial.loc[:, feature_names[0]].to_numpy()\n",
    "        current_feature_1 = trial.loc[:, feature_names[1]].to_numpy()\n",
    "        # get the bins from the parameters file\n",
    "        bin_ranges = processing_parameters.tc_params[feature_name]\n",
    "        # calculate the bin edges based on the ranges\n",
    "        bins = [np.linspace(el[0], el[1], num=bin_number+1) for el in bin_ranges]\n",
    "        # get the name\n",
    "#         feature_name = trial.columns[feature]\n",
    "        # allocate a list for the 2 halves\n",
    "        tc_half = []\n",
    "        # exclude nan values\n",
    "        keep_vector_full = (~np.isnan(current_feature_0)) & (~np.isnan(current_feature_1))\n",
    "        counts_feature_0 = current_feature_0[keep_vector_full]\n",
    "        counts_feature_1 = current_feature_1[keep_vector_full]\n",
    "        # get the counts\n",
    "        feature_counts = stat.binned_statistic_2d(counts_feature_0, counts_feature_1, counts_feature_0, \n",
    "                                               statistic='count', bins=bins)[0]\n",
    "        # zero the positions with less than 3 counts\n",
    "        feature_counts[feature_counts<3] = 0\n",
    "        # for first and second half\n",
    "        for half in np.arange(2):\n",
    "            # get the half vector\n",
    "#             print(current_feature.shape)\n",
    "            half_bound = int(np.floor(current_feature_0.shape[0]/2))\n",
    "            half_vector = np.arange(half_bound) + half_bound*half\n",
    "            half_feature_0 = current_feature_0[half_vector]\n",
    "            half_feature_1 = current_feature_1[half_vector]\n",
    "            # exclude nan values\n",
    "            keep_vector = (~np.isnan(half_feature_0)) & (~np.isnan(half_feature_1))\n",
    "            keep_feature_0 = half_feature_0[keep_vector]\n",
    "            keep_feature_1 = half_feature_1[keep_vector]\n",
    "\n",
    "            # allocate a list for the cells\n",
    "            tc_cell = []\n",
    "            # for all the cells\n",
    "            for cell in np.arange(cell_number):\n",
    "                # get the current cell\n",
    "                half_cell = current_calcium[half_vector, cell]\n",
    "                keep_cell = half_cell[keep_vector]\n",
    "\n",
    "                # calculate the TC\n",
    "                current_tc = stat.binned_statistic_2d(keep_feature_0, keep_feature_1, keep_cell, \n",
    "                                                   statistic='sum', bins=bins)[0]\n",
    "                # normalize the TC\n",
    "                norm_tc = current_tc/feature_counts\n",
    "                # remove nans and infs\n",
    "                norm_tc[np.isnan(norm_tc)] = 0\n",
    "                norm_tc[np.isinf(norm_tc)] = 0\n",
    "                # store\n",
    "                tc_cell.append(norm_tc)\n",
    "            # store the cells\n",
    "            tc_half.append(tc_cell)\n",
    "        # allocate memory for the full tc per cell\n",
    "        tc_cell_full = []\n",
    "        tc_cell_resp = np.zeros((cell_number, 2))\n",
    "        # calculate the full TC\n",
    "        for cell in np.arange(cell_number):\n",
    "            keep_cell = current_calcium[keep_vector_full, cell]\n",
    "            tc_cell = stat.binned_statistic_2d(counts_feature_0, counts_feature_1, \n",
    "                                            keep_cell, statistic='sum', bins=bins)[0]\n",
    "            tc_cell = tc_cell/feature_counts\n",
    "            tc_cell[np.isnan(tc_cell)] = 0\n",
    "            tc_cell[np.isinf(tc_cell)] = 0\n",
    "            # allocate memory for the shuffles\n",
    "            shuffle_array = np.zeros((shuffle_number, bin_number, bin_number))\n",
    "            # generate the shuffles\n",
    "            for shuffle in np.arange(shuffle_number):\n",
    "                # randomize the calcium activity\n",
    "                random_cell = keep_cell.copy()\n",
    "                np.random.shuffle(random_cell)\n",
    "                tc_random = stat.binned_statistic_2d(counts_feature_0, counts_feature_1,\n",
    "                                                     random_cell, statistic='sum', bins=bins)[0]\n",
    "                tc_random = tc_random/feature_counts\n",
    "                tc_random[np.isnan(tc_random)] = 0\n",
    "                tc_random[np.isinf(tc_random)] = 0\n",
    "                shuffle_array[shuffle, :, :] = tc_random\n",
    "            # get the threshold\n",
    "            resp_threshold = np.percentile(np.abs(shuffle_array.flatten()), percentile)\n",
    "            # threshold the TC\n",
    "#             tc_cell[np.abs(tc_cell)<resp_threshold] = 0\n",
    "            # fill up the responsivity matrix\n",
    "#             tc_cell_resp[cell, 0] = np.abs(np.max(np.abs(tc_cell)) - resp_threshold)/resp_threshold\n",
    "#             tc_cell_resp[cell, 0] = np.max(np.abs(tc_cell))/resp_threshold\n",
    "            tc_cell_resp[cell, 0] = np.mean(np.sort(np.abs(tc_cell), axis=None)[-3:])/resp_threshold\n",
    "            tc_cell_resp[cell, 1] = np.sum(np.abs(tc_cell)>resp_threshold) > 3\n",
    "            # store\n",
    "            tc_cell_full.append(tc_cell)\n",
    "#             tc_cell_resp.append(resp_threshold)\n",
    "        # store the halfs and fulls\n",
    "        tc_trial[feature_name] = tc_half\n",
    "        tc_trial_full[feature_name] = tc_cell_full\n",
    "        tc_trial_resp[feature_name] = tc_cell_resp\n",
    "    # store the trials\n",
    "    tc_all.append(tc_trial)\n",
    "    tc_full.append(tc_trial_full)\n",
    "    tc_resp.append(tc_trial_resp)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38c5c33",
   "metadata": {},
   "source": [
    "%%time\n",
    "# calculate consistency\n",
    "\n",
    "# define the number of shuffles\n",
    "shuffle_number = 100\n",
    "# define the percentile\n",
    "percentile = 95\n",
    "# allocate memory for the output\n",
    "tc_cons = []\n",
    "# get the number of features\n",
    "feature_number = feature_raw_trials[0].shape[1]\n",
    "# for all the trials\n",
    "for idx, trial in enumerate(feature_raw_trials):\n",
    "#     # get the calcium data\n",
    "#     current_calcium = calcium_trials[idx]\n",
    "    # get the number of cells\n",
    "    cell_number = calcium_trials[idx].shape[1]\n",
    "    # allocate memory for the trial TCs\n",
    "    tc_trial = {}\n",
    "    # for all the features\n",
    "    for pair_idx in np.arange(pair_number):\n",
    "\n",
    "        # get the name\n",
    "#         feature_name = trial.columns[feature]\n",
    "        feature_names = target_pairs[pair_idx]\n",
    "        feature_name = '|'.join(feature_names)\n",
    "        # allocate an array for the correlations and tests\n",
    "        tc_half = np.zeros([cell_number, 2])\n",
    "        # get the two halves\n",
    "        halves = tc_all[idx][feature_name]\n",
    "        \n",
    "        # calculate the real and shuffle correlation\n",
    "        for cell in np.arange(cell_number):\n",
    "            # get the current cell first and second half\n",
    "            current_first = halves[0][cell].flatten()\n",
    "            current_second = halves[1][cell].flatten()\n",
    "            # real correlation\n",
    "            real_correlation = np.corrcoef(current_first, current_second)[1][0]\n",
    "            \n",
    "            # shuffle array\n",
    "            shuffle_array = np.zeros([shuffle_number, 1])\n",
    "            # calculate the confidence interval\n",
    "            for shuffle in np.arange(shuffle_number):\n",
    "#                 random_second = halves[1][random.randrange(cell_number)].flatten()\n",
    "                random_second = current_second.copy().flatten()\n",
    "                np.random.shuffle(random_second)\n",
    "                shuffle_array[shuffle] = np.corrcoef(current_first, random_second)[1][0]\n",
    "            # turn nans into 0\n",
    "            shuffle_array[np.isnan(shuffle_array)] = 0\n",
    "            # rectify\n",
    "#             shuffle_array[shuffle_array<0] = 0\n",
    "            # get the confidence interval\n",
    "            conf_interval = np.percentile(shuffle_array, percentile)\n",
    "            # store the correlation and whether it passes the criterion\n",
    "            tc_half[cell, 0] = real_correlation\n",
    "            tc_half[cell, 1] = (real_correlation > conf_interval) & (real_correlation > 0) & \\\n",
    "                               (conf_interval > 0)\n",
    "#             freq, edges = np.histogram(shuffle_array.flatten())\n",
    "#             hist = hv.Histogram((edges, freq)).opts(title=str(real_correlation))\n",
    "#             raise ValueError\n",
    "            \n",
    "        # store for the variable\n",
    "        tc_trial[feature_name] = tc_half\n",
    "    # store for the trial\n",
    "    tc_cons.append(tc_trial)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be32fa46",
   "metadata": {},
   "source": [
    "# plot the criteria distributions\n",
    "# %%time\n",
    "# plot the results\n",
    "\n",
    "# get the number of features\n",
    "# feature_number = feature_raw_trials[0].shape[1]\n",
    "plot_list = []\n",
    "map_dict = {}\n",
    "# for all features\n",
    "for pair_idx in np.arange(pair_number):\n",
    "     # get the name\n",
    "#     feature_name = feature_raw_trials[0].columns[feature]\n",
    "    feature_name = list(tc_cons[0].keys())[pair_idx]\n",
    "    # collect data across trials\n",
    "    across_cons = np.vstack([el[feature_name] for el in tc_cons])\n",
    "    across_resp = np.vstack([el[feature_name] for el in tc_resp])\n",
    "    \n",
    "    # also generate maps to identify the trial and cell\n",
    "    trial_map = np.hstack([np.ones([el[feature_name].shape[0]])*idx \n",
    "                           for idx, el in enumerate(tc_cons)]).T.astype(int)\n",
    "    cell_map = np.hstack([np.arange(el[feature_name].shape[0]) \n",
    "                          for el in tc_cons]).T.astype(int)\n",
    "    \n",
    "    # plot\n",
    "    both_pass = (across_cons[:, 1]==1) & (across_resp[:, 1]==1)\n",
    "    both_plot = hv.Scatter((across_cons[both_pass, 0], across_resp[both_pass, 0]), \n",
    "                      kdims=['Consistency'], vdims=['Responsivity'])\n",
    "    both_plot.opts(title=feature_name, shared_axes=False)\n",
    "    none_plot = hv.Scatter((across_cons[~both_pass, 0], across_resp[~both_pass, 0]), \n",
    "                      kdims=['Consistency'], vdims=['Responsivity'])\n",
    "    plot_list.append(both_plot*none_plot)\n",
    "    \n",
    "    # store the maps and vector for plotting later\n",
    "    map_dict[feature_name] = [trial_map, cell_map, both_pass]\n",
    "    print(f'Number of cells passing the thresholds for {feature_name}: {np.sum(both_pass)}')\n",
    "hv.Layout(plot_list).opts(shared_axes=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004ad0bf",
   "metadata": {},
   "source": [
    "# plot the individual TCs\n",
    "\n",
    "# define the target trial\n",
    "target_trial = 0\n",
    "# define the target feature\n",
    "target_feature = 'cricket_0_mouse_distance|cricket_0_delta_heading'\n",
    "# target_feature = 'mouse_speed|cricket_0_speed'\n",
    "# target_feature = 'mouse_x|mouse_y'\n",
    "# get the maps and cell vector\n",
    "trial_map = map_dict[target_feature][0]\n",
    "cell_map = map_dict[target_feature][1]\n",
    "both_pass = map_dict[target_feature][2]\n",
    "# get the number of cells\n",
    "cell_number = calcium_trials[target_trial].shape[1]\n",
    "\n",
    "# get the half TCs\n",
    "half_0 = tc_all[target_trial][target_feature][0]\n",
    "half_1 = tc_all[target_trial][target_feature][1]\n",
    "fulls = tc_full[target_trial][target_feature]\n",
    "\n",
    "# get the list of cells that pass the threshold\n",
    "trial_idx = trial_map[both_pass]\n",
    "cell_idx = cell_map[both_pass]\n",
    "\n",
    "# allocate the plot list\n",
    "plot_list = []\n",
    "# for each cell\n",
    "for cell in np.arange(cell_number):\n",
    "    # get the responsivity and consistency values\n",
    "    current_cons = np.round(tc_cons[target_trial][target_feature][cell, 0], 2)\n",
    "    current_resp = np.round(tc_resp[target_trial][target_feature][cell, 0], 2)    \n",
    "    # plot them\n",
    "\n",
    "    # define the colormap based on the the criteria\n",
    "    if (target_trial in trial_idx) & (cell in cell_idx):\n",
    "        cmap = 'cet_linear_kry_5_98_c75_r'\n",
    "    else:\n",
    "        cmap = 'kbc_r'\n",
    "#     plot, x_edge, y_edge, counts = \\\n",
    "#         plot_map(feature_0, feature_1, current_calcium, target_features, cmap=cmap, bins=20)\n",
    "    half_plot_0 = hv.Image(half_0[cell])\n",
    "    half_plot_1 = hv.Image(half_1[cell])\n",
    "    full_plot = hv.Image(fulls[cell])\n",
    "    full_plot.opts(title=str(current_cons)+'_'+str(current_resp), cmap=cmap)\n",
    "    plot_list.append(half_plot_0)\n",
    "    plot_list.append(half_plot_1)\n",
    "    plot_list.append(full_plot)\n",
    "#     first_plot.opts(color=color, title=str(current_cons)+'_'+str(current_resp))\n",
    "\n",
    "# count_plot = hv.Image((x_edge, y_edge, np.log10(counts.T)), kdims=target_features)\n",
    "# count_plot.opts(cmap='viridis')\n",
    "# plot_list.append(count_plot)\n",
    "hv.Layout(plot_list).opts(shared_axes=False).cols(3)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8b6182",
   "metadata": {},
   "source": [
    "# turn into an array\n",
    "freq, bins = np.histogram(correlation_array, density=True, bins=20)\n",
    "bin_centers = bins[:-1] + np.diff(bins)/2\n",
    "hist_ori = hv.Curve((bin_centers, np.cumsum(freq)), kdims=['Correlation', 'Probability'])\n",
    "hist_ori.opts(fontsize=fontsize)\n",
    "\n",
    "freq, bins = np.histogram(shuffle_array.flatten(), density=True, bins=bins)\n",
    "bin_centers = bins[:-1] + np.diff(bins)/2\n",
    "hist_shuffle = hv.Curve((bin_centers, np.cumsum(freq)))\n",
    "\n",
    "\n",
    "(hist_ori*hist_shuffle)"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
