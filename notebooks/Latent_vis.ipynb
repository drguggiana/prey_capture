{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10315f57",
   "metadata": {},
   "source": [
    "# imports\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath(r'D:\\Code Repos\\prey_capture'))\n",
    "\n",
    "import panel as pn\n",
    "import holoviews as hv\n",
    "from bokeh.io import export_svgs, export_png\n",
    "from holoviews import opts, dim\n",
    "from holoviews.operation import histogram\n",
    "hv.extension('bokeh')\n",
    "from bokeh.resources import INLINE\n",
    "\n",
    "import importlib\n",
    "import processing_parameters\n",
    "import datetime\n",
    "\n",
    "import functions_bondjango as bd\n",
    "import functions_plotting as fp\n",
    "import functions_loaders as fl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "import paths\n",
    "import random\n",
    "import scipy.stats as stat\n",
    "import umap\n",
    "from scipy.signal import medfilt\n",
    "import sklearn.cluster as clust\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7165b1b8",
   "metadata": {},
   "source": [
    "# set up the figure config\n",
    "importlib.reload(fp)\n",
    "importlib.reload(processing_parameters)\n",
    "# define the target saving path\n",
    "save_path = os.path.join(paths.figures_path, 'Latent_vis')\n",
    "\n",
    "# define the printing mode\n",
    "save_mode = True\n",
    "# define the target document\n",
    "target_document = 'paper'\n",
    "# set up the figure theme\n",
    "fp.set_theme()\n",
    "# load the label dict\n",
    "label_dict = processing_parameters.label_dictionary\n",
    "variable_list = processing_parameters.variable_list"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d067705",
   "metadata": {},
   "source": [
    "# Load the weights for the motifs\n",
    "\n",
    "# order is [clusters, features]\n",
    "# define the number of clusters\n",
    "n_cluster = 15\n",
    "# get the path\n",
    "first_file = os.listdir(os.path.join(paths.vame_results, 'results'))[0]\n",
    "template_path = os.path.join(paths.vame_results, 'results', first_file, 'VAME', 'kmeans-'+str(n_cluster),\n",
    "                             'cluster_center_'+first_file+'.npy')\n",
    "# load the file\n",
    "motif_weights = np.load(template_path)\n",
    "\n",
    "# plot\n",
    "plot = hv.Raster(motif_weights, kdims=['Latents', 'Motifs'])\n",
    "plot.opts(width=800, height=800, tools=['hover'], colorbar=True, cmap='RdBu', clim=(-3.5, 3.5))\n",
    "plot\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a2f06e",
   "metadata": {},
   "source": [
    "# importlib.reload(processing_parameters)\n",
    "# get the search string\n",
    "# search_string = processing_parameters.search_string\n",
    "\n",
    "# # get the paths from the database\n",
    "# all_path = bd.query_database('analyzed_data', search_string)\n",
    "# input_path = [el['analysis_path'] for el in all_path if '_preproc' in el['slug']]\n",
    "\n",
    "# assemble the output path\n",
    "# out_path = os.path.join(paths.analysis_path, 'test_latentconsolidate.hdf5')\n",
    "# pprint(input_path)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cc7bee",
   "metadata": {},
   "source": [
    "%%time\n",
    "# load the data\n",
    "\n",
    "importlib.reload(fl)\n",
    "importlib.reload(processing_parameters)\n",
    "# get the paths\n",
    "path_list, query_list = fl.query_search_list()\n",
    "\n",
    "# get the data\n",
    "full_df = []\n",
    "frame_list = []\n",
    "meta_list = []\n",
    "# initialize a trial counter\n",
    "trial_idx = 0\n",
    "# for all the paths and queries\n",
    "for path, query in zip(path_list, query_list):\n",
    "#     # get rid of DG_210323_b for now\n",
    "#     path = [el for el in path if 'DG_210323_b' not in el]\n",
    "#     query = [el for el in query if 'DG_210323_b' not in el['analysis_path']]\n",
    "    data, frames, meta = fl.load_preprocessing(path, query, behavior_flag=True)\n",
    "    \n",
    "    # exclude the cells\n",
    "    without_cells = []\n",
    "    # for all the trials in data\n",
    "    for trial in data:\n",
    "        # TODO: fix this in the actual function\n",
    "        if 'badFile' in trial.columns:\n",
    "            continue\n",
    "        if 'sync_frames' in trial.columns:\n",
    "            trial = trial.drop(columns=['sync_frames'])\n",
    "        if 'latent_0' not in trial.columns:\n",
    "            continue\n",
    "\n",
    "        not_cells = [el for el in trial.columns if 'cell' not in el]\n",
    "        no_cells_trial = trial[not_cells]\n",
    "        # remove the nan rows in the latents\n",
    "        latents = [el for el in no_cells_trial.columns if 'latent' in el]\n",
    "        no_nan_vector = ~np.isnan(np.sum(no_cells_trial[latents].to_numpy(), axis=1))\n",
    "        no_cells_trial = no_cells_trial.iloc[no_nan_vector, :]\n",
    "        # save the trial number\n",
    "        no_cells_trial['trial_idx'] = trial_idx\n",
    "        # update the trial counter\n",
    "        trial_idx += 1\n",
    "        without_cells.append(no_cells_trial)\n",
    "        \n",
    "#         print(latents)\n",
    "#         print(no_nan_vector)\n",
    "#         print(without_cells[-1])\n",
    "#         raise ValueError\n",
    "        \n",
    "    \n",
    "    full_df.append(pd.concat(without_cells, axis=0))\n",
    "    frame_list.append(frames)\n",
    "    meta_list.append(meta)\n",
    "\n",
    "full_df = pd.concat(full_df, axis=0)\n",
    "print(full_df.shape)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e0ee45",
   "metadata": {},
   "source": [
    "print(full_df.columns)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2e0412",
   "metadata": {},
   "source": [
    "# Correlate the latents to the other behavioral variables\n",
    "\n",
    "# get the latents\n",
    "latents = [el for el in full_df.columns if 'latent' in el]\n",
    "\n",
    "# allocate memory for the average matrix\n",
    "average_matrix = []\n",
    "# for all the trials\n",
    "for trial_name, trial in full_df.groupby('trial_idx'):\n",
    "    latents_matrix = trial[latents].to_numpy().copy()\n",
    "    # get the behavior\n",
    "    behavior_matrix = trial[variable_list].to_numpy().copy()\n",
    "    temp_matrix = stat.spearmanr(latents_matrix, behavior_matrix)[0]\n",
    "    temp_matrix[np.isnan(temp_matrix)] = 0\n",
    "    average_matrix.append(temp_matrix)\n",
    "    \n",
    "\n",
    "average_matrix = np.mean(average_matrix, axis=0)\n",
    "print(average_matrix.shape)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05829d0f",
   "metadata": {},
   "source": [
    "# plot the matrix\n",
    "\n",
    "# Plot the matrix\n",
    "yticks = [(idx+0.5, el.replace('_', ' ').replace('l', 'L')) for idx, el in enumerate(latents)]\n",
    "xticks = [(idx+0.5, label_dict[el]) for idx, el in enumerate(variable_list)]\n",
    "# ticks = [(idx+0.5, idx) for idx, el in enumerate(variable_list)]\n",
    "plot_matrix = average_matrix.copy()[:len(latents), len(latents):]\n",
    "\n",
    "# plot_matrix = np.tril(plot_matrix, k=0)\n",
    "# plot_matrix[plot_matrix==0] = np.nan\n",
    "# hv.Raster(correlation_matrix)\n",
    "raster = hv.Raster(plot_matrix)\n",
    "raster.opts(tools=['hover'])\n",
    "# format the plot\n",
    "raster.opts(width=950, height=800, yticks=yticks, xticks=xticks, colorbar=True, cmap='RdBu', clim=(-1, 1), xrotation=45, xlabel='Behavior', ylabel='Latents')\n",
    "\n",
    "# assemble the file name\n",
    "save_name = os.path.join(save_path, '_'.join((target_document, 'overall_correlation')) + '.png')\n",
    "# save the figure\n",
    "fig = fp.save_figure(raster, save_name, fig_width=15, dpi=1200, fontsize=target_document, target='screen')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75f5732",
   "metadata": {},
   "source": [
    "def normalize_rows(data_in):\n",
    "    \n",
    "    for idx, el in enumerate(data_in):\n",
    "        data_in[idx, :] = (el-np.nanmin(el))/(np.nanmax(el)-np.nanmin(el))\n",
    "    return data_in"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7540bf56",
   "metadata": {},
   "source": [
    "print(full_df.columns)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6f9604",
   "metadata": {},
   "source": [
    "# visualize latents\n",
    "# define the target trial\n",
    "target_trial = 11\n",
    "\n",
    "# get the target trial\n",
    "# current_trial = pre_data[target_trial][1]\n",
    "current_trial = full_df.iloc[full_df['trial_idx'].to_numpy()==target_trial, :]\n",
    "# print(pre_data[target_trial][0])\n",
    "print(current_trial.shape)\n",
    "print(current_trial.columns[:50])\n",
    "# pprint(current_trial.iloc[-50:, 1])\n",
    "\n",
    "# get the time columns\n",
    "x = current_trial['time_vector']\n",
    "\n",
    "# define the target columns to keep\n",
    "target_columns = ['cricket_0_mouse_distance', 'cricket_0_delta_heading', \n",
    "                  'mouse_speed', 'cricket_0_speed', 'mouse_heading', 'mouse_x', 'mouse_y', 'cricket_0_x', 'cricket_0_y', 'motifs']\n",
    "target_columns += [el for el in current_trial.columns if 'latent' in el]\n",
    "# leave only the target columns\n",
    "current_trial = current_trial[target_columns]\n",
    "y = np.arange(len(current_trial.columns))\n",
    "\n",
    "pprint(current_trial.columns)\n",
    "y_labels = [(idx, el) for idx, el in enumerate(target_columns)]\n",
    "\n",
    "# transpose, get rid of nans and normalize the rows\n",
    "current_trial = current_trial.to_numpy().T\n",
    "current_trial[np.isnan(current_trial)] = 0\n",
    "current_trial = normalize_rows(current_trial)\n",
    "\n",
    "# visualize latents with respect to other variables\n",
    "trial_plot = hv.Image((x, y, current_trial))\n",
    "trial_plot.opts(frame_width=600, frame_height=600, tools=['hover'], colorbar=True, yticks=y_labels)\n",
    "\n",
    "latent_correlation = np.corrcoef(current_trial)\n",
    "correlation_plot = hv.Image((y, y, latent_correlation))\n",
    "correlation_plot.opts(frame_width=600, frame_height=600, tools=['hover'], colorbar=True,\n",
    "                      yticks=y_labels, xticks=y_labels, xrotation=45, cmap='RdBu', clim=(-1, 1))\n",
    "\n",
    "(trial_plot+correlation_plot).opts(shared_axes=False).cols(1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca649f1f",
   "metadata": {},
   "source": [
    "# generate 2D TCs with latents as x and y and a behavioral varible as the Z\n",
    "\n",
    "# define the target behavioral variable\n",
    "target_behavior = 'mouse_speed'\n",
    "# define the latents of interest\n",
    "first_latent = 'latent_0'\n",
    "second_latent = 'latent_1'\n",
    "\n",
    "# define the bins\n",
    "bins = 10\n",
    "# allocate a list for the plots\n",
    "plot_list = []\n",
    "# for all the trials\n",
    "# for trial in pre_data:\n",
    "# get the list of trials\n",
    "trial_list = np.unique(full_df['trial_idx'].to_numpy())\n",
    "for trial_idx in trial_list[:1]:\n",
    "    \n",
    "    trial = full_df.iloc[full_df['trial_idx'].to_numpy()==trial_idx, :]\n",
    "    # get just the dataframe\n",
    "#     trial = trial[1]\n",
    "    # skip if the behavior is not there\n",
    "    if target_behavior not in trial.columns:\n",
    "        continue\n",
    "    # get the variables of interest\n",
    "    feature_0 = trial.loc[:, first_latent]\n",
    "    feature_1 = trial.loc[:, second_latent]\n",
    "    behavior = trial.loc[:, target_behavior]\n",
    "    # remove nans and infs\n",
    "    feature_0[np.isnan(feature_0)] = 0\n",
    "    feature_1[np.isnan(feature_1)] = 0\n",
    "    behavior[np.isnan(behavior)] = 0\n",
    "    \n",
    "    feature_0[np.isinf(feature_0)] = 0\n",
    "    feature_1[np.isinf(feature_1)] = 0\n",
    "    behavior[np.isinf(behavior)] = 0\n",
    "    \n",
    "    # get the histogram\n",
    "    current_tc, x_edge, y_edge, bin_number = stat.binned_statistic_2d(feature_0, feature_1, behavior, statistic='mean', bins=bins)\n",
    "    \n",
    "    # plot and store\n",
    "    plot = hv.Image((x_edge, y_edge, np.array(current_tc)), kdims=[first_latent, second_latent])\n",
    "    plot.opts(tools=['hover'], cmap='Spectral')\n",
    "    plot_list.append(plot)\n",
    "\n",
    "hv.Layout(plot_list).cols(5)\n",
    "    \n",
    "    "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa398bb7",
   "metadata": {},
   "source": [
    "# generate a single map for all trials\n",
    "\n",
    "# define the target behavioral variable\n",
    "target_behavior = 'mouse_speed'\n",
    "# define the latents of interest\n",
    "first_latent = 'latent_0'\n",
    "second_latent = 'latent_1'\n",
    "\n",
    "# define the bins\n",
    "bins = 10\n",
    "# allocate memory for the accumulated trials\n",
    "feature_0_list = []\n",
    "feature_1_list = []\n",
    "behavior_list = []\n",
    "# # for all the trials\n",
    "# for trial in pre_data:\n",
    "# get the list of trials\n",
    "trial_list = np.unique(full_df['trial_idx'].to_numpy())\n",
    "# for all the trials\n",
    "for trial_idx in trial_list:\n",
    "    \n",
    "    trial = full_df.iloc[full_df['trial_idx'].to_numpy()==trial_idx, :]\n",
    "#     # get just the dataframe\n",
    "#     trial = trial[1]\n",
    "    # skip if the behavior is not there\n",
    "    if target_behavior not in trial.columns:\n",
    "        continue\n",
    "    # get the variables of interest\n",
    "    feature_0 = trial.loc[:, first_latent]\n",
    "    feature_1 = trial.loc[:, second_latent]\n",
    "    behavior = trial.loc[:, target_behavior]\n",
    "    # remove nans and infs\n",
    "    feature_0[np.isnan(feature_0)] = 0\n",
    "    feature_1[np.isnan(feature_1)] = 0\n",
    "    behavior[np.isnan(behavior)] = 0\n",
    "    \n",
    "    feature_0[np.isinf(feature_0)] = 0\n",
    "    feature_1[np.isinf(feature_1)] = 0\n",
    "    behavior[np.isinf(behavior)] = 0\n",
    "    \n",
    "    # store the variables in a list\n",
    "    feature_0_list.append(feature_0)\n",
    "    feature_1_list.append(feature_1)\n",
    "    behavior_list.append(behavior)\n",
    "    \n",
    "# concatenate\n",
    "feature_0 = pd.concat(feature_0_list, axis=0)\n",
    "feature_1 = pd.concat(feature_1_list, axis=0)\n",
    "behavior = pd.concat(behavior_list, axis=0)\n",
    "# get the histogram\n",
    "current_tc, x_edge, y_edge, bin_number = stat.binned_statistic_2d(feature_0, feature_1, behavior, statistic='mean', bins=bins)\n",
    "\n",
    "# plot and store\n",
    "plot = hv.Image((x_edge, y_edge, np.array(current_tc)), kdims=[first_latent, second_latent])\n",
    "plot.opts(tools=['hover'], cmap='Spectral')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15c3dd6",
   "metadata": {},
   "source": [
    "# function to separate the day from the time in the datetime field\n",
    "def separate_day_time(input_string):\n",
    "    split_string = input_string.split(' ')\n",
    "    day = split_string[0]\n",
    "#     time = split_string[1]\n",
    "    return day"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c70fc8",
   "metadata": {},
   "source": [
    "# generate averages of each latent over time\n",
    "\n",
    "# get a list of the latents\n",
    "latent_list = [el for el in full_df.columns if 'latent' in el]\n",
    "\n",
    "# allocate a list for the latent plots\n",
    "latent_plots = []\n",
    "# for all the latents\n",
    "for latent in latent_list:\n",
    "    # allocate a list for the overlay across mice\n",
    "    overlay_list = []\n",
    "    # run through all the mice\n",
    "    for mouse_name, mouse_data in full_df.groupby(['mouse']):\n",
    "\n",
    "        # transform the date so it starts at 0\n",
    "\n",
    "        # get the latent variable along with the date\n",
    "        current_variable = mouse_data.loc[:, [latent,'datetime']]\n",
    "        # convert the datetime into just date\n",
    "        current_variable['datetime'] = [el[:10] for el in current_variable['datetime']]\n",
    "        # group by the date and average\n",
    "        current_mean = current_variable.groupby(['datetime'], as_index=False).mean()\n",
    "        current_sem = current_variable.groupby(['datetime'], as_index=False).sem()\n",
    "        # get the dates\n",
    "        day_data = current_mean['datetime'].to_numpy()\n",
    "        # reformat day as a delta\n",
    "        # convert to datetime first\n",
    "        day_data = [datetime.datetime.strptime(el, '%Y-%m-%d') for el in day_data]\n",
    "        # calculate the deltas\n",
    "        delta_days = [(el-day_data[0]) for el in day_data]\n",
    "        delta_days = [el.days for el in delta_days]\n",
    "        # update the datetime field\n",
    "        current_mean['datetime'] = delta_days\n",
    "        current_sem['datetime'] = delta_days\n",
    "        \n",
    "        # filter out anything above the 10th day\n",
    "        selection_vector = current_mean['datetime'].to_numpy() < 11\n",
    "        current_mean = current_mean.iloc[selection_vector, :]\n",
    "        current_sem = current_sem.iloc[selection_vector, :]\n",
    "        \n",
    "        # zero the mean\n",
    "        current_mean[latent] -= current_mean.iloc[current_mean['datetime'].to_numpy() == 0, 1].to_numpy()\n",
    "\n",
    "#         # get the latent data\n",
    "#         y_mean = current_mean.loc[:, latent]\n",
    "#         y_sem = current_sem.loc[:, latent]\n",
    "#         # generate the plots\n",
    "#         curve = hv.Curve(current_mean, vdims=[latent], kdims=['datetime'])\n",
    "#         curve.opts(width=600, xrotation=45, tools=['hover'])\n",
    "#         spread = hv.Spread((current_mean['datetime'], current_mean[latent], current_sem[latent]), vdims=[latent, 'Error'], kdims=['datetime'])\n",
    "#         # store as overlay\n",
    "#         overlay_list.append(curve*spread)\n",
    "#     # store in the layout list\n",
    "#     latent_plots.append(hv.Overlay(overlay_list))\n",
    "    \n",
    "        # store as dataframe\n",
    "        overlay_list.append(current_mean)\n",
    "    # generate the accumulated dataframe\n",
    "    plot_df = pd.concat(overlay_list, axis=0)\n",
    "    # select data only for the first 10 days\n",
    "    selection_vector = plot_df['datetime'].to_numpy() < 11\n",
    "    plot_df = plot_df.iloc[selection_vector, :]\n",
    "    # plot\n",
    "    mouse_mean = plot_df.groupby(['datetime'], as_index=False).mean()\n",
    "    mouse_error = plot_df.groupby(['datetime'], as_index=False).std()\n",
    "    \n",
    "    curve = hv.Curve(mouse_mean, vdims=[latent], kdims=['datetime'])\n",
    "    curve.opts(width=600, xrotation=45, tools=['hover'], xlabel='Day')\n",
    "    spread = hv.Spread((mouse_mean['datetime'], mouse_mean[latent], mouse_error[latent]), vdims=[latent, 'Error'], kdims=['datetime'])\n",
    "    latent_plots.append(curve*spread)\n",
    "\n",
    "# turn into a layout and plot\n",
    "latent_layout = hv.Layout(latent_plots).cols(3)\n",
    "latent_layout\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a34b24",
   "metadata": {},
   "source": [
    "# group by date and generate the TC\n",
    "\n",
    "# define the target behavioral variable\n",
    "target_behavior = 'cricket_0_mouse_distance'\n",
    "# define the latents of interest\n",
    "first_latent = 'latent_0'\n",
    "second_latent = 'latent_1'\n",
    "\n",
    "# get a list of dates\n",
    "date_list = np.unique(full_df['datetime'])\n",
    "# allocate a plot list\n",
    "plot_list = []\n",
    "\n",
    "# for all the dates\n",
    "for date in date_list:\n",
    "    # get the data corresponding to this date\n",
    "    data_idx = np.argwhere(full_df.loc[:, 'datetime'].to_numpy() == date).flatten()\n",
    "    current_data = full_df.iloc[data_idx, :]\n",
    "    # get the variables of interest\n",
    "    feature_0 = current_data.loc[:, first_latent]\n",
    "    feature_1 = current_data.loc[:, second_latent]\n",
    "    behavior = current_data.loc[:, target_behavior]\n",
    "    # remove nans and infs\n",
    "    feature_0.loc[np.isnan(feature_0)] = 0\n",
    "    feature_1.loc[np.isnan(feature_1)] = 0\n",
    "    behavior.loc[np.isnan(behavior)] = 0\n",
    "    \n",
    "    feature_0.loc[np.isinf(feature_0)] = 0\n",
    "    feature_1.loc[np.isinf(feature_1)] = 0\n",
    "    behavior.loc[np.isinf(behavior)] = 0\n",
    "    \n",
    "    # generate the TC\n",
    "    current_tc, x_edge, y_edge, bin_number = stat.binned_statistic_2d(feature_0, feature_1, behavior, statistic='mean', bins=bins)\n",
    "\n",
    "    # plot and store\n",
    "    im = hv.Image((x_edge, y_edge, np.array(current_tc)), kdims=[first_latent, second_latent])\n",
    "    im.opts(tools=['hover'], cmap='Spectral', title=date)\n",
    "    plot_list.append(im)\n",
    "\n",
    "hv.Layout(plot_list).cols(5)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a906d7ef",
   "metadata": {},
   "source": [
    "%%time\n",
    "# UMAP embedding of the VAME data\n",
    "\n",
    "# compile the data\n",
    "# compiled_latent = np.vstack(latent_list)\n",
    "compiled_latent = full_df.loc[:, [el for el in full_df.columns if 'latent' in el]].to_numpy()\n",
    "\n",
    "# print(np.sum(compiled_latent, axis=0))\n",
    "# raise ValueError\n",
    "# embed using UMAP\n",
    "# original parameters 0.5 and 10\n",
    "# 0.1 and 30 also works\n",
    "# 0.05 and 30 works too\n",
    "reducer = umap.UMAP(min_dist=0.1, n_neighbors=20)\n",
    "embedded_data = reducer.fit_transform(compiled_latent[:, :])\n",
    "\n",
    "# # save the embedding\n",
    "# np.save(os.path.join(target_folder, 'UMAP_result'), embedded_data)\n",
    "\n",
    "# # generate the model name\n",
    "# model_name = os.path.join(target_folder, 'UMAP_model.pk')\n",
    "# # save the estimator\n",
    "# with open(model_name, 'wb') as file:\n",
    "#     pk.dump(reducer, file)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ca540c",
   "metadata": {},
   "source": [
    "# plot the UMAP results\n",
    "\n",
    "# get the labels\n",
    "# compiled_labels = np.expand_dims(np.hstack(distance_list), axis=1)\n",
    "compiled_labels = np.expand_dims(full_df.loc[:, 'cricket_0_delta_heading'].to_numpy().copy(), axis=1)\n",
    "# need to threshold, for some reason there's some weird distances\n",
    "# compiled_labels[compiled_labels>50] = 50\n",
    "# compiled_labels[compiled_labels<0] = 0\n",
    "compiled_labels = medfilt(compiled_labels, kernel_size=[21, 1])\n",
    "\n",
    "# define the sampling ratio\n",
    "sampling_ratio = 10\n",
    "\n",
    "umap_data = np.concatenate((embedded_data[::sampling_ratio, :],compiled_labels[::sampling_ratio, :]), axis=1)\n",
    "\n",
    "print(umap_data.shape)\n",
    "                            \n",
    "                            \n",
    "umap_plot = hv.Scatter(umap_data, vdims=['Dim 2','parameter'], kdims=['Dim 1'])\n",
    "# umap_plot = hv.HexTiles(umap_data, kdims=['Dim 1', 'Dim 2'])\n",
    "umap_plot.opts(color='parameter', colorbar=True, cmap='Spectral', tools=['hover'], alpha=0.5)\n",
    "umap_plot.opts(width=1200, height=1000, size=5)\n",
    "# umap_plot.opts(width=1200, height=1000)\n",
    "umap_plot\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1d51e2",
   "metadata": {},
   "source": [
    "# add a trajectory on top of the map\n",
    "\n",
    "# define the target trial\n",
    "target_trial = 30\n",
    "\n",
    "traj_data = embedded_data[full_df.loc[:, 'trial_idx'] == target_trial, :]\n",
    "trajectory = hv.Curve((traj_data[:, 0], traj_data[:, 1]))\n",
    "start = hv.Scatter((traj_data[0, 0], traj_data[0, 1])).opts(color='Red', size=10)\n",
    "end = hv.Scatter((traj_data[-1, 0], traj_data[-1, 1])).opts(color='Blue', size=10)\n",
    "\n",
    "umap_plot*trajectory*start*end"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31609dc4",
   "metadata": {},
   "source": [
    "# plot aggregated trajectories in UMAP space\n",
    "\n",
    "# allocate a list for the trials\n",
    "trial_plots = []\n",
    "# get the number of trials\n",
    "trial_list = np.unique(full_df.loc[:, 'trial_idx'])\n",
    "# for all the trials\n",
    "for trial in trial_list[::20]:\n",
    "    # get the corresponding coordinates\n",
    "    traj_data = embedded_data[full_df.loc[:, 'trial_idx'] == trial, :]\n",
    "    \n",
    "    # generate the curve\n",
    "    curve = hv.Curve((traj_data[::10, 0], traj_data[::10, 1]))\n",
    "    curve.opts(width=1200, height=1000, alpha=0.2)\n",
    "    start = hv.Scatter((traj_data[0, 0], traj_data[0, 1])).opts(color='Red', size=10)\n",
    "    start.opts(width=1200, height=1000)\n",
    "    end = hv.Scatter((traj_data[-1, 0], traj_data[-1, 1])).opts(color='Blue', size=10)\n",
    "    end.opts(width=1200, height=1000)\n",
    "    \n",
    "#     trial_plots.append(start)\n",
    "#     trial_plots.append(end)\n",
    "    \n",
    "    # store\n",
    "    trial_plots.append(curve*start*end)\n",
    "    \n",
    "# generate the figure\n",
    "overlay = umap_plot*hv.Overlay(trial_plots[:])\n",
    "overlay.opts({'Curve': dict(color=hv.Palette('Spectral'))})\n",
    "overlay"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54699a60",
   "metadata": {},
   "source": [
    "print(latent_list)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26788081",
   "metadata": {},
   "source": [
    "# calculate transition matrices\n",
    "print(full_df.columns, full_df.shape)\n",
    "\n",
    "# for all the mice and days\n",
    "for (mouse, day), data in full_df.groupby(['mouse', 'datetime']):\n",
    "    print(mouse)\n",
    "    # define the number of frames back to look\n",
    "    back_frames = 5\n",
    "    # get the number of motif\n",
    "    motif_number = latent_list[0].shape[1]\n",
    "    # allocate memory for the matrices\n",
    "    transition_matrices = np.zeros((back_frames, len(label_list), motif_number, motif_number))\n",
    "    # for all the label files\n",
    "    for idx, files in enumerate(label_list):\n",
    "        # remove the consecutive repeats\n",
    "        clean_label = [iterator[0] for iterator in it.groupby(files)]\n",
    "\n",
    "        # for all the frames\n",
    "        for idx_frame, frames in enumerate(clean_label):\n",
    "            # get the labels (coordinates)\n",
    "            x = motif_revsort[frames]\n",
    "            # for all the back frames\n",
    "            for bframes in np.arange(back_frames):\n",
    "                # skip if not further enough yet\n",
    "                if (idx_frame - bframes) < 0:\n",
    "                    continue\n",
    "                y = motif_revsort[clean_label[idx_frame - bframes - 1]]\n",
    "                transition_matrices[bframes, idx, x, y] += 1"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc324be",
   "metadata": {},
   "source": [
    "# plot the clustering result\n",
    "\n",
    "def plot_dendrogram(model, **kwargs):\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack(\n",
    "        [model.children_, model.distances_, counts]\n",
    "    ).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)\n",
    "\n",
    "plot_dendrogram(cluster_element, truncate_mode=\"level\", p=3)\n",
    "\n"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-prey_capture] *",
   "language": "python",
   "name": "conda-env-.conda-prey_capture-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
