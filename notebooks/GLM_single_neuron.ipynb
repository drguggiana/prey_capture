{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81436619",
   "metadata": {},
   "source": [
    "# imports\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath(r'D:\\Code Repos\\prey_capture'))\n",
    "\n",
    "\n",
    "import panel as pn\n",
    "import holoviews as hv\n",
    "from holoviews import opts, dim\n",
    "from holoviews.operation import histogram\n",
    "hv.extension('bokeh')\n",
    "from bokeh.resources import INLINE\n",
    "\n",
    "import paths\n",
    "import functions_bondjango as bd\n",
    "import functions_misc as fm\n",
    "import functions_plotting as fp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.mixture as mix\n",
    "import sklearn.decomposition as decomp\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_predict, train_test_split\n",
    "from sklearn import svm, datasets\n",
    "from sklearn import preprocessing\n",
    "import sklearn.linear_model as lin\n",
    "import sklearn.metrics as smet\n",
    "import scipy.signal as ss\n",
    "import scipy.stats as stat\n",
    "import scipy.optimize as opt\n",
    "\n",
    "import random\n",
    "# import functions_data_handling as fd\n",
    "# import functions_vame as fv\n",
    "import importlib\n",
    "import processing_parameters\n",
    "# import PSID\n",
    "# from PSID.evaluation import evalPrediction\n",
    "# import sklearn.cross_decomposition as cros\n",
    "# import umap\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbad38a2",
   "metadata": {},
   "source": [
    "def maxmin(array_in):\n",
    "    return (array_in-np.nanmin(array_in))/(np.nanmax(array_in)-np.nanmin(array_in))\n",
    "\n",
    "def basis_predictors(variable, basis_number, kernel, kernel_spacing, total_length,label):\n",
    "    # initialize the output dataframe\n",
    "    out_frame = pd.DataFrame()\n",
    "    # generate the displaced basis functions\n",
    "    for idx2 in np.arange(basis_number):\n",
    "        # generate the sizes of the before and after padding of the kernel\n",
    "        back = int(kernel_spacing*idx2)\n",
    "        front = int(total_length-kernel.shape[0]-back)\n",
    "        # generate the full kernel\n",
    "        if back == 0:\n",
    "            current_kernel = np.concatenate((kernel, np.zeros(front)))\n",
    "        elif idx2 == basis_number-1:\n",
    "            current_kernel = np.concatenate((np.zeros(back), kernel))\n",
    "        else:\n",
    "            current_kernel = np.concatenate((np.zeros(back), kernel, np.zeros(front)))\n",
    "\n",
    "        # convolve with the data\n",
    "        vector = np.convolve(variable, current_kernel, 'same')\n",
    "        # normalize to 0-1\n",
    "        vector = maxmin(vector)\n",
    "        # if the vector was all zeros, it'll turn into nans so remove\n",
    "        vector[np.isnan(vector)] = 0\n",
    "\n",
    "        # generate the field in the new data frame\n",
    "        out_frame[label+'_'+str(idx2)] = vector\n",
    "        \n",
    "    return out_frame"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e585e34",
   "metadata": {},
   "source": [
    "%%time\n",
    "# Load the desired files\n",
    "importlib.reload(processing_parameters)\n",
    "\n",
    "# define the threshold for matched cells\n",
    "match_threshold = 10\n",
    "# get the data paths\n",
    "try: \n",
    "    data_path = snakemake.input[0]\n",
    "except NameError:\n",
    "    # get the search list\n",
    "    search_list = processing_parameters.search_list\n",
    "    # allocate memory for the data\n",
    "    pre_normal_data = []\n",
    "    pre_mod_data = []\n",
    "    \n",
    "    # allocate a list for all paths (need to preload to get the dates)\n",
    "    all_paths = []\n",
    "    # for all the search strings\n",
    "    for search_string in search_list:\n",
    "\n",
    "        # query the database for data to plot\n",
    "        data_all = bd.query_database('analyzed_data', search_string)\n",
    "        data_all = [el for el in data_all if 'preproc' in el['slug']]\n",
    "        data_path = [el['analysis_path'] for el in data_all if '_preproc' in el['slug']]\n",
    "        all_paths.append(data_path)\n",
    "    # get the dates present\n",
    "    data_dates = np.unique([os.path.basename(el)[:10] for el in np.concatenate(all_paths)])\n",
    "    print(f'Dates present: {data_dates}')\n",
    "    # now load the files\n",
    "    for data_path in all_paths:\n",
    "        # load the calcium data\n",
    "        beh_data = []\n",
    "        # for all the files\n",
    "        for files in data_path:\n",
    "            # load the data\n",
    "            with pd.HDFStore(files) as h:\n",
    "                beh_data.append(h['full_traces'])\n",
    "                if '/matched_calcium' in h.keys():\n",
    "                    # get the cell matches\n",
    "                    cell_matches = h['cell_matches']\n",
    "                    \n",
    "                    # perform only if there are more files\n",
    "                    if len(data_dates) > 1:\n",
    "#                         print('Successful match')\n",
    "                        match_dates = [el for el in data_dates if el in cell_matches.columns]\n",
    "                        # get only the days present in the search\n",
    "                        cell_matches = cell_matches[match_dates]\n",
    "                        # generate a list with the number of days and the number of cells kept\n",
    "\n",
    "                        # get the unique cell combinations\n",
    "                        # unique contains the unique patterns followed by cells across days\n",
    "                        # inverse indicates which pattern is followed by each cell\n",
    "                        # count contains the number of times each pattern is found\n",
    "                        unique, inverse, counts = np.unique(~np.isnan(cell_matches.to_numpy()), axis=0, \n",
    "                                        return_counts=True, return_inverse=True)\n",
    "                        # remove the single day and no day cases\n",
    "                        counts[np.sum(unique, axis=1)==0] = 0\n",
    "                        counts[np.sum(unique, axis=1)==1] = 0\n",
    "\n",
    "                        # get an index vector with only the most popular pattern\n",
    "                        # (regardless of how many cells share it)\n",
    "                        cell_idx = np.array(inverse==np.argmax(counts))\n",
    "\n",
    "                        cell_matches = cell_matches.iloc[cell_idx, :]\n",
    "                    else:\n",
    "                        counts = 1\n",
    "                        cell_idx = cell_matches[data_dates].to_numpy()\n",
    "                        cell_idx = ~np.isnan(cell_idx)\n",
    "                        cell_matches = cell_matches.iloc[cell_idx, :]\n",
    "                        unique = np.array([[1]])\n",
    "                    # concatenate the latents\n",
    "                    dataframe = pd.concat([h['matched_calcium'], h['latents']], axis=1)\n",
    "\n",
    "                    # separate based on normal vs mod\n",
    "                    if 'dark' in files:\n",
    "                        pre_mod_data.append((files, dataframe, cell_matches))\n",
    "                    else:\n",
    "                        pre_normal_data.append((files, dataframe,  cell_matches))\n",
    "                    \n",
    "print(f'Number of matched cells: {np.sum(cell_idx)}')\n",
    "print(f'Number of matched trials: {unique[np.argmax(counts)].sum()}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e291781",
   "metadata": {},
   "source": [
    "# Leave only common cells across all datasets\n",
    "\n",
    "# allocate memory for the cleaned up data\n",
    "normal_data = []\n",
    "mod_data = []\n",
    "\n",
    "# print(pre_normal_data[0][2])\n",
    "# for all the normal trials\n",
    "for idx, el in enumerate(pre_normal_data):\n",
    "    # get the date\n",
    "    current_date = os.path.basename(el[0])[:10]\n",
    "    # get the corresponding indexes\n",
    "    current_idx = el[2][current_date].to_numpy()\n",
    "    # if they're all nans, skip the day\n",
    "    if np.isnan(np.sum(current_idx)):\n",
    "        continue\n",
    "    # get the current df\n",
    "    current_df = el[1]\n",
    "    labels = list(current_df.columns)\n",
    "    cells = [el for el in labels if 'cell' in el]\n",
    "    not_cells = [el for el in labels if 'cell' not in el]\n",
    "    # get the non-cell data\n",
    "    non_cell_data = current_df[not_cells]\n",
    "    # get the current calcium data\n",
    "    cell_data = current_df[cells]\n",
    "    # remove the non matched cells\n",
    "    cell_data = cell_data.iloc[:, current_idx]\n",
    "    # rename the cell fields\n",
    "    cell_names = ['cell_' + str(el) for el in np.arange(cell_data.shape[1])]\n",
    "    cell_data.columns = cell_names\n",
    "    # normalize the single trial activity\n",
    "#     cell_data = (cell_data-cell_data.mean())/cell_data.std()\n",
    "#     cell_data = cell_data/cell_data.std()\n",
    "#     cell_data = (cell_data-cell_data.min())/(cell_data.max()-cell_data.min())\n",
    "\n",
    "    # calculate a baseline for all cells\n",
    "    for name, single in cell_data.items():\n",
    "        # skip if there are only zeros\n",
    "        if np.sum(single) == 0:\n",
    "            continue\n",
    "        # get the baseline\n",
    "        baseline = np.percentile(single[single>0], 8)\n",
    "        # get the dF/F\n",
    "#         single = (single-baseline)/baseline\n",
    "        # clip the trace\n",
    "        single[single<baseline] = 0\n",
    "        # store\n",
    "        cell_data[name] = single\n",
    "\n",
    "    # remove the nans after normalization\n",
    "    cell_data[np.isnan(cell_data)] = 0\n",
    "    # assemble a new data frame with only the matched cells and the rest of the data\n",
    "    normal_data.append(pd.concat((non_cell_data, cell_data), axis=1))\n",
    "    \n",
    "print(normal_data[0].shape)\n",
    "print(normal_data[0].columns)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dd2470",
   "metadata": {},
   "source": [
    "# set up the feature and calcium matrices\n",
    "# list the radial features in the dataset\n",
    "radial_features = ['cricket_0_delta_heading', 'cricket_0_visual_angle', 'mouse_heading', \n",
    "                   'cricket_0_delta_head', 'cricket_0_heading', 'head_direction']\n",
    "# define the design matrix\n",
    "feature_list = ['mouse_speed', 'cricket_0_speed', 'mouse_x', 'mouse_y', 'cricket_0_x', 'cricket_0_y',\n",
    "                'cricket_0_delta_heading', 'cricket_0_mouse_distance', 'cricket_0_visual_angle',\n",
    "               'mouse_heading', 'cricket_0_delta_head', 'cricket_0_heading', 'head_direction',\n",
    "               'latent_0', 'latent_1', 'latent_2', 'latent_3', 'latent_4',\n",
    "               'latent_5', 'latent_6', 'latent_7', 'latent_8', 'latent_9']\n",
    "# feature_list = ['mouse_speed']\n",
    "\n",
    "# define the frame rate (fps)\n",
    "frame_rate = 10\n",
    "# define the width of the kernel (s), multiplied to convert to frames\n",
    "sigma = 1*frame_rate\n",
    "# calculate the kernel\n",
    "kernel = ss.gaussian(sigma*5, sigma)\n",
    "# define the number of basis functions per regressor\n",
    "basis_number = 9\n",
    "# define the kernel spacing (in s)\n",
    "kernel_spacing = 0.2*frame_rate\n",
    "# get the total length of the kernel\n",
    "total_length = kernel_spacing*(basis_number-1) + kernel.shape[0]\n",
    "# # get the start positions of the basis functions (assume sigma defines the interval)\n",
    "# basis_starts = [int(el) for el in np.arange(-sigma*((basis_number-1)/2), \n",
    "#                                        sigma*((basis_number-1)/2)+1, sigma)]\n",
    "# allocate memory for the output\n",
    "feature_trials = []\n",
    "# allocate memory for a data frame without the encoding model features\n",
    "feature_raw_trials = []\n",
    "# allocate memory for the calcium\n",
    "calcium_trials = []\n",
    "# get the number of trials\n",
    "trial_number = len(normal_data)\n",
    "# get the features\n",
    "for idx, el in enumerate(normal_data):\n",
    "    # get the intersection of the labels\n",
    "    label_intersect = [feat for feat in feature_list if feat in el.columns]\n",
    "    \n",
    "    if len(label_intersect) != len(feature_list):\n",
    "        continue\n",
    "    # get the features of interest\n",
    "    target_features = el.loc[:, feature_list]\n",
    "    # save the original features for simpler calculations\n",
    "    feature_raw_trials.append(target_features.copy())\n",
    "    # get the original columns\n",
    "    original_columns = target_features.columns\n",
    "    \n",
    "    # turn the radial variables into linear ones\n",
    "    # for all the columns\n",
    "    for label in original_columns:\n",
    "        # calculate head speed\n",
    "        if label == 'head_direction':\n",
    "            # get the head direction\n",
    "            head = target_features[label].copy().to_numpy()\n",
    "            # get the angular speed and acceleration of the head\n",
    "            speed = np.concatenate(([0], np.diff(ss.medfilt(head, 21))), axis=0)\n",
    "            acceleration = np.concatenate(([0], np.diff(head)), axis=0)\n",
    "            # add to the features\n",
    "            target_features['head_speed'] = speed\n",
    "            target_features['head_acceleration'] = acceleration\n",
    "        # check if the feature is radial\n",
    "        if label in radial_features:\n",
    "            # get the feature\n",
    "            rad_feature = target_features[label].copy().to_numpy()\n",
    "            # convert to radians\n",
    "            rad_feature = np.deg2rad(rad_feature)\n",
    "            # perform angular decomposition (assume unit circle)\n",
    "            x = np.cos(rad_feature)\n",
    "            y = np.sin(rad_feature)\n",
    "            # replace the original column by the extracted ones\n",
    "            target_features[label+'_x'] = x\n",
    "            target_features[label+'_y'] = y\n",
    "            # drop the original column\n",
    "            target_features.drop(labels=label, axis=1, inplace=True)\n",
    "        # check if the label is a speed and calculate acceleration\n",
    "        if 'speed' in label:\n",
    "            # get the speed\n",
    "            speed = target_features[label].copy().to_numpy()\n",
    "            # calculate the acceleration with the smoothed speed\n",
    "            acceleration = np.concatenate(([0], np.diff(ss.medfilt(speed, 21))), axis=0)\n",
    "            # add to the features\n",
    "            target_features[label.replace('speed', 'acceleration')] = acceleration\n",
    "    \n",
    "    # Generate the gaussian convolved and displaced regressors\n",
    "    # allocate an empty dataframe for the outputs\n",
    "    new_dataframe = pd.DataFrame()\n",
    "    # for all the regressors\n",
    "    for label in target_features:\n",
    "        # get the variable\n",
    "        variable = target_features[label].to_numpy().copy()\n",
    "        # Remove nans\n",
    "        variable[np.isnan(variable)] = 0\n",
    "\n",
    "        # get the basis function-based predictors\n",
    "        out_frame = basis_predictors(variable, basis_number, kernel, kernel_spacing, total_length,label)\n",
    "        # add to the dataframe\n",
    "        new_dataframe = pd.concat((new_dataframe, out_frame), axis=1)\n",
    "\n",
    "    # add a constant factor\n",
    "    constant = np.ones(new_dataframe.shape[0])\n",
    "    new_dataframe['constant'] = constant\n",
    "    # add a trial factor\n",
    "#     new_dataframe['trial'] = idx*np.ones(vector.shape[0])\n",
    "#     # for all the trials\n",
    "#     for trial in np.arange(trial_number):\n",
    "#         new_dataframe['trial_'+str(trial)] = np.zeros(vector.shape[0])\n",
    "#         if trial == idx:\n",
    "#             new_dataframe['trial_'+str(trial)] += 1\n",
    "\n",
    "    # replace the old dataframe with the new one\n",
    "    target_features = new_dataframe\n",
    "        \n",
    "    # store the columns\n",
    "    resulting_columns = target_features.columns\n",
    "    # turn the dataframe into an array\n",
    "    target_features = target_features.to_numpy()\n",
    "\n",
    "    # store the array\n",
    "    feature_trials.append(target_features)\n",
    "    \n",
    "    # get the calcium data\n",
    "    cells = [cell for cell in el.columns if 'cell' in cell]\n",
    "    cells = el.loc[:, cells].to_numpy()\n",
    "\n",
    "    # store\n",
    "    calcium_trials.append(cells)\n",
    "    \n",
    "\n",
    "# # concatenate the data\n",
    "# feature_matrix = np.concatenate(feature_matrix, axis=0)\n",
    "# calcium_matrix = np.concatenate(calcium_matrix, axis=0)\n",
    "\n",
    "# # define the number of time points to accumulate\n",
    "# time_points = 1\n",
    "\n",
    "# if time_points > 1:\n",
    "#     # allocate memory for the matrix\n",
    "#     design_matrix = np.zeros((feature_matrix.shape[0], feature_matrix.shape[1]*time_points))\n",
    "#     # create the source matrix\n",
    "#     source_matrix = np.vstack((np.zeros((time_points, feature_matrix.shape[1])), feature_matrix))\n",
    "\n",
    "#     # for all the timepoints\n",
    "#     for times in np.arange(feature_matrix.shape[0]):\n",
    "#         design_matrix[times, :] = source_matrix[times:times+time_points, :].flatten()\n",
    "\n",
    "#     # replace the calcium data\n",
    "#     feature_matrix = design_matrix\n",
    "\n",
    "print(f'Time by features: {feature_trials[0].shape}')\n",
    "print(f'Time by ROIs: {calcium_trials[0].shape}')\n",
    "print(resulting_columns)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a82821",
   "metadata": {},
   "source": [
    "# plot one of the design matrices\n",
    "hv.Image(feature_trials[0].T).opts(width=800, height=600, tools=['hover'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28da2e43",
   "metadata": {},
   "source": [
    "# plot the calcium\n",
    "hv.Image(calcium_trials[0].T).opts(width=800, height=600, tools=['hover'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6bd93c",
   "metadata": {},
   "source": [
    "# plot a histogram of the calcium data\n",
    "\n",
    "freq, bins = np.histogram(calcium_matrix.flatten(), bins=100)\n",
    "\n",
    "hv.Bars((bins, freq)).opts(width=600).opts(xrotation=45, logy=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88217f0e",
   "metadata": {},
   "source": [
    "%%time\n",
    "# run the GLM\n",
    "\n",
    "# define the trials to run together\n",
    "trial_groups = [list(np.arange(len(feature_trials)))]\n",
    "# allocate memory for the performances\n",
    "train_list = []\n",
    "test_list = []\n",
    "# allocate memory for the predictions and the weights\n",
    "predictions = []\n",
    "weights = []\n",
    "# get the cell number\n",
    "cell_number = calcium_matrix.shape[1]\n",
    "\n",
    "# for all the trial groups\n",
    "for trial_idx in trial_groups:\n",
    "    # concatenate the trials \n",
    "    feature_matrix = np.concatenate([feature_trials[el] for el in trial_idx], axis=0)\n",
    "    calcium_matrix = np.concatenate([calcium_trials[el] for el in trial_idx], axis=0)\n",
    "    \n",
    "    # for all the cells\n",
    "    for idx, cell in enumerate(calcium_matrix.T):\n",
    "\n",
    "        # convolve the cell with the kernel used above\n",
    "        cell = np.convolve(cell, kernel, 'same')\n",
    "    #     cell = cell > 0\n",
    "\n",
    "        # copy the feature matrix for scaling\n",
    "        feature = feature_matrix.copy()\n",
    "#         # add coupled predictors\n",
    "#         # extract the activity of the other cells\n",
    "#         # get a vector of indexes\n",
    "#         cell_idx = [el for el in list(np.arange(cell_number)) if el != idx]\n",
    "#         other_calcium = calcium_matrix[:, cell_idx]\n",
    "#         # get the average activity\n",
    "#         other_average = np.expand_dims(np.mean(other_calcium, axis=1), axis=1)\n",
    "#         # get the NMF-reduced activity\n",
    "#         nmf = decomp.NMF(n_components=15)\n",
    "#         other_nmf = nmf.fit_transform(other_calcium)\n",
    "#         # generate the predictors\n",
    "#     #     ca_variables = np.concatenate((other_calcium, other_average), axis=1)\n",
    "#     #     ca_variables = other_nmf\n",
    "#         ca_variables = np.concatenate((other_nmf, other_average), axis=1)\n",
    "\n",
    "#         # initialize a dataframe for the predictors\n",
    "#         temp_frame = pd.DataFrame()\n",
    "#         # for all the cells\n",
    "#         for others in np.arange(ca_variables.shape[1]):\n",
    "#             # load the cell\n",
    "#             variable = ca_variables[:, others]\n",
    "#             # initialize a random label (won't use them)\n",
    "#             label = str(others)\n",
    "#             # get the predictors\n",
    "#             out_frame = basis_predictors(variable, 2, kernel, kernel_spacing, total_length, label)\n",
    "#             # put in the new frame\n",
    "#             temp_frame = pd.concat((temp_frame, out_frame), axis=1)\n",
    "#         # add to the feature matrix\n",
    "#         feature = np.concatenate((feature, temp_frame.to_numpy()), axis=1)    \n",
    "\n",
    "    #     feature = np.exp(feature)\n",
    "    #     feature = np.log10(feature/(1-feature))\n",
    "    #     feature[np.isinf(feature)] = 0\n",
    "        # split the data\n",
    "        feature_train, feature_test, cell_train, cell_test = \\\n",
    "            train_test_split(feature, cell, test_size=0.3, shuffle=False)\n",
    "        # generate the scaler using only the train set\n",
    "    #     scaler = preprocessing.StandardScaler().fit(feature_train)\n",
    "        scaler = preprocessing.MaxAbsScaler().fit(feature_train)\n",
    "    #     scaler = preprocessing.RobustScaler().fit(feature_train)\n",
    "    #     scaler = preprocessing.MinMaxScaler().fit(feature_train)\n",
    "        # scale the features\n",
    "        feature_train = scaler.transform(feature_train)\n",
    "        feature_test = scaler.transform(feature_test)\n",
    "        feature_all = scaler.transform(feature)\n",
    "\n",
    "        # initialize the regressor\n",
    "    #     linear = lin.ElasticNetCV(max_iter=5000, l1_ratio=[.1, .5, .7, .9, .95, .99, 1], \n",
    "    #                                        n_jobs=7, alphas=[.001, .01, .1, 1, 10, 100])\n",
    "    #                                        n_jobs=7, alphas=[.001])\n",
    "        linear = lin.TweedieRegressor(alpha=.01, max_iter=5000, fit_intercept=False, power=1)\n",
    "    #     linear = lin.LogisticRegression(C=.001, penalty='l2', max_iter=5000, \n",
    "    #                                     solver='lbfgs', class_weight='balanced')\n",
    "    #     linear = lin.ElasticNet(alpha=0.01, positive=True, fit_intercept=False)\n",
    "\n",
    "        # train the classifier\n",
    "        linear.fit(feature_train, cell_train)\n",
    "        # predict train and test\n",
    "        linear_pred = linear.predict(feature_train)\n",
    "        linear_pred_last = linear.predict(feature_test)\n",
    "        # save the predictions\n",
    "        predictions.append(linear.predict(feature_all))\n",
    "        # save the weights\n",
    "        weights.append(linear.coef_)\n",
    "\n",
    "    #     # try the NNLS from scipy\n",
    "    #     linear, rnorm = opt.nnls(feature_train, cell_train, maxiter=5000)\n",
    "    #     print(rnorm)\n",
    "    #     linear_pred = feature_train@linear\n",
    "    #     linear_pred_last = feature_test@linear\n",
    "    #     predictions.append(feature_matrix@linear)\n",
    "\n",
    "        # save the performances\n",
    "        try:\n",
    "            train_perf = smet.r2_score(cell_train, linear_pred)\n",
    "            test_perf = smet.r2_score(cell_test, linear_pred_last)\n",
    "\n",
    "        except TypeError:\n",
    "            train_perf = smet.accuracy_score(cell_train, linear_pred)\n",
    "            test_perf = smet.accuracy_score(cell_test, linear_pred_last)\n",
    "\n",
    "        train_list.append(train_perf)\n",
    "        test_list.append(test_perf)\n",
    "    # report the number of positive test fits\n",
    "    print(f'Positive cell fits: {np.sum([1 for el in test_list if el > 0])}')\n",
    "    # generate histograms with the performances\n",
    "    train_hist, train_edges = np.histogram(train_list)\n",
    "    test_hist, test_edges = np.histogram(test_list, bins=50)\n",
    "\n",
    "    train_h = hv.Bars((train_edges, train_hist)).opts(xrotation=45, width=800)\n",
    "    test_h = hv.Bars((test_edges, test_hist)).opts(xrotation=45, width=800)\n",
    "    # also a scatter with train and test for every trial\n",
    "    combined = hv.Scatter((train_list, test_list)).opts(xrotation=45, ylim=(-1, 1))\n",
    "\n",
    "    (train_h+test_h+combined).opts(width=800, shared_axes=False).cols(1)\n",
    "\n",
    "    "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e21a8f4",
   "metadata": {},
   "source": [
    "# plot the fits\n",
    "\n",
    "# define the target cells\n",
    "# target_cells = [0, 1, 2, 3, 4]\n",
    "# target_cells = np.argsort([el1*el2 for el1, el2 in zip(test_list, train_list)])[-5:]\n",
    "# target_cells = np.argsort(train_list)[-5:]\n",
    "target_cells = np.argsort(test_list)[-5:]\n",
    "\n",
    "# allocate memory for the plots\n",
    "real_list = []\n",
    "pred_list = []\n",
    "# for all the target cells\n",
    "for cell in target_cells[::-1]:\n",
    "    # get the cell trace\n",
    "    cell_trace = calcium_matrix[:, cell]\n",
    "#     percentile = np.percentile(cell_trace, 30)\n",
    "#     cell_trace[cell_trace<percentile] = 0\n",
    "    cell_trace = np.convolve(cell_trace, kernel, 'same')\n",
    "#     cell_trace = cell_trace > 0\n",
    "    # get the corresponding prediction\n",
    "    prediction_trace = predictions[cell]\n",
    "#     prediction_trace = np.convolve(prediction_trace, kernel, 'same')\n",
    "#     prediction_trace = maxmin(prediction_trace)\n",
    "    \n",
    "    # plot them\n",
    "    real_trace = hv.Curve(cell_trace, kdims=['Time'], vdims=['Activity'],\n",
    "                          ).opts(width=800, tools=['hover'])\n",
    "    pred_trace = hv.Curve(prediction_trace).opts(width=800)\n",
    "#     feature_trace = hv.Curve(feature_all[:, 0]).opts(width=800)\n",
    "    real_list.append(real_trace*pred_trace)\n",
    "#     pred_list.append(pred_trace)\n",
    "\n",
    "# generate the layout\n",
    "# hv.Layout(real_list)*hv.Layout(pred_list)\n",
    "hv.Layout(real_list).opts(shared_axes=False).cols(1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da045a6",
   "metadata": {},
   "source": [
    "# Visualize weights\n",
    "\n",
    "# turn the weights into an array\n",
    "weight_array = np.vstack(weights)\n",
    "roi_number = weight_array.shape[0]\n",
    "\n",
    "print(weight_array.shape)\n",
    "weight_array = weight_array[:, :-1].reshape((roi_number, -1, basis_number))\n",
    "\n",
    "print(weight_array.shape)\n",
    "\n",
    "# allocate memory for a plot list\n",
    "weight_list = []\n",
    "\n",
    "# for all cells\n",
    "for cells in np.arange(weight_array.shape[0]):\n",
    "    im = hv.Image((np.squeeze(weight_array[cells, :, :]))).opts(width=250, tools=['hover'])\n",
    "    weight_list.append(im)\n",
    "    \n",
    "hv.Layout(weight_list).opts(shared_axes=False)\n",
    "# # condense the regressors from each parameter into one\n",
    "# weight_array = np.array([weight_array[:, el:el+basis_number].sum(axis=1) \n",
    "#                          for el in np.arange(int(weight_array.shape[1]/basis_number))])\n",
    "\n",
    "# print(' '.join(('Regressors by cells:', str(weight_array.shape))))\n",
    "# # visualize\n",
    "# hv.Image(weight_array).opts(width=800, height=800)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a36df37",
   "metadata": {},
   "source": [
    "# Calculate the per cell rev correlation\n",
    "\n",
    "# define the interval for calculation in frames\n",
    "target_interval = 10\n",
    "# define the target behavioral variable\n",
    "# target_behavior = 'cricket_0_mouse_distance'\n",
    "# target_behavior = ['mouse_x', 'mouse_y', 'mouse_head_x', 'mouse_head_y', \n",
    "#                    'mouse_body2_x', 'mouse_body2_y', 'mouse_body3_x', 'mouse_body3_y', \n",
    "#                    'mouse_base_x', 'mouse_base_y', 'mouse_speed',\n",
    "#                    'cricket_0_x', 'cricket_0_y','cricket_0_mouse_distance',\n",
    "#                    'cricket_0_delta_heading', 'cricket_0_visual_angle']\n",
    "\n",
    "# get the behavioral data\n",
    "# current_behavior = np.expand_dims(data.loc[:, target_behavior].to_numpy(), axis=1)\n",
    "# current_behavior = data.loc[:, target_behavior].to_numpy()\n",
    "beh_columns = len(resulting_columns)\n",
    "current_behavior = feature_matrix.copy()\n",
    "# # concatenate all the data\n",
    "# data = pd.concat(normal_data, axis=0)\n",
    "\n",
    "# # get the available columns\n",
    "# labels = list(data.columns)\n",
    "# cells = [el for el in labels if 'cell' in el]\n",
    "# # get the cell data\n",
    "# calcium_data = np.array(data[cells].copy()).T\n",
    "# # scale for min and max\n",
    "# calcium_data = (calcium_data - np.nanmin(calcium_data))/\\\n",
    "#                (np.nanmax(calcium_data) - np.nanmin(calcium_data))\n",
    "\n",
    "# calcium_data = calcium_matrix.T.copy()\n",
    "# # get rid of the nan values\n",
    "# calcium_data[np.isnan(calcium_data)] = 0\n",
    "# get the number of time points\n",
    "time_number = calcium_matrix.shape[0]\n",
    "# get the number of cells\n",
    "roi_number = calcium_matrix.shape[1]\n",
    "\n",
    "print(f'Number of features: {beh_columns}')\n",
    "print(f'Number of ROIs: {roi_number}')\n",
    "print(f'Number of time points: {time_number}')\n",
    "\n",
    "# pad the behavior data\n",
    "# padded_behavior = np.vstack((np.zeros((target_interval, 1)), current_behavior)).T\n",
    "\n",
    "padded_behavior = np.vstack((np.zeros((target_interval, beh_columns)), current_behavior, \n",
    "                             np.zeros((target_interval, beh_columns)))).T\n",
    "# remove nans (heads up, there are a lot)\n",
    "padded_behavior[np.isnan(padded_behavior)] = 0\n",
    "\n",
    "\n",
    "# allocate memory to save the ols\n",
    "ols_list = []\n",
    "prediction_list = []\n",
    "r2_list = []\n",
    "# for all the cells\n",
    "for cells in np.arange(roi_number):\n",
    "#     print(f'current cell: {cells}')\n",
    "    # allocate memory for the padded matrix\n",
    "#     time_matrix = np.zeros((time_number, target_interval))\n",
    "    time_matrix = np.zeros((time_number, 2*target_interval*beh_columns))\n",
    "    # get the calcium data\n",
    "    current_calcium = calcium_matrix[:, cells:cells+1]\n",
    "        \n",
    "    # fill the padded matrix\n",
    "    for frame, el in enumerate(current_calcium):\n",
    "#         time_matrix[frame, :] = padded_behavior[:, frame:frame+target_interval]\n",
    "        time_matrix[frame, :] = padded_behavior[:, frame:frame+2*target_interval].flatten()\n",
    "    # calculate the covariance matrix\n",
    "    cov_matrix = time_matrix.T@time_matrix\n",
    "    # calculate the STA\n",
    "    sta = time_matrix.T@current_calcium\n",
    "    try:\n",
    "        # calculate the ols estimate\n",
    "        ols = np.linalg.inv(cov_matrix) @ sta   \n",
    "    except np.linalg.LinAlgError:\n",
    "        ols = []\n",
    "        \n",
    "    ols_list.append(ols)\n",
    "#     ols_estimator = lin.PoissonRegressor(alpha=10, max_iter=5000, fit_intercept=False)\n",
    "#     ols_estimator = lin.TweedieRegressor(alpha=0, max_iter=10000, fit_intercept=True, power=1)\n",
    "#     ols_estimator.fit(time_matrix, current_calcium.ravel())\n",
    "    # save the ols\n",
    "#     ols_list.append(ols_estimator.coef_)\n",
    "#     current_prediction = ols_estimator.predict(time_matrix)\n",
    "    \n",
    "#     print(current_prediction)\n",
    "#     prediction_list.append(current_prediction)\n",
    "#     r2_list.append(smet.r2_score(current_calcium, current_prediction))\n",
    "    "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2880a8",
   "metadata": {},
   "source": [
    "# Visualize the STAs\n",
    "\n",
    "# allocate memory for the plots\n",
    "plot_ols = []\n",
    "\n",
    "# for all the estimates\n",
    "for ols in ols_list:\n",
    "    if isinstance(ols, list):\n",
    "        continue    \n",
    "    ols = ols.reshape(beh_columns, target_interval*2)\n",
    "    \n",
    "    # return the radial variables to angles\n",
    "    new_radials = [el+'_x' for el in radial_features]\n",
    "    new_ols = []\n",
    "    new_features = []\n",
    "    for idx, feature in enumerate(feature_list):\n",
    "        if feature in new_radials:\n",
    "            new_feature = np.rad2deg(np.arctan2(ols[idx+1, :], ols[idx, :]))\n",
    "            new_ols.append(new_feature)\n",
    "            new_features.append(feature[:-2])\n",
    "        else:\n",
    "            new_ols.append(ols[idx, :])\n",
    "            new_features.append(feature)\n",
    "    ols = np.array(new_ols)\n",
    "                \n",
    "    # generate the image\n",
    "    current_plot = hv.Image(ols).opts(tools=['hover'])\n",
    "#     current_plot = hv.Curve(ols[7, :])\n",
    "    # save on the list\n",
    "    plot_ols.append(current_plot)\n",
    "print(new_features)\n",
    "# show the plot\n",
    "hv.Layout(plot_ols).opts(shared_axes=True).cols(3)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5f7f00",
   "metadata": {},
   "source": [
    "# Generate 2D \"tuning curves\"\n",
    "\n",
    "# define trial groups\n",
    "n_groups = 2\n",
    "interval = np.floor(len(calcium_trials)/n_groups)\n",
    "temp_vector = np.array(np.arange(interval))\n",
    "trial_groups = [(temp_vector+idx*interval).astype(int) for idx in np.arange(n_groups)]\n",
    "\n",
    "# define the features to use\n",
    "tc_features = ['cricket_0_mouse_distance', 'cricket_0_delta_heading']\n",
    "# tc_features = ['mouse_speed', 'latent_0']\n",
    "# tc_features = ['latent_1', 'latent_0']\n",
    "# tc_features = ['cricket_0_x', 'cricket_0_y']\n",
    "# tc_features = ['mouse_speed', 'cricket_0_mouse_distance']\n",
    "# tc_features = ['mouse_y', 'mouse_x']\n",
    "# tc_features = ['cricket_0_visual_angle', 'cricket_0_delta_head']\n",
    "\n",
    "print(feature_raw_trials[0].columns)\n",
    "# allocate the meta plot list\n",
    "meta_list = []\n",
    "hist_list = []\n",
    "# define the font size\n",
    "fontsize = {\n",
    "    'ticks': 11,\n",
    "    'labels': 13\n",
    "}\n",
    "\n",
    "# define the plot labels\n",
    "label_x = tc_features[1]\n",
    "label_y = tc_features[0]\n",
    "# for all the trial groups\n",
    "for trial_idx in trial_groups:\n",
    "    # allocate the plot list\n",
    "    tc_list = []\n",
    "    hist_temp_list = []\n",
    "    \n",
    "    feature_raw = [feature_raw_trials[el] for el in trial_idx]\n",
    "    calcium_matrix = np.concatenate([calcium_trials[el] for el in trial_idx], axis=0)\n",
    "    # get the relevant features\n",
    "    feature0 = pd.concat([el[tc_features[0]] for el in feature_raw])\n",
    "    feature0[np.isnan(feature0)] = 0\n",
    "    feature0 = ss.medfilt(feature0, 21)\n",
    "    feature1 = pd.concat([el[tc_features[1]] for el in feature_raw])\n",
    "    feature1[np.isnan(feature1)] = 0\n",
    "    feature1 = ss.medfilt(feature1, 21)\n",
    "\n",
    "    print(calcium_matrix.shape)\n",
    "    roi_number = calcium_matrix.shape[1]\n",
    "\n",
    "\n",
    "    st_base, x_edge, y_edge, idx = \\\n",
    "        stat.binned_statistic_2d(feature0, feature1, [], bins=20, statistic='count')\n",
    "    st_base[st_base==0] = 1\n",
    "    plot_st_base = np.log10(st_base)\n",
    "    plot_st_base[np.isinf(plot_st_base)] = 0\n",
    "    # st_base = (st_base-np.nanmin(st_base))/(np.nanmax(st_base)-np.nanmin(st_base))\n",
    "#     im_plot = hv.Image((plot_st_base), kdims=[tc_features[0], tc_features[1]], bounds=[0, 10, 0, 20])\n",
    "    im_plot = hv.Image((y_edge, x_edge, plot_st_base), kdims=[label_x, label_y])\n",
    "    im_plot.opts(width=250, tools=['hover'], fontsize=fontsize, xrotation=45)#, clim=(0, 0.2))\n",
    "\n",
    "    tc_list.append(im_plot)\n",
    "    # st_base = 1\n",
    "    # store the matrices for averaging\n",
    "    st_store = []\n",
    "    # for all the cells\n",
    "    for cells in np.arange(roi_number)[:]:\n",
    "        # get the calcium\n",
    "        current_calcium = calcium_matrix[:, cells]\n",
    "    #     current_calcium = ss.medfilt(pd.concat([el['mouse_speed'] for el in feature_raw]), 21)\n",
    "\n",
    "        # build 2d distribution\n",
    "        st, x_edge, y_edge, idx = \\\n",
    "            stat.binned_statistic_2d(feature0, feature1, current_calcium, bins=20, statistic='sum')\n",
    "    #     st[np.isnan(st)] = 0\n",
    "    #     st = (st-np.nanmin(st))/(np.nanmax(st)-np.nanmin(st))\n",
    "    #     st[np.isnan(st)] = 0\n",
    "        st[st_base<3] = 0\n",
    "        st_plot = (st/st_base)\n",
    "        st_plot[np.isnan(st_plot)] = 0\n",
    "        im_plot = hv.Image((y_edge, x_edge, st_plot), kdims=[label_x, label_y])\n",
    "        im_plot.opts(width=250, tools=['hover'], cmap='viridis', fontsize=fontsize, xrotation=45)#, clim=(0, 0.2))\n",
    "        tc_list.append(im_plot)\n",
    "        st_store.append(st)\n",
    "        \n",
    "        # store the actual map too\n",
    "        hist_temp_list.append(st_plot)\n",
    "\n",
    "    plot_ave = np.mean(st_store, axis=0)/st_base\n",
    "    # plot_ave[np.isinf(plot_ave)] = 0\n",
    "    im_plot = hv.Image((y_edge, x_edge, plot_ave), kdims=[label_x, label_y])\n",
    "    im_plot.opts(width=250, tools=['hover'], fontsize=fontsize, xrotation=45)#, clim=(0, 0.2))\n",
    "\n",
    "    tc_list.append(im_plot)\n",
    "    meta_list.append(tc_list)\n",
    "    hist_list.append(hist_temp_list)\n",
    "# reorder the list\n",
    "\n",
    "# top_half = [el for idx, el in enumerate(tc_list) if idx < len(tc_list)/2]\n",
    "# bottom_half = [el for idx, el in enumerate(tc_list) if idx >= len(tc_list)/2]\n",
    "# tc_list = [val for pair in zip(top_half, bottom_half) for val in pair]\n",
    "# lists = [l1, l2, ...]\n",
    "# [val for tup in zip(*lists) for val in tup]\n",
    "meta_list = [val for tup in zip(*meta_list) for val in tup]\n",
    "# create the layout\n",
    "hv.Layout(meta_list).opts(shared_axes=False)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd6b4a5",
   "metadata": {},
   "source": [
    "# calculate correlations between maps and plot\n",
    "\n",
    "# allocate memory for the correlations\n",
    "correlation_array = []\n",
    "# for all the cells\n",
    "for one, two in zip(hist_list[0], hist_list[1]):\n",
    "    # calculate the correlation between the pairs\n",
    "    corr_coef = np.corrcoef(one.flatten(), two.flatten())[1][0]\n",
    "    if np.isnan(corr_coef):\n",
    "#         continue\n",
    "        corr_coef = 0\n",
    "    correlation_array.append(corr_coef)\n",
    "\n",
    "correlation_array = np.array(correlation_array)\n",
    "\n",
    "# define the number of shuffles\n",
    "shuffle_number = 100\n",
    "# allocate memory for the shuffle results\n",
    "shuffle_array = np.zeros((correlation_array.shape[0], shuffle_number))\n",
    "# for all the shuffles\n",
    "for shuffle in np.arange(shuffle_number):\n",
    "    # allocate memory for the correlations\n",
    "    correlation_shuffle = []\n",
    "    # shuffle the lists\n",
    "    list_0 = random.sample(hist_list[0], len(hist_list[0]))\n",
    "    list_1 = random.sample(hist_list[1], len(hist_list[0]))\n",
    "    # for all the cells\n",
    "    for one, two in zip(list_0, list_1):\n",
    "        # calculate the correlation between the pairs\n",
    "        corr_coef = np.corrcoef(one.flatten(), two.flatten())[1][0]\n",
    "        if np.isnan(corr_coef):\n",
    "            corr_coef = 0\n",
    "#             continue\n",
    "        correlation_shuffle.append(corr_coef)\n",
    "    shuffle_array[:, shuffle] = correlation_shuffle\n",
    "\n",
    "    \n",
    "# turn into an array\n",
    "freq, bins = np.histogram(correlation_array, density=True, bins=20)\n",
    "bin_centers = bins[:-1] + np.diff(bins)/2\n",
    "hist_ori = hv.Curve((bin_centers, np.cumsum(freq)), kdims=['Correlation', 'Probability'])\n",
    "hist_ori.opts(fontsize=fontsize)\n",
    "\n",
    "freq, bins = np.histogram(shuffle_array.flatten(), density=True, bins=bins)\n",
    "bin_centers = bins[:-1] + np.diff(bins)/2\n",
    "hist_shuffle = hv.Curve((bin_centers, np.cumsum(freq)))\n",
    "\n",
    "\n",
    "(hist_ori*hist_shuffle)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ab8b41",
   "metadata": {},
   "source": [
    "# ROC analysis?\n",
    "\n"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prey_capture_old",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
