{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "905cf8e4",
   "metadata": {},
   "source": [
    "# Notebook to train a NN for repairing pose estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d39ef5af",
   "metadata": {},
   "source": [
    "# imports\n",
    "\n",
    "import paths\n",
    "import numpy as np\n",
    "import functions_bondjango as bd\n",
    "import pandas as pd\n",
    "import processing_parameters\n",
    "import os\n",
    "from snakemake_scripts.sub_preprocess_S1 import process_corners\n",
    "import random\n",
    "import functions_plotting as fp\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Adam"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f01c0ccb",
   "metadata": {},
   "source": [
    "# load the data\n",
    "\n",
    "# define the likelihood threshold for the DLC points\n",
    "likelihood_threshold = 0.8\n",
    "\n",
    "# define the search string\n",
    "search_string = processing_parameters.search_string\n",
    "\n",
    "# define the target model\n",
    "if 'miniscope' in search_string:\n",
    "    target_model = 'video_experiment'\n",
    "else:\n",
    "    target_model = 'vr_experiment'\n",
    "\n",
    "# get the queryset\n",
    "file_set = bd.query_database(target_model, search_string)\n",
    "\n",
    "# allocate memory to accumulate the trajectories\n",
    "all_points = []\n",
    "\n",
    "# run through the files\n",
    "for files in file_set:\n",
    "    raw_path = files['bonsai_path']\n",
    "    calcium_path = files['bonsai_path'][:-4] + '_calcium.hdf5'\n",
    "\n",
    "    file_path_dlc = files['bonsai_path'].replace('.csv', '_dlc.h5')\n",
    "    # load the bonsai info\n",
    "    raw_h5 = pd.read_hdf(file_path_dlc)\n",
    "    # get the column names\n",
    "    column_names = raw_h5.columns\n",
    "    # take only the relevant columns\n",
    "    # DLC in small arena\n",
    "    filtered_traces = pd.DataFrame(raw_h5[[\n",
    "        [el for el in column_names if ('mouseSnout' in el) and ('x' in el)][0],\n",
    "        [el for el in column_names if ('mouseSnout' in el) and ('y' in el)][0],\n",
    "        [el for el in column_names if ('mouseBarL' in el) and ('x' in el)][0],\n",
    "        [el for el in column_names if ('mouseBarL' in el) and ('y' in el)][0],\n",
    "        [el for el in column_names if ('mouseBarR' in el) and ('x' in el)][0],\n",
    "        [el for el in column_names if ('mouseBarR' in el) and ('y' in el)][0],\n",
    "        [el for el in column_names if ('mouseHead' in el) and ('x' in el)][0],\n",
    "        [el for el in column_names if ('mouseHead' in el) and ('y' in el)][0],\n",
    "        [el for el in column_names if ('mouseBody1' in el) and ('x' in el)][0],\n",
    "        [el for el in column_names if ('mouseBody1' in el) and ('y' in el)][0],\n",
    "        [el for el in column_names if ('mouseBody2' in el) and ('x' in el)][0],\n",
    "        [el for el in column_names if ('mouseBody2' in el) and ('y' in el)][0],\n",
    "        [el for el in column_names if ('mouseBody3' in el) and ('x' in el)][0],\n",
    "        [el for el in column_names if ('mouseBody3' in el) and ('y' in el)][0],\n",
    "        [el for el in column_names if ('mouseBase' in el) and ('x' in el)][0],\n",
    "        [el for el in column_names if ('mouseBase' in el) and ('y' in el)][0],\n",
    "        [el for el in column_names if ('cricketHead' in el) and ('x' in el)][0],\n",
    "        [el for el in column_names if ('cricketHead' in el) and ('y' in el)][0],\n",
    "        [el for el in column_names if ('cricketBody' in el) and ('x' in el)][0],\n",
    "        [el for el in column_names if ('cricketBody' in el) and ('y' in el)][0],\n",
    "    ]].to_numpy(), columns=['mouse_snout_x', 'mouse_snout_y', 'mouse_barl_x', 'mouse_barl_y',\n",
    "                            'mouse_barr_x', 'mouse_barr_y', 'mouse_head_x', 'mouse_head_y',\n",
    "                            'mouse_x', 'mouse_y', 'mouse_body2_x', 'mouse_body2_y',\n",
    "                            'mouse_body3_x', 'mouse_body3_y', 'mouse_base_x', 'mouse_base_y',\n",
    "                            'cricket_0_head_x', 'cricket_0_head_y', 'cricket_0_x', 'cricket_0_y'])\n",
    "\n",
    "    # get the likelihoods\n",
    "    likelihood_frame = pd.DataFrame(raw_h5[[\n",
    "        [el for el in column_names if ('mouseHead' in el) and ('likelihood' in el)][0],\n",
    "        [el for el in column_names if ('mouseBody1' in el) and ('likelihood' in el)][0],\n",
    "        [el for el in column_names if ('mouseBody2' in el) and ('likelihood' in el)][0],\n",
    "        [el for el in column_names if ('mouseBody3' in el) and ('likelihood' in el)][0],\n",
    "        [el for el in column_names if ('mouseBase' in el) and ('likelihood' in el)][0],\n",
    "        [el for el in column_names if ('cricketHead' in el) and ('likelihood' in el)][0],\n",
    "        [el for el in column_names if ('cricketBody' in el) and ('likelihood' in el)][0],\n",
    "    ]].to_numpy(), columns=['mouse_head', 'mouse', 'mouse_body2',\n",
    "                            'mouse_body3', 'mouse_base',\n",
    "                            'cricket_0_head', 'cricket_0'])\n",
    "\n",
    "    # nan the trace where the likelihood is too low\n",
    "    # for all the columns\n",
    "    for col in likelihood_frame.columns:\n",
    "        # get the vector for nans\n",
    "        nan_vector = likelihood_frame[col] < likelihood_threshold\n",
    "        # nan the points\n",
    "        filtered_traces.loc[nan_vector, col+'_x'] = np.nan\n",
    "        filtered_traces.loc[nan_vector, col+'_y'] = np.nan\n",
    "\n",
    "    corner_info = pd.DataFrame(raw_h5[[\n",
    "        [el for el in column_names if ('corner_UL' in el) and ('x' in el)][0],\n",
    "        [el for el in column_names if ('corner_UL' in el) and ('y' in el)][0],\n",
    "        [el for el in column_names if ('corner_BL' in el) and ('x' in el)][0],\n",
    "        [el for el in column_names if ('corner_BL' in el) and ('y' in el)][0],\n",
    "        [el for el in column_names if ('corner_BR' in el) and ('x' in el)][0],\n",
    "        [el for el in column_names if ('corner_BR' in el) and ('y' in el)][0],\n",
    "        [el for el in column_names if ('corner_UR' in el) and ('x' in el)][0],\n",
    "        [el for el in column_names if ('corner_UR' in el) and ('y' in el)][0],\n",
    "    ]].to_numpy(), columns=['corner_UL_x', 'corner_UL_y', 'corner_BL_x', 'corner_BL_y',\n",
    "                            'corner_BR_x', 'corner_BR_y', 'corner_UR_x', 'corner_UR_y'])\n",
    "    # get the corners\n",
    "    corner_points = process_corners(corner_info)\n",
    "\n",
    "    # accumulate the points\n",
    "    all_points.append(filtered_traces)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0d5f93c",
   "metadata": {},
   "source": [
    "# Prepare the data\n",
    "\n",
    "# define the target animal\n",
    "target_animal = 'cricket'\n",
    "# define the amount of delay to include\n",
    "delay = 2\n",
    "# define the learning rate\n",
    "learning_rate = 0.001\n",
    "# define the test train split\n",
    "validation_split = 0.2\n",
    "# get the target column names\n",
    "column_list = [el for el in all_points[0].columns if (target_animal in el) and ('_x' in el)]\n",
    "column_list += [el for el in all_points[0].columns if (target_animal in el) and ('_y' in el)]\n",
    "# get the number of features\n",
    "number_features = len(column_list)\n",
    "# allocate memory for the data\n",
    "data_matrix = []\n",
    "\n",
    "# for all the files\n",
    "for files in all_points:\n",
    "    # select the features to use\n",
    "    current_points = np.array(files.loc[:, column_list])\n",
    "    # get the number of timepoints\n",
    "    number_timepoints = current_points.shape[0]\n",
    "\n",
    "    # allocate memory for the design matrix\n",
    "    design_matrix = np.zeros((number_timepoints, number_features*delay + number_features))\n",
    "    # pad the data according to the delay\n",
    "#     current_points = np.concatenate((np.zeros((delay, number_features)), current_points), axis=0)\n",
    "    current_points = np.concatenate((np.zeros((int(delay/2), number_features)), current_points,\n",
    "                                    np.zeros((int(delay/2), number_features))), axis=0)\n",
    "    # assemble the design matrix\n",
    "    # for all the points\n",
    "    for points in np.arange(number_timepoints):\n",
    "        design_matrix[points, :] = current_points[points:points+delay+1, :].reshape([1, -1])\n",
    "    # save\n",
    "    data_matrix.append(design_matrix)\n",
    "\n",
    "# concatenate the data\n",
    "data_matrix = np.concatenate(data_matrix, axis=0)\n",
    "\n",
    "# get a vector with only the y rows that have nans\n",
    "# nan_vector = np.any(np.isnan(data_matrix[:, :number_features]), axis=1)\n",
    "nan_vector = \\\n",
    "np.any(np.isnan(data_matrix[:, number_features*int(delay/2):\n",
    "                            number_features*int(delay/2)+number_features]), axis=1)\n",
    "\n",
    "# eliminate the points with NaNs in them\n",
    "nonan_matrix = data_matrix[~nan_vector, :]\n",
    "nan_matrix = data_matrix[nan_vector, :]\n",
    "\n",
    "# make nans 0\n",
    "nonan_matrix[np.isnan(nonan_matrix)] = 0\n",
    "\n",
    "# X = nonan_matrix[:, number_features:]\n",
    "# X = nonan_matrix\n",
    "# y = nonan_matrix[:, :number_features]\n",
    "X = np.concatenate((nonan_matrix[:, :number_features*int(delay/2)], \n",
    "                    nonan_matrix[:, -number_features*int(delay/2):]), axis=1)\n",
    "y = nonan_matrix[:, number_features*int(delay/2):number_features*int(delay/2)+number_features]\n",
    "\n",
    "# generate random 0s in the matrix\n",
    "random_zero = np.random.rand(*X.shape)\n",
    "random_zero = random_zero>0.5\n",
    "X[random_zero] = 0\n",
    "\n",
    "# # also zero additional point in the current frame\n",
    "# random_current = np.random.rand(X.shape[0], number_features)\n",
    "# random_current = random_current>0.5\n",
    "# X_subset = X[:, :number_features]\n",
    "# X_subset[random_current] = 0\n",
    "# X[:, :number_features] = X_subset\n",
    "\n",
    "# shuffle the data\n",
    "shuffle_idx = tf.random.shuffle(np.arange(X.shape[0]))\n",
    "X = X[shuffle_idx, :]\n",
    "y = y[shuffle_idx, :]\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "# normalize\n",
    "# X = tf.keras.utils.normalize(X, axis=1)\n",
    "# y = tf.keras.utils.normalize(y, axis=1)\n",
    "\n",
    "# split into training and test sets\n",
    "\n",
    "# get a selection vector based on the number of points and the desired split\n",
    "validation_idx = random.sample(list(np.arange(X.shape[0])), \n",
    "                               np.int(np.round(validation_split*X.shape[0])))\n",
    "# print(validation_idx)\n",
    "validation_vector = np.zeros([X.shape[0]])\n",
    "validation_vector[validation_idx] = 1\n",
    "validation_vector = validation_vector == 1\n",
    "train_vector = ~validation_vector\n",
    "\n",
    "train_X = X[train_vector, :]\n",
    "train_y = y[train_vector, :]\n",
    "\n",
    "validation_X = X[validation_vector, :]\n",
    "validation_y = y[validation_vector, :]\n",
    "\n",
    "# reshape the data\n",
    "# train_X = train_X.reshape([-1, len(column_list)/2, 2])\n",
    "\n",
    "# print(train_X[:10, 0])\n",
    "# print(validation_X[:10, 0])\n",
    "\n",
    "print(train_X.shape)\n",
    "print(validation_X.shape)\n",
    "print(train_y.shape)\n",
    "print(column_list)\n",
    "# # normalize\n",
    "# train_X = tf.keras.utils.normalize(train_X, axis=1)\n",
    "# train_y = tf.keras.utils.normalize(train_y, axis=1)\n",
    "# validation_X = tf.keras.utils.normalize(validation_X, axis=1)\n",
    "# validation_y = tf.keras.utils.normalize(validation_y, axis=1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e83e1202",
   "metadata": {},
   "source": [
    "# Create and visualize the network\n",
    "\n",
    "# create the network\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=[train_X.shape[1], ]))\n",
    "# model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(number_features, activation='relu'))\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mae')\n",
    "\n",
    "model.summary()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c55112d",
   "metadata": {},
   "source": [
    "# Fit the network\n",
    "\n",
    "# model.fit(X, y, validation_split=validation_split, batch_size=256, epochs=500, shuffle=True, verbose=2)\n",
    "# model.fit(train_X, train_y, validation_split=validation_split, batch_size=256, epochs=500, shuffle=True, verbose=2)\n",
    "model.fit(train_X, train_y, validation_data=(validation_X, validation_y), \n",
    "          batch_size=256, epochs=500, shuffle=True, verbose=2)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04190046",
   "metadata": {},
   "source": [
    "# Evaluate the network\n",
    "\n",
    "# predicted_data = model.predict(X[-100:, :])\n",
    "\n",
    "# fp.plot_2d([[y[-100:, 1], predicted_data[-100:, 1]]])\n",
    "\n",
    "predicted_data = model.predict(validation_X[:100, :])\n",
    "\n",
    "fp.plot_2d([[validation_y[:100, 1], predicted_data[:100, 1]]])\n",
    "\n",
    "# predicted_data = model.predict(train_X[:100, :])\n",
    "\n",
    "# fp.plot_2d([[train_y[:100, 1], predicted_data[:100, 1]]])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "210d3842",
   "metadata": {},
   "source": [
    "# Save the model\n",
    "\n",
    "# get the path to save the model\n",
    "save_path = os.path.join(paths.pose_repair_path, target_animal)\n",
    "\n",
    "# save the model\n",
    "model.save(save_path)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "baad5692",
   "metadata": {},
   "source": [
    "# Evaluate with single trials\n",
    "\n",
    "# select the target trial\n",
    "target_trial = 600\n",
    "\n",
    "# load the corresponding data points\n",
    "current_points = all_points[target_trial].loc[:, column_list].to_numpy()\n",
    "\n",
    "# get the number of timepoints\n",
    "number_timepoints = current_points.shape[0]\n",
    "\n",
    "# allocate memory for the design matrix\n",
    "design_matrix = np.zeros((number_timepoints, number_features*delay + number_features))\n",
    "# pad the data according to the delay\n",
    "# current_points = np.concatenate((np.zeros((delay, number_features)), current_points), axis=0)\n",
    "# # assemble the design matrix\n",
    "# # for all the points\n",
    "# for points in np.arange(number_timepoints):\n",
    "#     design_matrix[points, :] = current_points[points:points+delay+1, :].reshape([1, -1])\n",
    "\n",
    "current_points = np.concatenate((np.zeros((int(delay/2), number_features)), current_points,\n",
    "                                np.zeros((int(delay/2), number_features))), axis=0)\n",
    "# assemble the design matrix\n",
    "# for all the points\n",
    "for points in np.arange(number_timepoints):\n",
    "    design_matrix[points, :] = current_points[points:points+delay+1, :].reshape([1, -1])\n",
    "\n",
    "    \n",
    "# extract the prediction and validation matrices\n",
    "design_matrix[np.isnan(design_matrix)] = 0\n",
    "\n",
    "# eval_X = design_matrix[:, number_features:]\n",
    "# eval_X = design_matrix\n",
    "# eval_y = design_matrix[:, :number_features]\n",
    "eval_X = np.concatenate((design_matrix[:, :number_features*int(delay/2)], \n",
    "                    design_matrix[:, -number_features*int(delay/2):]), axis=1)\n",
    "eval_y = design_matrix[:, number_features*int(delay/2):number_features*int(delay/2)+number_features]\n",
    "\n",
    "# predict the data\n",
    "predicted_data = model.predict(eval_X)\n",
    "# define the target variable\n",
    "# target_variable = 5\n",
    "for target_variable in np.arange(number_features):\n",
    "    # plot against the real data\n",
    "    fp.plot_2d([[eval_y[:, target_variable], predicted_data[:, target_variable]]])\n"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-prey_capture]",
   "language": "python",
   "name": "conda-env-.conda-prey_capture-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
